TODO:
[X] crawl seed URL and grab all links recursively
[X] save all pages to a directory (as JSON)
[X] preprocess text
    [X] remove redundant space, newlines, and other whitespace
    [X] remove punctuation and special characters
    [X] convert all text to lowercase
    [X] tokenize text
    [X] exclude bibliography page
[X] display corpus and vocabulary statistics
    [X] # of unique words in corpus
    [X] total # of words in corpus
    [X] average page length (in words)
    [X] top 30 most frequent words and their frequencies (collection freq. and document freq.) ordered by collection freq.
    [X] top 30 most frequent words after removing stop words
[ ] write report and save as PDF
[ ] submit both `crawler.py` and `stats.py`, and `report.pdf`

