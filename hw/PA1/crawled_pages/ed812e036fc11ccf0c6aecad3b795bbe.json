{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-in-information-retrieval-1.html",
  "title": "Evaluation in information retrieval",
  "body": "\n\n\n\n\nEvaluation in information retrieval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Information retrieval system evaluation\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n\n\nEvaluation in information retrieval\n\n\nWe have seen in the preceding chapters many alternatives in designing an\nIR system.  How do we know which of these techniques are effective in\nwhich applications?  Should we use stop lists?  Should we stem?  Should\nwe use inverse document frequency weighting? \nInformation retrieval has developed as a highly empirical\ndiscipline, requiring careful and thorough evaluation\nto demonstrate the superior performance of novel techniques on\nrepresentative document collections.\n\n\nIn this chapter we begin with a discussion of measuring the\neffectiveness of IR systems (Section 8.1 ) and the test collections\nthat are most often used for this purpose (Section 8.2 ).\nWe then present the straightforward notion of relevant and \nnonrelevant documents and the formal evaluation methodology\nthat has been \ndeveloped for evaluating unranked retrieval results\n(Section 8.3 ). This includes explaining the kinds of \nevaluation measures that are standardly used for document retrieval and related\ntasks like text classification and why they are appropriate.  We then extend\nthese notions and develop further measures for evaluating ranked\nretrieval results (Section 8.4 ) and discuss developing\nreliable and informative test collections (Section 8.5 ).\n\n\nWe then step back to introduce the notion of user utility, and how it is \napproximated by the use of document relevance (Section 8.6 ).\nThe key utility measure is user happiness.  Speed of\nresponse and the size of the index are factors in user happiness.  It seems\nreasonable to assume that\nrelevance of results is the most important factor:\nblindingly fast, useless answers do not make a user happy.  However,\nuser perceptions do not always coincide with system designers' notions\nof quality.  For example, user happiness commonly depends very strongly\non user interface design issues, including the layout, clarity, and\nresponsiveness of the user interface, which are independent of the\nquality of the results returned.  We touch on\nother measures of the quality of a system, in particular\nthe generation of high-quality\nresult summary snippets, which strongly influence \nuser utility, but are not measured in the basic relevance ranking\nparadigm (Section 8.7 ). \n\n\n\n\nSubsections\n\nInformation retrieval system evaluation\nStandard test collections\nEvaluation of unranked retrieval sets\nEvaluation of ranked retrieval results\nAssessing relevance\n\nCritiques and justifications of the concept of relevance\n\n\nA broader perspective: System quality and user\n  utility\n\nSystem issues\nUser utility\nRefining a deployed system\n\n\nResults snippets\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Information retrieval system evaluation\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}