{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/review-of-basic-probability-theory-1.html",
  "title": "Review of basic probability theory",
  "body": "\n\n\n\n\nReview of basic probability theory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The Probability Ranking Principle\n Up: Probabilistic information retrieval\n Previous: Probabilistic information retrieval\n    Contents \n    Index\n\n\n\n \n\nReview of basic probability theory\n\n\nWe hope that the reader has seen a little basic probability\ntheory previously.  We will give a very quick review; some\nreferences for further reading appear at the end of the\nchapter. A variable  represents an event (a subset of the\nspace of possible outcomes).  Equivalently, we can represent\nthe subset via a  random\nvariable  , which is a function from\noutcomes to real numbers; the subset is the domain over which\nthe random variable  has a particular\nvalue. \nOften we\nwill not know with certainty whether an event is true in the\nworld.  We can ask the probability of the event \n. For two events  and , the\njoint event of both events occurring is described by the joint probability\n. The conditional probability  expresses\nthe probability of event  given that event  occurred.\nThe fundamental relationship between joint and\nconditional probabilities is given by the  chain\nrule :\n\n\n\n\n\n\n(56)\n\n\nWithout making any assumptions, the probability of a joint event equals the probability of one of the events multiplied by the probability of the other event conditioned on knowing the first event happened.\n\n\nWriting \n for the complement of an event, we similarly have:\n\n\n\n\n\n\n(57)\n\n\nProbability theory also has a  partition rule , which says that if an event  can be divided into an exhaustive set of disjoint subcases, then the probability of  is the sum of the probabilities of the subcases.  A special case of this rule gives that:\n\n\n\n\n\n\n(58)\n\n\n\nFrom these we can derive  Bayes' Rule  for inverting conditional probabilities:\n\n\n\n\n\n\n(59)\n\n\nThis equation can also be thought of as a way of updating probabilities.  We start off with an initial estimate of how likely the event  is when we do not have any other information; this is the  prior probability .  Bayes' rule lets us derive a  posterior probability  after having seen the evidence , based on the  likelihood  of  occurring in the two cases that  does or does not hold.\n\nFinally, it is often useful to talk about the  odds  of an event, which provide a kind of multiplier for how probabilities change:\n\n\n\n\n\n\n(60)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The Probability Ranking Principle\n Up: Probabilistic information retrieval\n Previous: Probabilistic information retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}