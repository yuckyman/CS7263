{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html",
  "title": "The text classification problem",
  "body": "\n\n\n\n\nThe text classification problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Naive Bayes text classification\n Up: Text classification and Naive\n Previous: Text classification and Naive\n    Contents \n    Index\n\n\n\n\n \n\nThe text classification problem\n\n\nIn text classification, we are given a description \n of a document, where  is the\n  document space ; and a fixed set of\n classes \n.\n Classes are also called \n categories \nor\n labels .  Typically, the document space \n\nis\nsome type of high-dimensional space, and the classes are\nhuman defined for the needs of an application, as in the\nexamples China and documents that talk about multicore computer chips above. We are given a\n training set  \n of labeled documents\n\n,\nwhere\n\n. For example:\n\n\n\n\n\n\n(111)\n\n\nfor the one-sentence document\nBeijing joins the World Trade Organization\nand the class (or label) China.\n\n\nUsing a   learning method  or  learning algorithm , we then wish to learn a\nclassifier     or  classification function   that maps\ndocuments to classes:\n\n\n\n\n\n\n\n\n(112)\n\n\n\nThis type of learning is called\n   supervised learning  because a\nsupervisor (the human who defines the classes and labels\ntraining documents) serves as a teacher directing the\nlearning process. We denote the supervised learning method\nby  and write \n. The\nlearning method  takes the training set\n\n as input and returns the learned\nclassification function .\n\n\nMost names for learning methods  are\nalso used for classifiers . We talk about the Naive\nBayes (NB) learning method  when we say that\n``Naive Bayes is robust,'' meaning that it can be applied to\nmany different learning problems and is unlikely to produce\nclassifiers that fail catastrophically. But when\nwe say that ``Naive Bayes had an error rate of 20%,'' we\nare describing an experiment in which a particular NB\nclassifier  (which was produced\nby the NB learning method) had a 20% error\nrate in an application.\n\n\nFigure 13.1  shows an example of text\nclassification from the Reuters-RCV1 collection, introduced\nin Section 4.2 , page 4.2 . There are six\nclasses (UK, China, ..., sports),\neach with three training documents.  We show a few mnemonic\nwords for each document's content. The training set provides\nsome typical examples for each class, so that we can learn\nthe classification function .\nOnce we have learned , we can apply it to the\n  test set  (or  test data ),\nfor example, the new document first private\nChinese airline whose class is unknown. In Figure 13.1 ,\nthe classification function assigns the new document to\nclass \n China, which is the\ncorrect assignment.\n\n\nThe classes in text classification often have some\ninteresting structure such as the hierarchy in\nFigure 13.1 . There are two instances each of\nregion categories, industry categories, and subject area\ncategories. A hierarchy can be an important aid in solving a\nclassification problem; see Section 15.3.2  for\nfurther discussion.  Until then, we will make the\nassumption in the text classification chapters\nthat the classes form a\nset with no subset relationships between them.\n\n\n\n\nFigure 13.1:\nClasses, training set, and test set in text\nclassification .\n\n\n\n\nDefinition eqn:gammadef stipulates that a\ndocument is a member of exactly one class. This is not the\nmost appropriate model for the hierarchy in\nFigure 13.1 . For instance, a document about the\n2008 Olympics should be a\nmember of two classes: the China class and the\nsports class. This type of classification problem is\nreferred to as an \n any-of  \nproblem and we will\nreturn to it in Section 14.5 (page ). For the\ntime being, we only consider\n one-of  \nproblems\nwhere a document is a member of exactly one class.\n\n\nOur goal in text classification is high accuracy on test\ndata or new data - for example, the newswire\narticles that we will encounter tomorrow morning in the\nmulticore chip example.  It is easy to achieve high accuracy\non the training set (e.g., we can simply memorize the\nlabels). But high accuracy on the training set in general\ndoes not mean that the classifier will work well on new data\nin an application.\nWhen we use the training set to learn a classifier for test\ndata, we make the assumption that training data and test\ndata are similar or from the same distribution. We defer a\nprecise definition of this notion to  Section 14.6 (page ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Naive Bayes text classification\n Up: Text classification and Naive\n Previous: Text classification and Naive\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}