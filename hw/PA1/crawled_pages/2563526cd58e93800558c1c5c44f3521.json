{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html",
  "title": "Using query likelihood language models in IR",
  "body": "\n\n\n\n\nUsing query likelihood language models in IR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Estimating the query generation\n Up: The query likelihood model\n Previous: The query likelihood model\n    Contents \n    Index\n\n\n\n\n \n\nUsing query likelihood language models in IR\n\n\nLanguage modeling is a quite general formal approach to IR, with many\nvariant realizations. The original and basic method for using language\nmodels in IR is the  query likelihood model .  In it, we\nconstruct from \neach document  in the collection a language model .  Our goal is\nto rank documents by , where the probability of a document is\ninterpreted as the likelihood that it is relevant to the query.  Using\nBayes rule (as introduced in probirsec), we have:\n\n\n\n\n\n\n(98)\n\n\n is the same for all documents, and so can be ignored.\nThe prior probability of a document  is often treated as uniform\nacross all  and so it can also be \nignored, but we could implement a genuine prior which could include\ncriteria like authority, length, genre, newness, and number of previous\npeople who have read the document.  But, given these simplifications, we\nreturn results ranked by simply , the probability of the query\n under the language model derived from .\nThe Language Modeling approach thus attempts to model the query generation\nprocess: Documents are ranked by the probability that a query would be\nobserved as a random sample from the respective document model.\n\n\nThe most common way to do this is using the multinomial unigram language\nmodel, which is equivalent to a multinomial Naive Bayes\nmodel (page 13.3 ), where the\ndocuments are the classes, each treated in the estimation as a separate\n``language''.  Under this model, we have that:\n\n\n\n\n\n\n\n(99)\n\n\nwhere, again \n is the multinomial coefficient for the query ,\nwhich we will henceforth ignore, since it is a constant for a\nparticular query.\n\n\nFor retrieval based on a language model (henceforth  LM ), we \ntreat the generation of queries as a random process.\nThe approach is to \n\n\nInfer a LM for each document.\n\nEstimate , the probability of generating the query\n  according to each of these document models.\n\nRank the documents according to these probabilities.\n\n\nThe intuition of the basic model is that the user has a prototype document\nin mind, and\ngenerates a query based on words that appear in this document.\nOften, users\nhave a reasonable idea of terms that are likely to occur in documents of\ninterest and they will choose query terms that distinguish these\ndocuments from others in the collection.Collection statistics are an integral part of the language model, rather\nthan being used heuristically as in many other approaches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Estimating the query generation\n Up: The query likelihood model\n Previous: The query likelihood model\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}