{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/assessing-relevance-1.html",
  "title": "Assessing relevance",
  "body": "\n\n\n\n\nAssessing relevance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Critiques and justifications of\n Up: Evaluation in information retrieval\n Previous: Evaluation of ranked retrieval\n    Contents \n    Index\n\n\n\n \n\nAssessing relevance\n\n\nTo properly evaluate a system, your \ntest information needs must be germane to the documents in the test\ndocument collection, and appropriate for predicted usage of the system.  These\ninformation needs are best designed by domain experts.  Using random\ncombinations of\nquery terms as an information need is generally not a good idea because\ntypically they will not resemble the actual distribution of\ninformation needs. \n\n\nGiven information needs and documents, you need to collect relevance\nassessments.  This is a time-consuming and expensive process involving\nhuman beings.  For tiny collections like Cranfield, exhaustive\njudgments of relevance for each query and document pair were\nobtained.  For large modern collections, it is usual for relevance to\nbe assessed only for a subset of the documents for each query.\nThe most standard approach is  pooling , where relevance is\nassessed over a subset of the collection that is\nformed from the top  documents returned by a number of different IR\nsystems (usually the ones to be evaluated),\nand perhaps other sources such as the results of Boolean keyword searches or\ndocuments found by expert searchers in an interactive process.\n\n\n\n\n\nTable 8.2:\nCalculating the kappa statistic.\n\n \n \nJudge 2 Relevance\n \n\n \nYes\n No\nTotal\n\nJudge 1\nYes\n300\n 20\n320\n\nRelevance\nNo\n10\n 70\n80\n\n \nTotal\n310\n 90\n400\n\n\n\n\n\n\n\nObserved proportion of the times the judges agreed \n\n\nPooled marginals \n\n\n\n\nProbability that the two judges agreed by chance \n\n\nKappa statistic \n\n \n\n\n\n\n\nA human is not a device that reliably reports a gold\nstandard judgment of relevance of a document to a query.  Rather,\nhumans and their relevance judgments are quite idiosyncratic and\nvariable.  But this is not a problem to be solved: in the final\nanalysis, the success of an IR system depends on how good it is at\nsatisfying the needs of these idiosyncratic humans, one information\nneed at a time.\n\n\nNevertheless, it is interesting to consider and measure how much\nagreement between judges there is on relevance judgments.\nIn the social sciences, a common measure for agreement between judges is\nthe  kappa statistic .  It is designed for categorical judgments and\ncorrects a simple agreement rate for the rate of chance agreement.\n\n\n\n\n\n\n(46)\n\n\nwhere  is the proportion of the times the judges agreed, and \nis the proportion of the times they would be expected to agree by chance.\nThere are choices in how the latter is estimated: if we simply say we are\nmaking a two-class decision and assume nothing more, then the expected chance\nagreement rate is 0.5.  \nHowever, normally the class distribution assigned is skewed, and it is usual to use\n\n marginal \nstatistics to \ncalculate expected agreement.There are still two ways to do it depending on whether one pools the marginal distribution across judges or uses the marginals for each judge separately; both forms have been used, but we present the pooled version because it is more conservative in the presence of systematic differences in assessments across judges.\nThe calculations are shown in Table 8.2 .\nThe kappa value will be 1 if two judges always agree, 0 if they agree\nonly at the rate given by chance, and negative if they are worse than\nrandom.  If there are more than two judges, it is normal to calculate\nan average pairwise kappa value.  As a rule of thumb, a kappa value\nabove 0.8 is taken as good agreement, a kappa value between 0.67 and\n0.8 is taken as fair agreement, and agreement below 0.67 is seen as data\nproviding a dubious basis for an evaluation, though the precise cutoffs\ndepend on the purposes for which the data will be used.\n\n\nInterjudge agreement of relevance has been measured within the TREC\nevaluations and for medical IR collections.  Using\nthe above rules of thumb, the level of agreement normally falls in the\nrange of ``fair'' (0.67-0.8).\nThe fact that human agreement on a binary relevance\njudgment is quite modest is one reason for not requiring more\nfine-grained relevance labeling from the test set creator.\nTo answer the question of whether\nIR evaluation results are valid despite the variation of individual\nassessors' judgments, people have experimented with evaluations taking\none or the other of two judges' opinions as the gold standard.\nThe choice can make a considerable absolute \ndifference to reported scores, but has in general been found to have\nlittle impact on the relative effectiveness ranking of either different systems or\nvariants of a single system which are being compared for effectiveness.\n\n\n\n\nSubsections\n\nCritiques and justifications of the concept of relevance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Critiques and justifications of\n Up: Evaluation in information retrieval\n Previous: Evaluation of ranked retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}