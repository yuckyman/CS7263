{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/cluster-cardinality-in-k-means-1.html",
  "title": "Cluster cardinality in K-means",
  "body": "\n\n\n\n\nCluster cardinality in K-means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Model-based clustering\n Up: K-means\n Previous: K-means\n    Contents \n    Index\n\n\n\n\n Cluster cardinality in K-means\n\nWe stated in Section 16.2  that the number of\nclusters  is an input to most flat clustering algorithms.\nWhat do we do if we cannot come up with a plausible\nguess for ?  \n\n\nA naive approach would be to select the optimal value of\n according to the objective function, namely the value of\n that minimizes RSS.  Defining \n\nas the minimal RSS of all clusterings with  clusters, we\nobserve that \n is a monotonically\ndecreasing function in  (Exercise 16.7 ),\nwhich reaches its minimum 0 for  where  is the\nnumber of documents.  We would end up with each document\nbeing in its own cluster.  Clearly, this is not an optimal\nclustering.\n\n\nA heuristic method that gets around this problem is to\nestimate \n as follows.\nWe first perform\n\n(e.g., ) clusterings with  clusters (each\nwith a different initialization) and\ncompute the RSS of each. Then we take the minimum of the \nRSS values.\nWe denote this minimum by\n\n.  Now we can  inspect\nthe values \n as \nincreases and find the ``knee'' in the curve - the point\nwhere successive decreases in\n\n become noticeably smaller.\nThere are two such points in Figure 16.8 , one at\n, where the gradient flattens slightly, and a clearer\nflattening at . This is typical: there is seldom a\nsingle best number of clusters. We still need to employ an\nexternal constraint to choose from a number of possible\nvalues of  (4 and 9 in this case).\n\n\nA second type of criterion for cluster cardinality imposes a\npenalty for each new cluster - where conceptually we start\nwith a single cluster containing all documents and then\nsearch for the optimal number of clusters  by\nsuccessively incrementing  by one. To determine the cluster\ncardinality in this way, we create a generalized\nobjective function that combines two elements:  distortion ,\na measure of how much documents deviate from the prototype\nof their clusters\n(e.g., RSS\nfor  -means); and a\nmeasure of  model complexity . We interpret a\nclustering here as a model of the data. Model complexity in\nclustering is usually the number of clusters or a function\nthereof. For  -means, we then get this\nselection criterion for :\n \n\n\n\n\n \n \n\n(195)\n\n\nwhere   is a weighting factor. A large value\nof  favors solutions with few clusters. For\n, there is no penalty for more clusters and\n is the best solution.\n\n\nThe obvious difficulty with Equation 195 is that we\nneed to determine . Unless this is easier than\ndetermining  directly, then we are back to square\none. In some cases, we can choose values of  that\nhave worked well for similar data sets in the past. For\nexample, if we periodically cluster news stories from a\nnewswire, there is likely to be a fixed value of \nthat gives us the right  in each successive\nclustering. In this application, we would not be able to determine \nbased on past experience since  changes.\n\n\nA theoretical justification for Equation 195 is the \n\n Akaike Information Criterion  or AIC, an\ninformation-theoretic measure that trades off distortion against model complexity.\nThe general form of AIC is:\n\n\n\n\n\n\n(196)\n\n\nwhere , the negative maximum log-likelihood of the data for\n clusters, is a measure of distortion  and , the number of parameters of a\nmodel with  clusters, is a measure of model\ncomplexity. We will not attempt to derive the AIC here, but\nit is easy to understand intuitively. The first property of a good\nmodel of the data is that each data point is modeled well by\nthe model. This is the goal of low distortion. But\nmodels should also be small (i.e., have low model\ncomplexity) since a model that merely\ndescribes the data (and therefore has zero distortion) is\nworthless. \nAIC provides a theoretical justification for one particular\nway of weighting these two factors,\ndistortion and model complexity, when selecting a model.\n\n\nFor  -means, the AIC can be stated as follows:\n\n\n\n\n\n\n(197)\n\n\nEquation 197  is a special case\nof Equation 195 for .\n\n\nTo derive Equation 197 from Equation 196 observe that\n in  -means since each element of the \ncentroids is a parameter that can be varied\nindependently; and that\n\n (modulo a constant) if we view the model\nunderlying  -means as a Gaussian mixture with hard\nassignment, uniform\ncluster priors and identical spherical covariance matrices\n(see Exercise 16.7 ).\n\n\nThe derivation of AIC is based on a number of\nassumptions,  e.g., that the data are \n\n\n  .\nThese assumptions are only approximately true for data sets\nin information retrieval.\nAs a\nconsequence, the AIC can rarely be applied without modification in text clustering. In Figure 16.8 , the\ndimensionality of the vector space is  50,000. \nThus, \n dominates the smaller RSS-based\nterm (\n, not shown in the figure) and the minimum\nof the expression is reached for .\nBut as we know,  (corresponding to the four classes\nChina,\nGermany, Russia and Sports) is a better\nchoice than .\nIn practice,\nEquation 195 is often\nmore useful than Equation 197 -  with the caveat that we need\nto come up with an estimate for\n.\n\n\nExercises.\n\nWhy are documents that do not use the same term\nfor the concept car likely to end up in the same\ncluster in  -means clustering?\n\n\n\n  \nTwo of the possible termination conditions for  -means were\n(1) assignment does not change, (2) centroids do not\nchange (page 16.4 ). Do these two conditions\nimply each other?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Model-based clustering\n Up: K-means\n Previous: K-means\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}