{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/linear-versus-nonlinear-classifiers-1.html",
  "title": "Linear versus nonlinear classifiers",
  "body": "\n\n\n\n\nLinear versus nonlinear classifiers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Classification with more than\n Up: Vector space classification\n Previous: Time complexity and optimality\n    Contents \n    Index\n\n\n\n\n \n\nLinear versus nonlinear classifiers\n\n\nIn this section, we show that the two learning methods Naive\nBayes and Rocchio are instances of linear classifiers, the\nperhaps most important group of text classifiers, and\ncontrast them with nonlinear classifiers.  To simplify the\ndiscussion, we will only consider two-class classifiers in\nthis section and define a  linear classifier  as a\ntwo-class classifier that decides class membership by\ncomparing a linear combination of the features to a\nthreshold.\n\n\n\n\nFigure 14.8:\nThere are an infinite number of\nhyperplanes that separate two linearly separable classes.\n\n\n\n\nIn two dimensions, a linear classifier is a line. Five\nexamples are shown in Figure 14.8 . These lines have\nthe functional form \n.  The classification\nrule of a linear classifier is to assign a document to \nif \n and to  if\n\n.  Here, \n is the\ntwo-dimensional vector representation of the document and\n\n is the parameter vector that defines (together\nwith ) the decision boundary. An alternative geometric\ninterpretation of a linear classifier is provided\nin Figure 15.7 (page ).\n\n\nWe can generalize this 2D linear classifier to higher\ndimensions by defining a hyperplane as we did in\nEquation 140, repeated here as\nEquation 144:\n\n\n\n\n\n\n(144)\n\n\nThe assignment criterion then is:\nassign to  if \n and to\n if \n.  We\ncall a hyperplane that we use as a linear classifier a\n  decision hyperplane .\n\n\n\n\nFigure 14.9:\nLinear classification algorithm.\n\n\n\n\nThe corresponding algorithm for linear classification in  dimensions is\nshown in Figure 14.9 .\nLinear classification at first seems trivial given the\nsimplicity of this algorithm. However,\nthe difficulty is in training the linear classifier, that\nis, in determining the parameters \nand  based on the training set. In general, some learning\nmethods compute much better parameters than others where our\ncriterion for evaluating the quality of a learning method is\nthe effectiveness of the learned linear classifier on new data.\n\n\nWe now show that Rocchio and Naive Bayes are linear classifiers.\nTo see this for Rocchio,\nobserve that a vector  is on the decision boundary if it\nhas equal distance\nto the two class centroids:\n\n\n\n\n\n \n \n\n(145)\n\n\nSome basic arithmetic shows that\nthis corresponds to a\nlinear classifier with normal vector \n and \n (Exercise 14.8 ).\n\n\nWe can derive the linearity of Naive Bayes from its decision\nrule, which chooses the category  with the largest\n\n (Figure 13.2 ,\npage 13.2 ) where:\n\n\n\n\n\n\n(146)\n\n\nand  is the number of tokens in the document that\nare part of the vocabulary.\nDenoting the complement category as , we obtain\nfor the log odds:\n\n\n\n\n\n \n \n\n(147)\n\n\n\nWe choose class  if the odds are greater than 1 or,\nequivalently, if the log odds are greater than 0. It is\neasy to see that\nEquation 147 is an instance of Equation 144 for \n\n,  number of\noccurrences of  in , \nand\n\n.\nHere, the index , \n, refers to terms of\nthe vocabulary (not to positions in  as  does; cf. variantmultinomial) and\n and  are -dimensional vectors.\nSo in log space, Naive Bayes is a linear\nclassifier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprime\n0.70\n0\n1\ndlrs\n-0.71\n1\n1\n\nrate\n0.67\n1\n0\nworld\n-0.35\n1\n0\n\ninterest\n0.63\n0\n0\nsees\n-0.33\n0\n0\n\nrates\n0.60\n0\n0\nyear\n-0.25\n0\n0\n\ndiscount\n0.46\n1\n0\ngroup\n-0.24\n0\n0\n\nbundesbank\n0.43\n0\n0\ndlr\n-0.24\n0\n0\n\n\nA linear classifier.\nThe dimensions  and parameters  of a linear\nclassifier for the class interest (as in\ninterest rate) in Reuters-21578. The threshold is . Terms like\ndlr and world have negative weights\nbecause they are indicators for the competing class currency.\n \n\n\n\nWorked example.\nTable 14.4  defines a linear\nclassifier for the category interest in Reuters-21578 (see\nSection 13.6 , page 13.6 ). We assign document\n ``rate discount dlrs\nworld'' to interest since\n\n. \nWe assign \n ``prime dlrs'' to the complement\nclass (not in interest) since \n.\nFor\nsimplicity, we assume a simple binary vector representation\nin this example: 1 for occurring terms, 0 for non-occurring\nterms.\nEnd worked example.\n\n\n\nA linear problem with noise.\nIn this hypothetical web page classification scenario,\nChinese-only web pages are solid circles and\nmixed Chinese-English web pages  are squares. The two\nclasses are separated by a linear class boundary (dashed\nline, short dashes), except for three noise documents (marked with arrows).\n\n\n\nFigure 14.10 \nis a graphical example of a\nlinear problem, which we define\nto mean that the underlying distributions  and\n\n of the two classes are separated by a\nline. We call this separating line the  class boundary . It is\nthe ``true'' boundary of the two classes and we distinguish\nit from the  decision boundary  that the\nlearning method computes to approximate the class boundary.\n\n\nAs is typical in text classification, there are\nsome  noise\ndocuments  in Figure 14.10  (marked with arrows) that do not fit well into\nthe overall distribution of the classes.  In\nSection 13.5  (page 13.5 ), we defined a\nnoise feature as a misleading feature that, when included in\nthe document representation, on average increases the\nclassification error. Analogously, a noise document is a\ndocument that, when included in the training set, misleads\nthe learning method and increases classification error.\nIntuitively, the underlying distribution partitions the\nrepresentation space into areas with mostly homogeneous\nclass assignments. A document that does not conform with the\ndominant class in its area is a noise document.\n\n\nNoise documents are one reason why training a linear\nclassifier is hard. If we pay too much attention to\nnoise documents when choosing the decision hyperplane of the\nclassifier, then it will be inaccurate on new data. More\nfundamentally, it is usually difficult to determine which documents are\nnoise documents and therefore potentially misleading.\n\n\nIf there exists a hyperplane that perfectly separates the\ntwo classes, then we call the two classes \n linearly separable .\nIn fact,\nif linear separability holds, then there is an infinite\nnumber of linear separators (Exercise 14.4 ) as\nillustrated by Figure 14.8 , where \nthe number of possible separating hyperplanes is infinite.\n\n\nFigure 14.8  illustrates another challenge in\ntraining a linear classifier. If we are dealing with a\nlinearly separable problem, then we need a \ncriterion for selecting among all decision hyperplanes that\nperfectly separate the training data. In general, some of these hyperplanes will\ndo well on new data, some will not.\n\n\n\n\nFigure 14.11:\nA nonlinear problem.\n\n\n\n\nAn example of a  nonlinear classifier  is kNN. The\nnonlinearity of kNN  is intuitively clear when\nlooking at examples like Figure 14.6 . The decision boundaries\nof kNN (the double lines in Figure 14.6 ) are locally linear segments, but in general\nhave a complex shape that is not equivalent to a line in 2D or\na hyperplane in higher dimensions.\n\n\nFigure 14.11  is another example of a\nnonlinear problem: there is\nno good linear separator between the distributions \nand \n because of the circular ``enclave''\nin the upper left part of the graph.  Linear classifiers\nmisclassify the enclave, whereas a nonlinear classifier like\nkNN will be highly accurate for this type of problem if the\ntraining set is large enough.\n\n\nIf a problem is nonlinear and its class\nboundaries cannot be approximated well with linear\nhyperplanes, then nonlinear classifiers are often more\naccurate than linear classifiers.\nIf a problem is linear, it is best to use a simpler linear\nclassifier. \n\n\nExercises.\n\n   Prove that the number of\nlinear separators of two classes is either infinite or zero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Classification with more than\n Up: Vector space classification\n Previous: Time complexity and optimality\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}