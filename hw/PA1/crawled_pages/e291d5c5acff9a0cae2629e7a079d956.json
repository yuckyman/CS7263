{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/implementation-notes-1.html",
  "title": "Implementation notes",
  "body": "\n\n\n\n\nImplementation notes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Hierarchical clustering\n Previous: Cluster labeling\n    Contents \n    Index\n\n\n\n\n \n\nImplementation notes\n  Most problems that require the\ncomputation of a large number of dot products benefit from\nan inverted index. This is also the case for HAC\nclustering. Computational savings due to the inverted index\nare large if there are many zero similarities - either\nbecause many documents do not share any terms or because an\naggressive stop list is used.\n\n\nIn low dimensions, more aggressive optimizations are\npossible that make the computation of most pairwise\nsimilarities unnecessary (Exercise 17.10 ). However,\nno such algorithms\nare known in higher dimensions. We encountered the same\nproblem in kNN classification (see Section 14.7 ,\npage 14.7 ).\n\n\nWhen using GAAC on a large document set in high\ndimensions, we have to take care to avoid dense\ncentroids. For dense centroids, clustering can take time\n\n where  is the size of the\nvocabulary, whereas complete-link clustering is \n where  is the average size of the vocabulary of a\ndocument. So for large vocabularies complete-link clustering\ncan be\nmore efficient than an unoptimized implementation of GAAC.\nWe discussed this problem in the context of  -means\nclustering in Chapter 16  (page 16.4 ) and\nsuggested two solutions: truncating centroids (keeping only\nhighly weighted terms) and representing clusters by means of\nsparse medoids instead of dense centroids. These optimizations\ncan also be applied to GAAC and centroid clustering.\n\n\nEven with these optimizations, HAC algorithms are all\n or \n and therefore\ninfeasible for large \nsets of 1,000,000 or more documents.\nFor such large sets, HAC can only be used in\ncombination with a flat clustering algorithm like\n -means. Recall that  -means requires a set of seeds as\ninitialization (Figure 16.5 , page 16.5 ). If\nthese seeds are badly chosen, then the resulting clustering\nwill be of poor quality.  We can employ an HAC algorithm to\ncompute seeds of high quality.  If the HAC algorithm is\napplied to a document subset of size , then the\noverall runtime of  -means cum HAC seed generation is\n. This is because the application of a quadratic\nalgorithm to a sample of size  has an overall\ncomplexity of . An appropriate adjustment can be\nmade for an \n algorithm to guarantee\nlinearity.  This algorithm is referred to as the\n  Buckshot algorithm .  It\ncombines the determinism and higher reliability of HAC with\nthe efficiency of  -means.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Hierarchical clustering\n Previous: Cluster labeling\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}