{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/indirect-relevance-feedback-1.html",
  "title": "Indirect relevance feedback",
  "body": "\n\n\n\n\nIndirect relevance feedback\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Summary\n Up: Relevance feedback and pseudo\n Previous: Pseudo relevance feedback\n    Contents \n    Index\n\n\n\n \n\nIndirect relevance feedback\n\n\nWe can also use indirect sources of evidence rather than explicit\nfeedback on relevance as the basis for relevance feedback. This is\noften called  implicit (relevance)\n  feedback . Implicit feedback is less\nreliable than explicit feedback, but is more useful than pseudo\nrelevance feedback, which contains no evidence of user judgments.\nMoreover, while users are often reluctant to provide explicit feedback,\nit is easy to collect implicit feedback in large quantities for a high\nvolume system, such as a web search engine.\n\n\nOn the web, DirectHit introduced the idea of ranking more highly documents that users chose to look at more often. In other words, clicks on links were assumed to indicate that the page was likely relevant to the query. This approach makes various assumptions, such as that the document summaries displayed in results lists (on whose basis users choose which documents to click on) are indicative of the relevance of these documents. In the original DirectHit search engine, the data about the click rates on pages was gathered globally, rather than being user or query specific. This is one form of the general area of  clickstream mining . Today, a closely related approach is used in ranking the advertisements that match a web search query (Chapter 19 ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Summary\n Up: Relevance feedback and pseudo\n Previous: Pseudo relevance feedback\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}