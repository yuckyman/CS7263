{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/designing-parsing-and-scoring-functions-1.html",
  "title": "Designing parsing and scoring functions",
  "body": "\n\n\n\n\nDesigning parsing and scoring functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Putting it all together\n Up: Components of an information\n Previous: Query-term proximity\n    Contents \n    Index\n\n\n\n\n \n\nDesigning parsing and scoring functions\n \nCommon search interfaces, particularly for consumer-facing\nsearch applications on the web, tend to mask query operators\nfrom the end user. The intent is to hide the complexity of\nthese operators from the largely non-technical audience for\nsuch applications, inviting  free text queries . Given such interfaces, how should a search equipped with indexes for various retrieval operators treat a query such as rising interest rates?  More generally, given the various factors we have studied that could affect the score of a document, how should we combine these features?\n\n\nThe answer of course depends on the user population, the query distribution and the collection of documents. Typically, a query parser is used to translate the user-specified keywords into a query with various operators that is executed against the underlying indexes. Sometimes, this execution can entail multiple queries against the underlying indexes; for example, the query parser may issue a stream of queries:\n\n\nRun the user-generated query string as a phrase query. Rank them by vector space scoring using as query the vector consisting of the 3 terms rising interest rates.\n\nIf fewer than ten documents contain the phrase rising interest rates, run the two 2-term phrase queries rising interest and interest rates; rank these using vector space scoring, as well.\n\nIf we still have fewer than ten results, run the vector space query consisting of the three individual query terms.\n\n\nEach of these steps (if invoked) may yield a list of scored documents, for each of which we compute a score. This score must combine contributions from vector space scoring, static quality, proximity weighting and potentially other factors - particularly since a document may appear in the lists from multiple steps.  This demands an aggregate scoring function that  accumulates evidence  of a document's relevance from multiple sources. How do we devise a query parser and how do we devise the aggregate scoring function?\n\n\nThe answer depends on the setting.  In many enterprise settings we have application builders who make use of a toolkit of available scoring operators, along with a query parsing layer, with which to manually configure the scoring function as well as the query parser.  Such application builders make use of the available zones, metadata and knowledge of typical documents and queries to tune the parsing and scoring.  In collections whose characteristics change infrequently (in an enterprise application, significant changes in collection and query characteristics typically happen with infrequent events such as the introduction of new document formats or document management systems, or a merger with another company).  Web search on the other hand is faced with a constantly changing document collection with new characteristics being introduced all the time. It is also a setting in which the number of scoring factors can run into the hundreds, making hand-tuned scoring a difficult exercise.  To address this, it is becoming increasingly common to use machine-learned scoring, extending the ideas we introduced in Section 6.1.2 , as will be discussed further in Section 15.4.1 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Putting it all together\n Up: Components of an information\n Previous: Query-term proximity\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}