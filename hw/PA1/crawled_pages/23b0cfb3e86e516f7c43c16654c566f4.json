{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html",
  "title": "Tf-idf weighting",
  "body": "\n\n\n\n\nTf-idf weighting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The vector space model\n Up: Term frequency and weighting\n Previous: Inverse document frequency\n    Contents \n    Index\n\n\n\n\nTf-idf weighting\n\n\nWe now combine the definitions of term frequency and inverse document frequency, to produce a composite weight for each term in each document. The  tf-idf  weighting scheme assigns to term  a weight in document  given by\n\n\n\n\n\n\n\n\n(22)\n\n\n\nIn other words, \n assigns to term  a weight in document  that is\n\n\n\nhighest when  occurs many times within a small number of documents (thus lending high discriminating power to those documents);\n\n\n\n\nlower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal);\n\n\n\n\nlowest when the term occurs in virtually all documents.\n\n\n\n\n\n \nAt this point, we may view each document as a  vector  with one component corresponding to each term in the dictionary, together with a weight for each component that is given by (22). For dictionary terms that do not occur in a document, this weight is zero.  This vector form will prove to be crucial to scoring and ranking; we will develop these ideas in Section 6.3 . As a first step, we introduce the overlap score measure: the score of a document  is the sum, over all query terms, of the number of times each of the query terms occurs in . We can refine this idea so that we add up not the number of occurrences of each query term  in , but instead the tf-idf weight of each term in .\n\n\n\n\n\n\n(23)\n\n\nIn Section 6.3  we will develop a more rigorous form of Equation 23.\n\n\nExercises.\n\nWhy is the idf of a term always finite?\n\n\n\nWhat is the idf of a term that occurs in every document? Compare this with the use of stop word lists.\n\n\n\nConsider the table of term frequencies for 3 documents denoted Doc1, Doc2, Doc3 in Figure 6.9 .\n\n\n\nFigure 6.9:\nTable of tf values for Exercise 6.2.2.\n\n\n\nCompute the tf-idf weights for the terms car, auto, insurance, best, for each document, using the idf values from Figure 6.8 .\n\n\n\nCan the tf-idf weight of a term in a document exceed 1?\n\n\n\n  \nHow does the base of the logarithm in (21) affect the score calculation in (23)? How does the base of the logarithm affect the relative scores of two documents on a given query?\n\n\n\nIf the logarithm in (21) is computed base 2, suggest a simple approximation to the idf of a term.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The vector space model\n Up: Term frequency and weighting\n Previous: Inverse document frequency\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}