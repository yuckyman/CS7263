{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/exercises-2.html",
  "title": "Exercises",
  "body": "\n\n\n\n\nExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Support vector machines and\n Up: Vector space classification\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n\nExercises\n\n\n\n\n\n\nExercises.\n\n   In Figure 14.13 , which of the three\nvectors , , and  is (i)\nmost similar to  according to dot product similarity,\n(ii) most similar to  according to cosine similarity,\n(iii) closest to  according to Euclidean distance?\n\n\n\n  \nDownload Reuters-21578 and train and test Rocchio\nand kNN classifiers for the classes\nacquisitions,\ncorn,\ncrude,\nearn,\ngrain,\ninterest,\nmoney-fx,\nship,\ntrade, and\nwheat.\nUse the ModApte split.\nYou may want to use one of a number of software packages that\nimplement Rocchio classification and kNN classification, for\nexample, the Bow toolkit (McCallum, 1996).\n\n\n\nDownload 20 Newgroups (page 8.2 ) and train and test Rocchio\nand kNN classifiers for its 20 classes.\n\n\n\nShow that the decision boundaries in Rocchio\nclassification are, as in kNN, given by the Voronoi tessellation.\n\n\n\n Computing the distance between a dense centroid\nand a sparse vector is  for a naive implementation\nthat iterates over all  dimensions. Based on the equality\n\n\nand assuming that  has been precomputed,\nwrite down an algorithm that is \n instead, where\n is the number of distinct terms in the test document.\n\n\n\nProve that the region of the plane consisting of all points\nwith the same  nearest neighbors is a convex polygon.\n\n\n\n  \nDesign an algorithm that performs an efficient 1NN search in\n1 dimension (where efficiency is with respect to the number\nof documents ). What is the time complexity of the algorithm?\n\n\n\nDesign an algorithm that performs an efficient 1NN search in\n2 dimensions with at most polynomial (in ) preprocessing time.\n\n\n\n Can one design an exact efficient algorithm for\n1NN for very large  along the ideas you used to solve the\nlast exercise? \n\n\n\n  \nShow that \nEquation 145 defines a hyperplane\nwith \n and \n.\n\n\n\n\nFigure 14.14:\nA simple non-separable set of points.\n\n\n\n\n\nWe can easily construct non-separable data sets in high\ndimensions by embedding a non-separable set like the one\nshown in Figure 14.14 . \nConsider  embedding Figure 14.14  in 3D and then  perturbing the\n4 points slightly (i.e., moving them a small\ndistance in a random direction).\nWhy would you expect the resulting\nconfiguration to\nbe linearly separable?\nHow likely is then a non-separable set of  points\nin -dimensional space?\n\n\n\n   \nAssuming two classes, show that the percentage of\nnon-separable assignments of the vertices of a hypercube\ndecreases with dimensionality  for . For example,\nfor  the proportion of non-separable assignments is 0,\nfor , it is . One of the two non-separable cases\nfor  is shown in\nFigure 14.14 , the other is its mirror image. Solve\nthe exercise either analytically or by simulation. \n\n\n\nAlthough we point out the similarities of Naive Bayes\nwith linear vector space classifiers, it does not make\nsense to represent count vectors (the document\nrepresentations in NB) in a continuous vector\nspace. There is however a formalization of NB that is\nanalogous to Rocchio. Show that NB assigns a document to the\nclass (represented as a parameter vector) whose\nKullback-Leibler (KL) divergence\n(Section 12.4 , page 12.4 )\nto the document (represented as a count\nvector as in Section 13.4.1 (page ), normalized to sum to 1) is\nsmallest.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Support vector machines and\n Up: Vector space classification\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}