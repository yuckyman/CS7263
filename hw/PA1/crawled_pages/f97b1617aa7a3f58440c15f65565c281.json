{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/large-and-difficult-category-taxonomies-1.html",
  "title": "Large and difficult category taxonomies",
  "body": "\n\n\n\n\nLarge and difficult category taxonomies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Features for text\n Up: Improving classifier performance\n Previous: Improving classifier performance\n    Contents \n    Index\n\n\n\n\n \n\nLarge and difficult category taxonomies\n\n\nIf a text classification problem consists of a small number of\nwell-separated categories, then many classification algorithms are\nlikely to work well.  But many real classification problems consist of\na very large number of often very similar categories. The reader might\nthink of examples like web directories (the Yahoo! Directory or the\nOpen Directory Project), library classification schemes (Dewey Decimal\nor Library of Congress) or the classification schemes used in legal or\nmedical applications.  For instance, the Yahoo! Directory consists of over 200,000 categories in a deep hierarchy.  Accurate classification over large sets of\nclosely related classes is inherently difficult.\n\n\nMost large sets of categories have a hierarchical structure, and\nattempting to exploit the hierarchy by doing\n hierarchical classification  is a \npromising approach.  \nHowever, at present the effectiveness gains from doing this\nrather than just working with the classes that are the leaves of the\nhierarchy remain modest.But the technique can be very useful simply to improve\nthe scalability of building classifiers over large hierarchies.  \nAnother simple way to improve the scalability of\nclassifiers over large hierarchies is the use of aggressive feature\nselection. \nWe provide references to some work on hierarchical classification\nin Section 15.5 .\n\n\nA general result in machine learning is that you can\nalways get a small boost in classification accuracy by combining\nmultiple classifiers, provided only that the mistakes that they make\nare at least somewhat independent.  There is now a large literature on\ntechniques such as voting, bagging, and boosting multiple\nclassifiers.  Again, there are some pointers in the references.\nNevertheless, ultimately a hybrid automatic/manual solution may be\nneeded to achieve sufficient classification accuracy.  A common\napproach in such situations is to run a classifier first, and to\naccept all its high confidence decisions, but to put low confidence\ndecisions in a queue for manual review.  Such a process also\nautomatically leads to the production of new training data which can\nbe used in future\nversions of the machine learning classifier.  However, note that this\nis a case in point where the resulting training data is clearly\nnot randomly sampled from the space of documents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Features for text\n Up: Improving classifier performance\n Previous: Improving classifier performance\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}