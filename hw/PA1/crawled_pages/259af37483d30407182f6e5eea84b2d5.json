{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/footnode.html",
  "title": "Footnotes",
  "body": "\n\n\n\n\nFootnotes\n\n\n\n\n\n\n\n\n\n\n\n\n...\nemail.\nIn modern parlance, the word ``search'' has tended to replace\n    ``(information) retrieval''; the term ``search'' is quite ambiguous,\n    but in context we use the two synonymously.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... it.\nFormally, we take the transpose of the\n  matrix to be able to get the terms as column vectors.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... retrieval.\nSome information\n  retrieval researchers prefer the term inverted\n    file, but expressions like\n  index construction and index compression\nare much more common than inverted file construction\nand inverted file compression. For consistency, we\nuse (inverted) index throughout this book.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nIn a (non-positional) inverted index, a posting is just a document ID, but it is inherently associated with a term, via the postings list it is placed on; sometimes we will also talk of a (term, docID) pair as a posting.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... merged.\nUnix users can note that these steps are similar to use of\n   the sort and then uniq commands.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... collection.\nThe notation  is used to express an asymptotically\n    tight bound on the complexity of an algorithm.  Informally, this is\n    often written as , but this notation really expresses an\n    asymptotic upper bound, which need not be tight\n    (Cormen et al., 1990).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... ,\nA classifier is a function that takes objects of some sort\n    and assigns them to one of a number of distinct classes (see Chapter 13 ).  Usually\n    classification is done by machine learning methods such as probabilistic models,\n    but it can also be done by hand-written rules.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nThat is, as defined here, tokens that are\n  not indexed (stop words) are not terms, and if multiple\n  tokens are collapsed together via normalization, they are\n  indexed as one term, under the normalized form.\n  However, we later relax this definition when discussing classification and\n  clustering in nbayeslsi, where there is no index. In these\n  chapters, we drop the requirement of inclusion in the dictionary.\n  A term means a normalized word.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... query.\nFor the free text case, this is straightforward.  The Boolean case is\n    more complex: this tokenization may produce multiple terms from one \n    query word.  This can be handled by combining the terms with an AND\n    or as a phrase query phrasequery.\n    It is harder for a system to handle the opposite case where the user\n    entered as two terms something that was tokenized together in \n    the document processing.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... tokens.\nIt is also often referred to as  term normalization ,\n  but we prefer to reserve the name term for the output of the\n  normalization process.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\ndocuments.\nAt the time we wrote this chapter (Aug. 2005), this was actually\n   the case on \n   Google: the top result for the query C.A.T. was a site\n   about cats, the Cat Fanciers Web Site http://www.fanciers.com/.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... part-of-speech-tagging.\nPart of speech taggers classify words as nouns, verbs, etc. - or,\n    in practice, often as finer-grained classes like ``plural proper\n    noun''.  Many fairly accurate (c. 96% per-tag accuracy) part-of-speech\n    taggers now exist, \n    usually trained by machine learning methods on hand-tagged text.\n    See, for instance, Manning and Schütze (1999,  ch. 10).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... maintain.\nSo-called perfect hash functions are designed to preclude collisions, but are rather more complicated both to implement and to compute.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...clusters \n A cluster  \nin this chapter is\na group of tightly coupled computers that work together\nclosely. This sense of the word is different from the use of\ncluster as a group of\ndocuments that are semantically similar in flatclustlsi.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... structure\nSee,\nfor example, (Cormen et al., 1990, Chapter 19).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... list.\nNote that the origin\nis 0 in the table. Because we never need to encode a docID or\na gap of 0, in practice the origin is usually 1, so that\n10000000 encodes 1, 10000101 encodes 6 (not 5 as in the\ntable), and so on. \n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nremoved.\nWe assume here that  has no leading\n0s. If there are any, they are\nremoved  before deleting the leading 1.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\ndistribution\nReaders who want to review basic\nconcepts of probability theory\nmay want to consult\nRice (2006) or\nRoss (2006). Note that we are interested in\nprobability distributions over integers (gaps, frequencies,\netc.), but that the coding properties of a probability\ndistribution are independent of whether the outcomes are\nintegers or something else.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nNote that, unfortunately, the conventional symbol for both\nentropy and harmonic number is . Context should make\nclear which is meant in this chapter.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... all,\nA system may not fully order all documents in the collection in\n    response to a query or at any rate an evaluation exercise may be\n    based on submitting only the top  results for each information need.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... agreement.\nFor a contingency table, as in Table 8.2 , a marginal statistic\n   is formed by summing a \n   row or column.  The marginal \n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... need.\nThere are exceptions, in domains where recall is \n  emphasized. For instance, in many legal disclosure cases, a legal associate\n  will review every document that matches a keyword search. \n  \n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... find:\n\nIn the equation,  \n returns a value of  which\n   maximizes the value of the function .\nSimilarly, \n returns a value of  which\n   minimizes the value of the function .\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nIn most modern database\nsystems, one can enable full-text search for text columns.\nThis usually means that an inverted index is created and\nBoolean or vector space search enabled, effectively\ncombining core database with information retrieval\ntechnologies.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nThe\n  representation is simplified in a number of respects. For\n  example, we\n  do not show the root node and text is not\n  embedded in text nodes. See http://www.w3.org/DOM/.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... war.\nTo represent the semantics\n  of NEXI queries fully we would also need to designate one node\n  in the tree as a ``target node'', for example, the\n  section in the tree in Figure 10.3 . Without the\n  designation of a target node, the tree in Figure 10.3  is not\n  a search for sections embedded in articles (as specified\n  by NEXI), but a search for articles that contain sections.\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... hold.\nThe term likelihood is just a synonym of\n    probability. It is the probability of an event or data\n    according to a model.  The term is usually used when people\n    are thinking of holding the data fixed, while varying the model.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nautomaton.\nFinite automata can have outputs attached to either their\n    states or their arcs; we use states here, because that maps\n    directly on to the way probabilistic automata are usually formalized.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... documents.\nIn the IR context that we are leading up to, taking the\n   stop probability to be fixed across models seems reasonable.\n   This is because we\n   are generating queries, and the length distribution of queries is\n   fixed and independent of the document from which we are generating\n   the language model.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... collection.\n\n Of course, in other cases, they do not.  The answer to this within the \nlanguage modeling approach is translation language models, as briefly discussed in Section 12.4 .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... distribution.\nIn the context of probability theory, (re)normalization\n    refers to summing numbers that cover an event space and dividing\n    them through by their sum, so that the result is a probability\n    distribution which sums to 1.  This is distinct from both the\n    concept of term normalization in Chapter 2  and\n    the concept of length normalization in Chapter 6 , which is\n    done with a  norm.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... model.\nIt is also referred to as Jelinek-Mercer smoothing.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n....\nWe will explain in the next section\nwhy  is proportional to (), not equal to the quantity on\nthe right.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n....\nOur assumption here\n  is that the length of test documents is bounded.  would exceed  for extremely long\ntest documents.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nmagnitude.\nIn fact, if the length of documents is not\n  bounded, the number of parameters in the multinomial case\n  is infinite.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... symbol.\nOur terminology is\n  nonstandard. The random variable  is a\n  categorical variable, not a multinomial variable, and the\ncorresponding  NB model should perhaps be called a\n  sequence model . We have chosen to present this sequence model and the multinomial model in Section 13.4.1  as the same\nmodel because they are computationally identical.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n....\nTake care not to confuse expected mutual\ninformation with pointwise mutual information, which is\ndefined as \n where \n\n and  are defined as in \nEquation 133.\nThe two\nmeasures have different properties. See\nSection 13.7 .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nFeature scores were computed on the first\n100,000 documents, except for poultry, a rare class, for\nwhich 800,000 documents were used. We have omitted\nnumbers and other special words from the top\nten lists.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nReuters-RCV1.\nWe trained the classifiers on the first\n100,000 documents and computed  on the next\n100,000. The graphs are averages over five classes.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... wrong.\nWe can make this\ninference because,\nif the two events are\nindependent, then \n, where  is the\n distribution. See, for example,\n Rice (2006).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nvector\nRecall from basic linear algebra that\n\n, i.e., the\ndot product of  and  equals the product\nby matrix multiplication of the transpose of  and\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nclassification.\nWe\nwrite \n\n\nfor  \nand assume\nthat the length of\n  test\n  documents is bounded as we did on\n  page 13.2 .\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... ).\n\nThe generalization of a polygon to higher dimensions is a\n polytope . A polytope is a region in -dimensional space\nbounded by -dimensional hyperplanes.\nIn  dimensions, the decision boundaries for kNN \nconsist of segments of \n-dimensional hyperplanes that form\nthe Voronoi tessellation into convex polytopes for the training set\nof documents. The decision criterion of assigning a document\nto the majority class of its  nearest neighbors\napplies equally to  (tessellation into polygons) and \n(tessellation into polytopes).\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...polytomous \nA synonym\nof polytomous is polychotomous.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... it.\nAs discussed in Section 14.1 (page ), we present the general case of points\n  in a vector space, but if the points are length normalized document vectors,\n  then all the action is taking place on the surface of a unit sphere,\n  and the decision surface intersects the sphere's surface.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... gives:\nRecall that \n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nWe\nwrite \n\n\nfor  (page 13.2 ) and assume\nthat the length of\n  test\n  documents is bounded as we did on\n  page 13.2 .\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nSVM\\@.\nMaterializing the features refers to directly calculating higher\n   order and interaction terms and then putting them into a linear model.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nThese results are in terms of the break-even  \n   (see Section 8.4 ).  Many researchers disprefer \n   this measure for text classification evaluation, since its\n   calculation may involve interpolation rather than an actual\n   parameter setting of the system and it is not clear why this value\n   should be reported rather than maximal  or another point on\n   the precision/recall curve motivated by the task at hand.\n   While earlier results in (Joachims, 1998) suggested notable gains \n   on this task from the use\n   of higher order polynomial or rbf kernels, this was with\n   hard-margin SVMs.  With soft-margin SVMs, a simple linear SVM with\n   the default  performs best.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... modest.\nUsing the small hierarchy in Figure 13.1 (page ) as an\n   example, the leaf classes are ones like poultry and\n   coffee, as opposed to higher-up classes like industries.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nfeasible.\nAn upper bound on the number of\n  clusterings is .  The\nexact number of different partitions of  documents into\n clusters is the Stirling number of the second kind.  See\nhttp://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html\nor Comtet (1974).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... .\nRecall our note of caution from\nFigure 14.2  (page 14.2 ) when looking at\nthis and other 2D figures in this and the following\nchapter: these illustrations\ncan be misleading because 2D projections of length-normalized\nvectors distort\nsimilarities and distances between points.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nmodel.\n is the random variable we\ndefined in Section 13.3  (page 13.4 ) for the\nBernoulli Naive Bayes model. It takes the values 1 (term\n is present in the document) and 0 (term\n is absent in the document).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... EM.\nFor example, this problem is common when EM is used to\n  estimate parameters of\nhidden Markov models, probabilistic grammars, and machine\ntranslation models \nin natural\nlanguage processing\n(Manning and Schütze, 1999).\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nclustering.\nIn this chapter, we only consider\n  hierarchies\nthat are\nbinary\ntrees like the one shown in Figure 17.1  - but hierarchical\nclustering can be easily extended to other types of trees.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... updated.\nWe assume\n  that we use a deterministic method for breaking ties,\n  such as always choose the merge that is the first cluster with respect to a\n  total ordering of the subsets of the document set\n  .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... (a))\nThroughout this chapter, we equate\n  similarity with proximity in 2D depictions of clustering.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nsimilarity.\nIf you are bothered by the possibility\nof ties, assume that  has coordinates\n\n and that all other points have\ninteger coordinates.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nlabeling.\nSelecting the most frequent terms is \na non-differential \nfeature selection technique we discussed in Section 13.5 .\nIt can also be used for labeling clusters.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... 2.1.\nCf. Zipf's law of the distribution of words in text in Chapter 5  (page 5.2 ), which is a power law with .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... host\nThe number of hosts is assumed to far exceed .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... cluster\nPlease note the different usage of ``clusters'' elsewhere in this book, in the sense of Chapters 16 17 .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n... graph\nThis is consistent with our usage of  for the number of documents in the collection.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n...\nNote that  represents  raised to the th power, not the transpose of  which is denoted .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n"
}