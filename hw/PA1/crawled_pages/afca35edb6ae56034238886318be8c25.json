{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/choosing-what-kind-of-classifier-to-use-1.html",
  "title": "Choosing what kind of classifier to use",
  "body": "\n\n\n\n\nChoosing what kind of classifier to use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Improving classifier performance\n Up: Issues in the classification\n Previous: Issues in the classification\n    Contents \n    Index\n\n\n\n \n\nChoosing what kind of classifier to use\n\n\nWhen confronted with a need to build a text classifier, the first\nquestion to ask is how much training data is there currently\navailable?  None?  Very little? Quite a lot? Or a huge amount,\ngrowing every day?  Often one of the biggest practical challenges in fielding a\nmachine learning classifier in real applications \nis creating or obtaining enough training data. For\nmany problems and algorithms, hundreds or thousands of examples from each\nclass are required to produce a high performance\nclassifier and many real world contexts involve large sets of categories.\nWe will initially assume that the classifier is needed as\nsoon as possible; if a lot of time is available for implementation,\nmuch of it might be spent on assembling data resources.\n\n\nIf you have no labeled training data, and especially if there are existing\nstaff knowledgeable about the domain of the data, then you should\nnever forget the solution of using hand-written rules.  That is, you write \nstanding queries, as we touched on at the beginning\nof Chapter 13 . For example:\n\nif (wheat or grain) and not (whole\n  or bread) then \n\n\nIn practice, rules get a lot bigger than this, and can be phrased\nusing more sophisticated query languages than just Boolean\nexpressions, including the use of numeric scores.  With\ncareful crafting (that is, by humans tuning the rules on development\ndata), the accuracy of such rules can become very high.\nJacobs and Rau (1990) report identifying articles about takeovers with\n92% precision and 88.5% recall, and\nHayes and Weinstein (1990) report 94% recall and 84% precision over 675\ncategories on Reuters newswire documents.  Nevertheless the amount of\nwork to create such well-tuned rules is very large.  A reasonable\nestimate is 2 days per class, and extra time has to go into\nmaintenance of rules, as the content of documents in classes\ndrifts\nover time (cf. page 13.4 ).\n\n\nIf you have fairly little data and you are going to train a supervised\nclassifier, then machine learning theory says you should stick to a\nclassifier with high bias, as we discussed in Section 14.6 (page ).\nFor example, there are theoretical and empirical results\nthat Naive Bayes does well in such circumstances\n(Forman and Cohen, 2004, Ng and Jordan, 2001), although this effect is not necessarily\nobserved in practice with regularized models over textual data\n(Klein and Manning, 2002). At any rate, a very low bias model like\na nearest neighbor model is probably counterindicated. Regardless, the\nquality of the model will be adversely affected by the limited\ntraining data. \n\n\nHere, the theoretically interesting answer is to try to\napply  semi-supervised training\nmethods .  This includes methods such as bootstrapping or the \nEM algorithm, which we will introduce in Section 16.5 (page ).  In\nthese methods, the system gets some labeled documents, and a   \nfurther large supply of unlabeled documents over which it can attempt\nto learn.  One of the big advantages\nof Naive Bayes is that it can be straightforwardly extended to\nbe a semi-supervised learning algorithm, but for SVMs, there is also\nsemi-supervised learning work which goes under the title of\n transductive SVMs .  See the references for pointers.\n\n\nOften, the practical answer\nis to work out how to get more labeled data as quickly as you can.\nThe best way to do this is to insert yourself into a process where\nhumans will be willing to label data for you as part of their natural\ntasks.  For example, in many cases humans will sort or route email for\ntheir own purposes, and these actions give information about\nclasses. The alternative of getting human labelers expressly for the\ntask of training classifiers is often difficult to organize, and the\nlabeling is often of lower quality, because the labels are not\nembedded in a realistic task context.  Rather than getting people to\nlabel all or a random sample of documents, there has also been\nconsiderable research on  active learning , where a system is\nbuilt which decides which documents a human should label. Usually\nthese are the ones on which a classifier is uncertain of the correct\nclassification.  This can be effective in reducing annotation costs by\na factor of 2-4, but has the problem that the good documents to label\nto train one type of classifier often are not the good documents to\nlabel to train a different type of classifier.\n\n\nIf there is a reasonable amount of labeled data, then you are in the\nperfect position to use everything that we have presented about\ntext classification. For instance, you may wish to use an SVM.\nHowever, if you are deploying a linear classifier such as an SVM, you\nshould probably design an application that overlays a Boolean\nrule-based classifier over the machine learning classifier.  Users\nfrequently like to adjust things that do not come out quite\nright, and if management gets on the phone and wants the\nclassification of a particular document fixed right now, then this is\nmuch easier to do by hand-writing a rule than by working out how to\nadjust the weights of an SVM without destroying the overall\nclassification accuracy.  This is one reason why machine learning\nmodels like decision trees which produce user-interpretable\nBoolean-like models retain considerable popularity.\n\n\nIf a huge amount of data are available, then the choice of classifier\nprobably has little effect on your results and the best choice may be\nunclear (cf. Banko and Brill, 2001).  It may be best to choose a\nclassifier based on the scalability of training or even runtime \nefficiency.  To get to this point, you need to have huge amounts of\ndata. The general rule of thumb is that each doubling of the training\ndata size produces a linear increase in classifier performance, but\nwith very large amounts of data, the improvement becomes sub-linear.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Improving classifier performance\n Up: Issues in the classification\n Previous: Issues in the classification\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}