{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html",
  "title": "Naive Bayes text classification",
  "body": "\n\n\n\n\nNaive Bayes text classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Relation to multinomial unigram\n Up: Text classification and Naive\n Previous: The text classification problem\n    Contents \n    Index\n\n\n\n\n \n\nNaive Bayes text classification\n  The first supervised learning method\nwe introduce is  the  multinomial Naive Bayes \nor \n multinomial NB \nmodel, a probabilistic learning method. The probability of a document  being in\nclass  is computed as\n\n\n\n\n\n \n \n\n(113)\n\n\nwhere \n is the conditional probability of\nterm \n occurring in a document of class\n.We interpret \n\n as a measure of how much evidence\n\n contributes that  is the correct class.\n is the prior probability of a document occurring in\nclass . If a document's terms do not provide clear\nevidence for one class versus another, we choose the one that\nhas a higher prior probability.\n\n are the\ntokens in  that are part of the vocabulary we use for\nclassification and  is the number of such tokens in . For example, \n for the\none-sentence document Beijing and Taipei join\nthe WTO might be \n, with , if\nwe treat the terms and and the as stop words.\n\n\nIn text classification, our goal is to find the best\nclass for the document. The best class in NB classification\nis the\nmost likely or \n maximum a posteriori \n( MAP ) class :\n\n\n\n\n\n\n(114)\n\n\nWe write  for  because we do not know the true\nvalues of the parameters\n and\n\n, but estimate them from the\ntraining set as we will see in a moment.\n\n\nIn Equation 114, \nmany conditional probabilities are\nmultiplied, one for each position \n.\nThis can result in a floating point underflow. It\nis therefore better to perform the computation by adding\nlogarithms of probabilities instead of multiplying\nprobabilities. The class with the highest log probability\nscore is still the most probable; \n and the logarithm function is monotonic. Hence,\nthe maximization that is actually done in most\nimplementations of NB is:\n\n\n\n\n\n \n \n\n(115)\n\n\n\nEquation 115 has a simple interpretation.  Each\nconditional parameter \n is a weight that\nindicates how good an indicator \n is for\n. Similarly, the prior \n\nis a weight that indicates the relative frequency of\n. More frequent classes are more likely to be the\ncorrect class than infrequent\nclasses. \nThe\nsum of log prior and term weights is then a measure of how\nmuch evidence there is for the document being in the class, and\nEquation 115  selects the class for which we have\nthe most evidence.\n\n\nWe will initially work with this intuitive interpretation of\nthe multinomial NB model and defer a formal derivation to\nSection 13.4 .\n\n\nHow do we estimate the parameters\n\n and\n\n?\nWe first try\nthe   maximum likelihood estimate  (MLE; probtheory), which\nis simply the relative frequency and\ncorresponds to the most likely value of each parameter given\nthe training data. For the priors this estimate is:\n\n\n\n\n\n \n \n\n(116)\n\n\nwhere  is the number of documents in class  and\n is the total number of documents.\n\n\nWe estimate the conditional probability \n\n as the relative frequency\nof term  in\ndocuments belonging to class :\n\n\n\n\n\n\n\n(117)\n\n\nwhere  is the number of occurrences of  in\ntraining documents from class ,\nincluding multiple\noccurrences of a term in a document. We have made the\n positional independence assumption here,\nwhich we will discuss in more detail in the next section: \n is a count of occurrences\nin all positions  in the documents in the training set.\nThus, we do not compute different estimates for different\npositions and, for example, if a word occurs twice in a document, in positions\n and , then \n\n.\n\n\nThe problem with the MLE estimate is that it is zero for a\nterm-class combination that did not occur in the training\ndata. If\nthe term WTO in the training data only\noccurred in China documents, then the MLE estimates\nfor the other classes, for example UK, will be\nzero:\n\n\n\n\n\n\n(118)\n\n\nNow, the one-sentence document Britain is a\nmember of the WTO\nwill get a conditional probability of\nzero for UK because we are multiplying the conditional\nprobabilities for all terms in\nEquation 113. \nClearly, the model should\nassign a high probability to the UK class because\nthe term Britain\noccurs. The problem is that the zero probability\nfor WTO cannot be ``conditioned away,'' no\nmatter how strong the evidence for the class UK\nfrom other features. \nThe estimate is 0 because of\n   sparseness : The training data are never large enough\nto represent the frequency of rare events adequately, for\nexample, \nthe frequency of WTO occurring in\nUK documents.\n\n\n\n\nFigure 13.2:\nNaive Bayes algorithm (multinomial model):\nTraining and testing.\n\n\n\n\nTo eliminate zeros, we use\n  add-one \nor  Laplace\nsmoothing, which simply\nadds one to each count (cf. Section 11.3.2 ):\n\n\n\n\n\n\n(119)\n\n\nwhere  is the number of terms in the vocabulary.\nAdd-one smoothing\ncan be interpreted as a uniform prior (each term occurs once\nfor each class) that is then updated as evidence\nfrom the training data comes in. Note that this is\na prior probability for the occurrence of a term as opposed\nto the prior probability of a class which we estimate in\nEquation 116 on the document level.\n\n\nWe have now introduced all the elements we need for training\nand applying an NB classifier. The complete\nalgorithm is described in\nFigure 13.2 .\n\n\n\n\n\n\nTable 13.1:\nData for parameter\nestimation examples.  \n  \ndocID\nwords in document\nin   China?\n \n training set\n1\nChinese Beijing Chinese\nyes\n \n  \n2\nChinese Chinese Shanghai\nyes\n \n  \n3\nChinese Macao\nyes\n \n  \n4\nTokyo Japan Chinese\nno\n \n test set\n5\nChinese Chinese Chinese Tokyo Japan\n?\n \n\n\n\n\nWorked example.\nFor the example in Table 13.1 , the multinomial\nparameters we need to classify the test document are the\npriors \n and \n and the\nfollowing conditional probabilities:\n\n\n\n\n\nThe denominators are  and  because \nthe lengths of  and \n are \n8 and 3, respectively, and because \nthe constant  in\nEquation 119 is 6 as the vocabulary consists of six\nterms.\n\n\nWe then get:\n\n\n\n\n\nThus, the classifier assigns\nthe test document to  = China. The reason for\nthis classification decision is that\nthe three occurrences of the positive\nindicator Chinese in  outweigh the occurrences of\nthe two negative indicators Japan and Tokyo. End worked example.\n\n\n\n\n\nTable 13.2:\nTraining and test times for\nNB.  \n\n mode\ntime complexity\n \n training\n\n\n \n testing\n\n\n \n\n\n\nWhat is the time complexity of NB?  The complexity\nof computing the parameters is \n because\nthe set of parameters consists of \n\nconditional probabilities and  priors. The\npreprocessing necessary for computing the parameters\n(extracting the vocabulary, counting terms, etc.)  can be\ndone in one pass through the training data. The time\ncomplexity of this component is therefore \n,\nwhere \n is the number of documents and  is\nthe average length of a document.  \n\n\n We use \n\nas a notation for  here, where  is the length of the\ntraining collection. \nThis is \nnonstandard;\n is not defined for an average. \nWe prefer expressing the time\ncomplexity in terms of \n and  \nbecause these are the primary statistics used to\ncharacterize training collections.\n\n\nThe time complexity of\nAPPLYMULTINOMIALNB in\nFigure 13.2  is\n\n.\n and  are the numbers of\ntokens and types, respectively, in the test\ndocument .\nAPPLYMULTINOMIALNB can be modified to be\n\n\n(Exercise 13.6 ).\nFinally, assuming\nthat the length of test documents is bounded,\n \n because \n\n for a fixed constant .\n\nTable 13.2  summarizes the time complexities.\nIn general, we have \n, so both training\nand testing complexity are linear in the time it takes\nto scan the data. Because we have to look at the data at\nleast once, NB can be said to have optimal time\ncomplexity. Its efficiency is one reason why NB\nis a popular text classification method.\n\n\n\n\nSubsections\n\nRelation to multinomial unigram language model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Relation to multinomial unigram\n Up: Text classification and Naive\n Previous: The text classification problem\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}