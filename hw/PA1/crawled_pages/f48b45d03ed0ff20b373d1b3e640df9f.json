{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html",
  "title": "Cluster labeling",
  "body": "\n\n\n\n\nCluster labeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Implementation notes\n Up: Hierarchical clustering\n Previous: Divisive clustering\n    Contents \n    Index\n\n\n\n\n \n\nCluster labeling\n\n\nIn many applications of flat clustering and hierarchical\nclustering, particularly in analysis tasks and in user\ninterfaces (see applications in Table 16.1 ,\npage 16.1 ), human users interact with\nclusters. In such settings, we must label clusters, so that\nusers can see what a cluster is about.\n\n\n Differential cluster labeling \nselects cluster labels by comparing the distribution of terms in one cluster with that\nof other clusters. The feature selection methods \nwe\nintroduced in Section 13.5 (page ) can all\nbe used for differential cluster\nlabeling. In particular, mutual information (MI)\n(Section 13.5.1 , page 13.5.1 )\nor, equivalently, information gain and the -test\n(Section 13.5.2 , page 13.5.2 )\nwill identify cluster labels that characterize one cluster\nin contrast to other clusters.\nA combination of a\ndifferential test with a penalty for rare terms\noften gives the best labeling results because rare terms are\nnot necessarily representative of the cluster as a whole.\n\n\n\n\n\n \n \nlabeling method\n\n \n# docs\ncentroid\nmutual information\ntitle\n\n4\n622\n\n\noil\nplant\nmexico\nproduction\ncrude\npower\n000\nrefinery\ngas\nbpd\n\n\n\n\nplant oil production \nbarrels \ncrude bpd mexico \ndolly \ncapacity\npetroleum\n\n\n\n\n\nMEXICO: Hurricane Dolly heads for Mexico coast\n\n\n\n9\n1017\n\n\npolice\nsecurity\nrussian\npeople\nmilitary\npeace\nkilled\ntold\ngrozny\ncourt\n\n\n\n\npolice killed military security peace told \ntroops \nforces\nrebels \npeople\n\n\n\n\nRUSSIA: Russia's Lebed meets rebel chief in Chechnya\n\n\n\n10\n1259\n\n\n00\n000\ntonnes\ntraders\nfutures\nwheat\nprices\ncents\nseptember\ntonne\n\n\n\n\ndelivery \ntraders futures tonne tonnes \ndesk \nwheat prices 000 00\n\n\n\n\nUSA: Export Business - Grain/oilseeds complex\n\n\n\n\n\n\nAutomatically computed cluster labels.This is\nfor three of ten clusters (4, 9, and 10) in a  -means clustering \nof the first 10,000 documents in \nReuters-RCV1.  The last three columns show cluster summaries\ncomputed by three\nlabeling methods: most highly weighted terms in centroid (centroid),\nmutual information, and the title of the document closest to the centroid of\nthe cluster (title). Terms selected by only one of the\nfirst two\nmethods are in bold.\n \n\n\n\nWe apply three labeling methods to a  -means clustering \nin Table 17.2 . In this example, there is almost\nno difference between MI and . We therefore omit the\nlatter. \n\n\n Cluster-internal labeling  computes a label that solely\ndepends on the cluster itself, not on other clusters.\nLabeling a cluster with the title of the document closest to the\ncentroid is one cluster-internal method. Titles\nare easier to read than a list of terms. A full\ntitle can also contain important context that didn't make it\ninto the top 10 terms selected by MI. \nOn the web, anchor text can play a role similar to a title since\nthe anchor text pointing to a page can serve as a concise summary of\nits contents.\n\n\nIn Table 17.2 ,\nthe\ntitle for cluster 9 suggests that many of its documents are\nabout the Chechnya conflict, a fact the MI terms do not\nreveal. However, a single document is unlikely to be representative of\nall documents in a cluster. An example is cluster 4,\nwhose selected title\nis misleading. The main topic of the cluster is\noil. Articles about hurricane Dolly only ended up in this\ncluster because of its effect on oil prices.\n\n\nWe can also use a list of terms with high weights in the\ncentroid of the cluster as a label.  Such highly weighted\nterms (or, even better, phrases, especially noun phrases)\nare often more representative of the cluster than a few\ntitles can be, even if they are not filtered for\ndistinctiveness as in the differential methods. However, a list\nof phrases takes more time to digest for users than a well\ncrafted title.\n\n\nCluster-internal methods are efficient, but they fail to\ndistinguish terms that are frequent in the collection as a\nwhole from those that are frequent only in the\ncluster. Terms like year or Tuesday may be\namong the most frequent in a cluster, but they are not\nhelpful in understanding the contents of a cluster with a\nspecific topic like oil.\n\n\nIn Table 17.2 , the centroid method selects a few more\nuninformative terms (000, court, cents,\nseptember) than MI (forces, desk), but\nmost of the terms selected by either method are good\ndescriptors.  We get a good sense of the documents in a\ncluster from scanning the selected terms.\n\n\nFor hierarchical clustering, additional complications arise\nin cluster labeling.  Not only do we need to distinguish an\ninternal node in the tree from its siblings, but also from\nits parent and its children.  Documents in child nodes are\nby definition also members of their parent node, so we\ncannot use a naive differential method to find labels that\ndistinguish the parent from its children. However, more\ncomplex criteria, based on a combination of overall\ncollection frequency and prevalence in a given cluster, can\ndetermine whether a term is a more informative label for a\nchild node or a parent node (see Section 17.9 ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Implementation notes\n Up: Hierarchical clustering\n Previous: Divisive clustering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}