{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/assessing-as-a-feature-selection-methodassessing-chi-square-as-a-feature-selection-method-1.html",
  "title": "Assessing as a feature selection methodAssessing chi-square as a feature selection method",
  "body": "\n\n\n\n\nAssessing as a feature selection methodAssessing chi-square as a feature selection method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Frequency-based feature selection\n Up: Feature selectionChi2 Feature selection\n Previous: Feature selectionChi2 Feature selection\n    Contents \n    Index\n\n\n\n\nAssessing\n     as a feature selection\n    methodAssessing chi-square as a feature\n    selection method\n\nFrom a statistical point of view,  feature selection\nis problematic. For a test with one degree of freedom, the\nso-called Yates correction should be used (see\nSection 13.7 ), which makes it harder to reach\nstatistical significance. Also, whenever a statistical test\nis used multiple times, then the probability of getting at\nleast one error increases. If 1,000 hypotheses are rejected,\neach with 0.05 error probability, then\n\n calls of the test will be\nwrong on average. However, in text classification it rarely\nmatters whether a few additional terms are added to the\nfeature set or removed from it. Rather, the relative\nimportance of features is important. \nAs long as  feature selection\nonly ranks features with respect to their usefulness and is\nnot used to make statements about statistical dependence or\nindependence of variables, we need not be overly concerned\nthat it does not adhere strictly to statistical theory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Frequency-based feature selection\n Up: Feature selectionChi2 Feature selection\n Previous: Feature selectionChi2 Feature selection\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}