{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html",
  "title": "Evaluation of clustering",
  "body": "\n\n\n\n\nEvaluation of clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: K-means\n Up: Flat clustering\n Previous: Cardinality - the number\n    Contents \n    Index\n\n\n\n\n \n\nEvaluation of clustering\n\n\nTypical objective functions in clustering formalize the goal\nof attaining high intra-cluster similarity (documents within\na cluster are similar) and low inter-cluster similarity\n(documents from different clusters are dissimilar). This is\nan  internal criterion  for the quality of a\nclustering. But good scores on an internal criterion do not\nnecessarily translate into good effectiveness in an\napplication. An alternative to internal criteria is direct\nevaluation in the application of interest. For search result\nclustering, we may want to measure the time it takes users\nto find an answer with different clustering algorithms. This\nis the most direct evaluation, but it is expensive,\nespecially if large user studies are necessary.\n\n\nAs a surrogate for user judgments, we can use a set of classes\nin an\nevaluation benchmark or gold standard\n(see Section 8.5 ,\npage 8.5 , and Section 13.6 ,\npage 13.6 ).\nThe gold\nstandard is ideally produced by human judges with a good\nlevel of inter-judge agreement\n(see Chapter 8 , page 8.1 ).\nWe can then\ncompute an  external criterion \nthat evaluates\nhow well the clustering matches the gold standard classes.\nFor example, we may want to say\nthat the optimal\nclustering of the search results for jaguar in\nFigure 16.2  consists of three classes\ncorresponding to the three senses car, animal, and\noperating system.  In this type of evaluation, we\nonly use the partition provided by the gold standard, not\nthe class labels.\n\n\nThis section introduces four external criteria of clustering\nquality.  Purity is a simple and transparent\nevaluation measure. Normalized mutual information can be\ninformation-theoretically \ninterpreted.  The Rand\nindex penalizes both false positive and false negative\ndecisions during clustering.  The F measure in\naddition supports differential weighting of these two types\nof errors.\n\n\n\n\n\n\nTo compute  purity ,\neach cluster is assigned to the class which is most\nfrequent in the cluster, and then the accuracy of this\nassignment is measured by counting the number of correctly\nassigned documents and dividing by . Formally:\n\n\n\n\n\n\n(182)\n\n\nwhere\n\nis the set of clusters and\n\n is the set of classes. We interpret\n as the set of documents in \n and\n as the set\nof documents in  in Equation 182.\n\n\nWe present an example of how to compute purity in\nFigure 16.4 . Bad\nclusterings have purity values close to 0, a perfect\nclustering has a purity of  1 . Purity is compared with the other three\nmeasures discussed in this chapter in Table 16.2 .\n\n\n\n\n\nTable 16.2:\nThe four external evaluation measures applied to\nthe clustering in Figure 16.4 .\n\n \npurity\nNMI\nRI\n\n\nlower bound\n0.0\n0.0\n0.0\n0.0\n\nmaximum\n1\n1\n1\n1\n\nvalue for Figure 16.4\n0.71\n0.36\n0.68\n0.46\n\n\n \n\n\n\n\n\nHigh purity is easy to achieve when the number of\nclusters is large - in particular, purity is  1  if each document gets\nits own cluster. \nThus, we cannot use purity to\ntrade off \nthe quality of the clustering against the\nnumber of clusters.\n\n\nA measure that allows us to make this tradeoff is\n normalized mutual\ninformation  or  NMI :\n\n\n\n\n\n\n(183)\n\n\n is mutual information (cf. Chapter 13 ,\npage 13.5.1 ):\n\n\n\n\n\n\n\n\n(184)\n \n\n\n\n(185)\n\n\nwhere \n, , and \n are the\nprobabilities of a document being\nin cluster , class , and in the intersection of \nand , respectively. \nEquation 185  is equivalent to\nEquation 184 for maximum likelihood estimates of the\nprobabilities (i.e., the estimate of each probability is the\ncorresponding relative frequency).\n\n\n is entropy as defined in Chapter 5 \n(page 5.3.2 ):\n\n\n\n\n\n\n\n\n(186)\n \n\n\n\n(187)\n\n\nwhere, again, the second equation is based on maximum\nlikelihood estimates of the probabilities.\n\n\n\n in Equation 184 measures the\namount of information by which our knowledge about the\nclasses increases when we are told what the clusters are.\nThe minimum of \n is 0 if the\nclustering is random with respect to class membership. In that\ncase, knowing that a document is in a particular cluster\ndoes not give us any new information about what its class\nmight be. Maximum mutual information is reached for a\nclustering \n that perfectly recreates the\nclasses - but also if clusters in \n are\nfurther subdivided into smaller clusters\n(Exercise 16.7 ).  In particular, a clustering\nwith  one-document clusters has maximum MI.  So MI has\nthe same problem as purity: it does not penalize large\ncardinalities and thus does not formalize our bias that,\nother things being equal, fewer clusters are better.\n\n\nThe normalization by the denominator \n in Equation 183 fixes this problem since entropy\ntends to increase with the number of clusters.  For example,\n reaches its maximum  for , which\n\n\nensures that NMI is low for .  Because NMI is\nnormalized, we can use it to compare clusterings with\ndifferent numbers of clusters. The particular form of the\ndenominator is chosen because  \n is a tight upper bound on \n (Exercise 16.7 ). Thus,\nNMI is always a number between 0 and 1.\n\n\nAn alternative to this information-theoretic interpretation\nof clustering\nis to view it as a series of decisions, one for each of\nthe \n\npairs of documents in the collection. We\nwant to assign \ntwo\ndocuments to the same cluster if and only if they are similar.\nA true positive (TP) decision assigns two similar documents to\nthe same cluster, a true negative (TN) decision assigns two\ndissimilar documents to different clusters.\nThere are two types of errors we can commit.\nA   \n(FP) decision\nassigns two dissimilar documents to the same cluster. A\n   \n(FN) decision assigns two similar documents to\ndifferent clusters. \nThe  Rand index  \n(  ) measures the percentage of decisions that\nare correct.  That is, it is simply accuracy (Section 8.3 ,\npage 8.3 ). \n\n\n\n\n\n\nAs an example, we compute RI for\nFigure 16.4 . We first compute \n.\nThe three clusters\ncontain 6, 6, and 5 points, respectively, so the total\nnumber of ``positives'' or pairs of documents\nthat are in the same cluster is:\n\n\n\n\n\n\n(188)\n\n\nOf these, the x pairs in cluster 1, the o pairs in\ncluster 2, the  pairs in cluster 3, and the x pair in\ncluster 3 are true positives:\n\n\n\n\n\n\n(189)\n\n\nThus, \n. \n\n\n and  are computed similarly,\nresulting in the following contingency table:\n\n\n \nSame cluster\nDifferent clusters\n\nSame class\n\n\n\n\n\nDifferent classes\n\n\n\n\n\n\n\n is then \n.\n\n\nThe Rand index gives equal weight to false positives and false\nnegatives. Separating similar documents\nis sometimes worse than putting pairs of dissimilar\ndocuments in the same cluster. We can use the\n F measure \nmeasuresperf to\npenalize false negatives more strongly than false positives by selecting a value , thus\ngiving more weight to recall.\n\n\n\n\n\nBased on the numbers in the contingency table,\n\n and \n. \nThis gives us \n for  and \n for .\nIn information retrieval,\nevaluating clustering with  has the\nadvantage that the measure is already familiar to the\nresearch community.\n\n\nExercises.\n\nReplace every point  in Figure 16.4  with\ntwo identical copies of  in the same class.\n(i) Is it less difficult, equally difficult or more\ndifficult to cluster this set of 34 points as opposed to the\n17 points in Figure 16.4 ? (ii)\nCompute purity, NMI,\nRI, and  for the clustering with 34 points.\nWhich  measures increase and which stay the same after doubling the number of\npoints? \n(iii) Given your assessment in (i) and the results in (ii),\nwhich measures are best suited to compare the quality of the\ntwo clusterings?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: K-means\n Up: Flat clustering\n Previous: Cardinality - the number\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}