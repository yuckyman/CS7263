{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/centroid-clustering-1.html",
  "title": "Centroid clustering",
  "body": "\n\n\n\n\nCentroid clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Optimality of HAC\n Up: Hierarchical clustering\n Previous: Group-average agglomerative clustering\n    Contents \n    Index\n\n\n\n\n \n\nCentroid clustering\n\n\n\n\nIn centroid clustering, the similarity of two clusters is\ndefined as the similarity of their centroids:\n\n\n\n\n\n\n\n\n(207)\n \n\n\n\n(208)\n \n\n\n\n(209)\n\n\nEquation 207  is centroid similarity. Equation 209 \nshows that centroid similarity is equivalent to average\nsimilarity of all pairs of documents from different\nclusters. Thus, the difference between GAAC and centroid\nclustering is that GAAC considers all pairs of documents in\ncomputing average pairwise similarity \n(Figure 17.3 , (d))\nwhereas centroid\nclustering excludes pairs from the same cluster\n(Figure 17.3 , (c)).\n\n\nFigure 17.11  shows the first three steps of a\ncentroid clustering. \nThe first two iterations form the clusters\n with centroid  \nand\n with centroid  \nbecause the pairs\n \n and\n\n have the highest centroid similarities. In the\nthird iteration, the highest\ncentroid similarity is between  and  producing the\ncluster \n with centroid .\n\n\nLike GAAC,\ncentroid clustering is not best-merge persistent and\ntherefore \n (Exercise 17.10 ).\n\n\n\n\n\n\nIn contrast to the other three HAC algorithms,\ncentroid clustering is not monotonic. \nSo-called \n inversions \ncan occur: Similarity can\nincrease during clustering as in the example in\nFigure 17.12 , where we define similarity as negative distance.\nIn the first merge, the similarity of  and  is .\nIn the second merge, the similarity of the\ncentroid of  and  (the circle) and  is\n\n. This is an example of an\ninversion: similarity increases in this sequence of\ntwo clustering steps. In a monotonic HAC\nalgorithm, similarity is monotonically decreasing\nfrom iteration to iteration.\n\n\nIncreasing similarity in a series of HAC clustering steps\ncontradicts the fundamental assumption that small\nclusters are more coherent than large clusters. An inversion\nin a dendrogram shows up as a horizontal merge line that is\nlower than the previous merge line.\nAll merge lines in\n and 17.5  are higher than\ntheir predecessors because single-link and complete-link\nclustering are monotonic clustering algorithms.\n\n\nDespite its non-monotonicity, centroid clustering is often used\nbecause its similarity measure - the similarity of two\ncentroids - is conceptually simpler than the average of all\npairwise similarities in GAAC. Figure 17.11  is all one needs to\nunderstand centroid clustering. There is no equally simple\ngraph that would explain how GAAC works.\n\n\nExercises.\n\n   For a fixed set of  documents there are up to \ndistinct similarities between clusters in single-link and\ncomplete-link clustering. How many distinct cluster\nsimilarities are there in GAAC and centroid clustering?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Optimality of HAC\n Up: Hierarchical clustering\n Previous: Group-average agglomerative clustering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}