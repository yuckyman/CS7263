{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-17.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Exercises\n Up: Hierarchical clustering\n Previous: Implementation notes\n    Contents \n    Index\n\n\n\n \n\nReferences and further reading\n\n\nAn excellent general review of clustering\nis (Jain et al., 1999).\nEarly references for specific HAC algorithms are (King, 1967) (single-link),\n(Sneath and Sokal, 1973) (complete-link, GAAC) and\n(Lance and Williams, 1967) (discussing a large variety of\nhierarchical clustering algorithms). The single-link\nalgorithm in Figure 17.9  is similar to\n Kruskal's algorithm  for\nconstructing a\nminimum spanning\ntree. A graph-theoretical proof of the correctness of\nKruskal's algorithm (which is\nanalogous to the proof in Section 17.5 ) is provided by\nCormen et al. (1990, Theorem 23.1). See Exercise 17.10 \nfor the connection between minimum spanning trees and\nsingle-link clusterings.\n\n\nIt is often claimed that hierarchical clustering algorithms\nproduce better clusterings than flat algorithms\n(Jain and Dubes (1988, p. 140),\nCutting et al. (1992), Larsen and Aone (1999)) although more\nrecently there have been experimental results suggesting the\nopposite (Zhao and Karypis, 2002). Even without a consensus\non average behavior, there is no doubt that results of EM\nand  -means are highly variable since they will often\nconverge to a local optimum of poor quality.  The\nHAC algorithms we have presented here are\ndeterministic and thus more predictable.\n\n\nThe complexity of complete-link, group-average and centroid\nclustering is sometimes given as \n(Day and Edelsbrunner, 1984, Murtagh, 1983, Voorhees, 1985b) because a document similarity computation\nis an order of magnitude more expensive than a simple\ncomparison, the main operation executed in the merging steps\nafter the  similarity matrix has been computed.\n\n\nThe centroid algorithm described here is due to\nVoorhees (1985b).  Voorhees recommends\ncomplete-link and centroid clustering over single-link for a\nretrieval application.  The Buckshot algorithm was\noriginally published by Cutting et al. (1993). \nAllan et al. (1998) \napply\nsingle-link clustering to  first story detection .\n\n\nAn important HAC technique not discussed here is\n Ward's method \n(El-Hamdouchi and Willett, 1986, Ward Jr., 1963), also\ncalled \n minimum variance clustering . In each step, it\nselects the merge with the smallest RSS\n(Chapter 16 , page 191 ).  The merge criterion\nin Ward's method (a function of all individual distances\nfrom the centroid) is closely related to the merge criterion\nin GAAC (a function of all individual similarities to the\ncentroid).\n\n\nDespite its importance for making the results of clustering\nuseful, comparatively little work has been done on labeling\nclusters. Popescul and Ungar (2000) obtain good results\nwith a combination of  and collection frequency of a\nterm.  Glover et al. (2002b) use information gain for\nlabeling clusters of web pages.  \nStein and zu Eissen's approach is ontology-based (2004).\nThe more complex problem of\nlabeling nodes in a hierarchy (which requires distinguishing\nmore general labels for parents from more specific labels\nfor children) is tackled by Glover et al. (2002a) and\nTreeratpituk and Callan (2006).  Some clustering\nalgorithms attempt to find a set of labels first and then\nbuild (often overlapping) clusters around the labels,\nthereby avoiding the problem of labeling altogether\n(Osinski and Weiss, 2005, Zamir and Etzioni, 1999, Käki, 2005).  We know of no\ncomprehensive study that compares the quality of such\n``label-based'' clustering to the clustering algorithms\ndiscussed in this chapter and in Chapter 16 .  In principle,\nwork on multi-document  summarization \n(McKeown and Radev, 1995) is also applicable to cluster\nlabeling, but multi-document summaries are usually longer\nthan the short text fragments needed when labeling\nclusters (cf. snippets). Presenting clusters in a way that users can understand is a UI problem. We\nrecommend reading (Baeza-Yates and Ribeiro-Neto, 1999, ch. 10) for an\nintroduction to user interfaces in IR.\n\n\nAn example of an efficient divisive algorithm is bisecting\n -means (Steinbach et al., 2000).  \n Spectral clustering  algorithms\n(Kannan et al., 2000, Dhillon, 2001, Zha et al., 2001, Ng et al., 2001a),\nincluding\n principal direction divisive partitioning  (PDDP)\n(whose bisecting decisions are based on\n SVD , see Chapter 18 )\n(Boley, 1998, Savaresi and Boley, 2004),\nare computationally more expensive than\nbisecting\n -means, but have the advantage of being deterministic.\n\n\nUnlike  -means and EM, most hierarchical clustering\nalgorithms do not have a probabilistic\ninterpretation. Model-based hierarchical clustering\n(Kamvar et al., 2002, Vaithyanathan and Dom, 2000, Castro et al., 2004) is an exception.\n\n\nThe evaluation methodology described in\nSection 16.3  (page 16.3 )\nis also applicable to hierarchical clustering. Specialized\nevaluation measures for hierarchies are\ndiscussed by \nFowlkes and Mallows (1983),\nLarsen and Aone (1999)\nand \nSahoo et al. (2006).\n\n\nThe R environment (R Development Core Team, 2005) offers good support for\nhierarchical clustering.  The R function hclust\nimplements single-link, complete-link, group-average, and\ncentroid clustering; and Ward's method.  Another option\nprovided is median clustering which represents each\ncluster by its medoid (cf. k-medoids in\nChapter 16 , page 16.4 ).  Support for\nclustering vectors in high-dimensional spaces is provided by\nthe software package CLUTO\n(http://glaros.dtc.umn.edu/gkhome/views/cluto).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Exercises\n Up: Hierarchical clustering\n Previous: Implementation notes\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}