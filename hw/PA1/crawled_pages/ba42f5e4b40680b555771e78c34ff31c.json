{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-13.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Vector space classification\n Up: Text classification and Naive\n Previous: Evaluation of text classification\n    Contents \n    Index\n\n\n\n\n \n\nReferences and further reading\n\n\nGeneral introductions to statistical classification and\nmachine learning can be found in (Hastie et al., 2001),\n(Mitchell, 1997), and (Duda et al., 2000),\nincluding many important methods (e.g.,\n decision trees  and  boosting )\nthat we do not cover. A comprehensive review of text\nclassification methods and results is\n(Sebastiani, 2002).\nManning and Schütze (1999, Chapter 16) give an accessible\nintroduction to text classification with coverage of\ndecision trees,  perceptrons  and maximum entropy models.  More\ninformation on the superlinear time complexity of learning\nmethods that are more accurate than Naive Bayes can be found\nin (Perkins et al., 2003) and (Joachims, 2006a).\n\n\nMaron and Kuhns (1960) described one of the first NB\ntext classifiers.  Lewis (1998) focuses on the\nhistory of NB classification. Bernoulli and\nmultinomial models and their accuracy for different\ncollections are discussed by McCallum and Nigam (1998).\nEyheramendy et al. (2003) present additional NB\nmodels.  Domingos and Pazzani (1997), \nFriedman (1997), and Hand and Yu (2001)  analyze why NB performs\nwell although its probability estimates are poor.  The\nfirst paper also discusses NB's optimality when the\nindependence assumptions are true of the\ndata. Pavlov et al. (2004) propose a modified\ndocument representation that partially addresses the\ninappropriateness of the independence assumptions.\nBennett (2000) attributes the tendency of NB\nprobability estimates to be close to either 0 or 1 to the\neffect of document length.  Ng and Jordan (2001) show\nthat NB is sometimes (although rarely) superior to\ndiscriminative methods because it more quickly reaches its\noptimal error rate.  The basic NB model presented\nin this chapter can be tuned for better effectiveness\n(Rennie et al. 2003;Kocz and Yih 2007).  The problem of\n concept drift  and other reasons why\nstate-of-the-art classifiers do not always excel in practice\nare discussed by Forman (2006) and\nHand (2006).\n\n\nEarly uses of mutual information and  for feature\nselection in text classification are\nLewis and Ringuette (1994) and Schütze et al. (1995), respectively.\nYang and Pedersen (1997) review feature selection methods and\ntheir impact on classification effectiveness. They find that\n  pointwise mutual information  is not competitive with\nother methods.  Yang and Pedersen refer to\nexpected mutual information (Equation 130) as\ninformation gain (see Exercise 13.6 ,\npage 13.6 ).  (Snedecor and Cochran, 1989) is a good\nreference for the  test in statistics, including the\nYates' correction for continuity for \ntables. Dunning (1993) discusses problems of the\n test when counts are small.  Nongreedy feature\nselection techniques are described by\nHastie et al. (2001).  Cohen (1995)\ndiscusses the pitfalls of using multiple significance tests\nand methods to avoid them.  Forman (2004) evaluates\ndifferent methods for feature selection for multiple\nclassifiers.\n\n\nDavid D. Lewis defines the  ModApte split  at\nwww.daviddlewis.com/resources/testcollections/reuters21578/readme.txtbased on Apté et al. (1994).  Lewis (1995) describes\n  utility measures  for the\nevaluation of text classification systems.\nYang and Liu (1999) employ significance tests in the\nevaluation of text classification methods.\n\n\nLewis et al. (2004) find that SVMs (Chapter 15 ) perform better on\nReuters-RCV1 than kNN and Rocchio (Chapter 14 ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Vector space classification\n Up: Text classification and Naive\n Previous: Evaluation of text classification\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}