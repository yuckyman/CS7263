{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/a-simple-example-of-machine-learned-scoring-1.html",
  "title": "A simple example of machine-learned scoring",
  "body": "\n\n\n\n\nA simple example of machine-learned scoring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Result ranking by machine\n Up: Machine learning methods in\n Previous: Machine learning methods in\n    Contents \n    Index\n\n\n\n\n \n\nA simple example of machine-learned scoring\n\n\nIn this section we generalize the methodology of Section 6.1.2 (page ) to\nmachine learning of the scoring function. In Section 6.1.2  we\nconsidered a case where we had to combine Boolean indicators of\nrelevance; here we consider more general factors to further develop\nthe notion of  machine-learned relevance .  In particular,\nthe factors we now consider go beyond Boolean functions of query term\npresence in document zones, as in Section 6.1.2 . \n\n\nWe develop the ideas in a setting where the scoring\nfunction is a linear combination of two factors: (1) the vector\nspace cosine similarity between query and document and (2) the minimum\nwindow width  within which the query terms lie.  As we noted in \nSection 7.2.2 (page ), query term proximity is often very indicative of a\ndocument being on topic, especially with longer documents and on the web. \nAmong other things, this quantity gives us an implementation of implicit\nphrases. Thus we have\none factor that depends on the statistics of query terms in the document\nas a bag of words, and another that depends on proximity weighting.\nWe consider only two features in the development of the\nideas because a two-feature exposition\nremains simple enough to visualize. The technique can be generalized \nto many more features.\n\n\nAs in Section 6.1.2 , we are provided with a set of   training examples, each of which is a pair consisting of\na query and a document, together with a relevance judgment\nfor that document on that query that is either\nrelevant or nonrelevant.  For each such\nexample we can compute the vector space cosine similarity,\nas well as the window width .  The result is a\ntraining set as shown in Table 15.3 , which\nresembles Figure 6.5 (page ) from Section 6.1.2 .\n\n\n\n\n\nTable 15.3:\nTraining examples for machine-learned scoring.\n\nExample\nDocID\nQuery\nCosine score\n\nJudgment\n\n\n37\nlinux operating system\n0.032\n3\nrelevant\n\n\n37\npenguin logo\n0.02\n4\nnonrelevant\n\n\n238\noperating system\n0.043\n2\nrelevant\n\n\n238\nruntime environment\n0.004\n2\nnonrelevant\n\n\n1741\nkernel layer\n0.022\n3\nrelevant\n\n\n2094\ndevice driver\n0.03\n2\nrelevant\n\n\n3191\ndevice driver\n0.027\n5\nnonrelevant\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nHere, the two features (cosine score denoted  and window width\n) are real-valued predictors.\nIf we once again quantify the judgment relevant as 1 and\nnonrelevant as 0, we seek a scoring function that combines the values\nof the features to generate a value that is (close to) 0 or 1.  We wish this\nfunction to be in agreement with our set of training examples as far\nas possible.  Without loss of generality, a linear classifier will use\na linear combination of\nfeatures of the form \n\n\n\n\n\n\n(179)\n\n\nwith the coefficients  to be learned from the training data.\nWhile it is possible to formulate this as an error minimization\nproblem as we did in Section 6.1.2 , it is instructive to visualize the\ngeometry of Equation 179.  The examples in\nTable 15.3  can be plotted on a two-dimensional plane with\naxes corresponding to the cosine score  and the window width\n.  This is depicted in Figure 15.7 . \n\n\n\n\nA collection of training examples.Each R denotes a training example labeled relevant, while each N is a training example labeled nonrelevant.\n\n\n\nIn this setting, the function \n from\nEquation 179 represents a plane ``hanging above'' Figure 15.7 .\nIdeally this plane (in the direction perpendicular to the page\ncontaining Figure 15.7 ) assumes values close to 1 above the points\nmarked R, and values close to 0 above the points marked N.  Since a\nplane is unlikely to assume only values close to 0 or 1 above the\ntraining sample points, we make use of thresholding: given any\nquery and document for which we wish to determine relevance, we pick a\nvalue  and if \n we declare the\ndocument to be relevant, else we declare the document to be\nnonrelevant. \n As we know from\nFigure 14.8 (page ),\nall points that satisfy\n\n form a line \n(shown as a dashed line in Figure 15.7 )\nand we thus have a\n linear classifier  that separates relevant from\nnonrelevant instances.\nGeometrically, we can find the separating line as follows.\nConsider the line\npassing through the plane \n whose height is\n above the page containing Figure 15.7 .  Project this line\ndown onto Figure 15.7 ; this will be the dashed line in\nFigure 15.7 .  Then, any subsequent query/document pair that falls\nbelow the dashed line in Figure 15.7  is deemed nonrelevant; above\nthe dashed line, relevant. \n\n\nThus, the problem of making a binary relevant/nonrelevant judgment\ngiven training examples as above turns into one of learning the dashed\nline in Figure 15.7  separating relevant training examples from the\nnonrelevant ones.  Being in the - plane, this line\ncan be written as a linear equation involving  and ,\nwith two parameters (slope and intercept).  The methods of linear\nclassification that we have already looked at in\nclassificationsvm provide methods for choosing this\nline.  Provided we can build a\nsufficiently rich collection of training samples, we can thus\naltogether avoid hand-tuning score functions as in\nSection 7.2.3 (page ).  The bottleneck of course is the ability to\nmaintain a suitably representative set of training examples, whose\nrelevance assessments must be made by experts. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Result ranking by machine\n Up: Machine learning methods in\n Previous: Machine learning methods in\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}