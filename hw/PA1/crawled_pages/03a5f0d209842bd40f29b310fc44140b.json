{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html",
  "title": "Text classification and Naive Bayes",
  "body": "\n\n\n\n\nText classification and Naive Bayes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The text classification problem\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n\n\n\n\nText classification and Naive Bayes\n\n\nThus far, this book has mainly discussed the process of\n ad hoc retrieval , where users\nhave transient information needs that they try to address\nby posing one or more queries to a search engine. However,\nmany users have ongoing information needs. \nFor example, you might need to track developments in\nmulticore computer chips. One\nway of doing this is to issue the query\nmulticore and computer and chip\nagainst an index of recent newswire articles each\nmorning. In this and the following two chapters we examine\nthe question: How can this repetitive task be automated? To\nthis end, many systems support   standing queries . A standing query is like any other\nquery except that it is periodically executed on a\ncollection to which new documents are incrementally added\nover time.\n\n\nIf your standing query is just multicore and\ncomputer and chip, you will tend to miss many relevant\nnew articles which use other terms such as multicore\nprocessors.  To achieve good recall, standing queries thus\nhave to be refined over time and can gradually become quite\ncomplex.  In this example, using a Boolean search engine\nwith stemming, you might end up with a query like\n(multicore or multi-core) and (chip\nor processor or microprocessor).\n\n\nTo capture the generality and scope of the problem space to which\nstanding queries belong, we now introduce the general notion of\na   classification  problem. Given a set of\nclasses, we seek to determine which class(es) a given\nobject belongs to. In the example, the\nstanding query serves to divide new newswire articles into\nthe two classes: documents about multicore computer\nchips and documents not about multicore\ncomputer chips. We refer to this as two-class\nclassification. Classification using standing queries is\nalso called   routing  or  filtering  and will be\ndiscussed further in Section 15.3.1 (page ).\n\n\nA class need not be as narrowly focused as the\nstanding query multicore computer chips.  Often, \na class is a more general subject area like China or\ncoffee.  Such more general classes are usually referred\nto as  topics , and the classification task is then\ncalled   text classification , \n text categorization ,\n topic classification , or \n topic spotting .  An example for\nChina appears in Figure 13.1 .  Standing\nqueries and topics differ in their degree of specificity,\nbut the methods for solving routing, filtering, and text\nclassification are essentially the same. We therefore\ninclude routing and filtering under the rubric of text\nclassification in this and the following chapters.\n\n\nThe notion of  classification is very general and has many\napplications within and beyond information retrieval (IR). For\ninstance, in computer vision, a classifier may be used to\ndivide images into classes such as landscape, portrait, and\nneither. We focus here on examples from information\nretrieval such as:\n\n\n\nSeveral of the preprocessing steps necessary for\nindexing as discussed in Chapter 2 :\ndetecting a document's encoding (ASCII, Unicode UTF-8\netc; page 2.1.1 );\nword segmentation (Is the white space between two letters a\nword boundary or not? page 24 ) ; truecasing (page 2.2.3 ); and\nidentifying the language of a document (page 2.5 ).\n\nThe automatic detection of  spam  pages (which then are\nnot included in the search engine index).\n\nThe automatic detection of sexually explicit content\n(which is included in search results only if the user\nturns an option such as SafeSearch off).\n\n\n\n Sentiment detection or the automatic classification of a movie or product\nreview as positive or negative. \nAn example application is a user searching for negative\nreviews before buying a camera to make sure it has no\nundesirable features or quality problems.\n\n\n\nPersonal   email sorting . A user may have folders like\n  talk announcements, electronic bills,\n  email from family and friends, and so on, and may want a\n  classifier to classify \n  each incoming email and automatically move it to the\n  appropriate folder. \nIt is easier to find messages in sorted folders than in a\n  very large inbox.\nThe most common case of this\n  application is a  spam  folder\nthat holds all suspected spam messages.\n\n\n\nTopic-specific  or vertical search.\n  Vertical search\nengines  restrict searches to a particular topic. For\nexample, the query computer science\non a vertical search engine for the topic China will\nreturn a list of Chinese computer science departments with\nhigher precision and recall than the query computer\nscience China on a general purpose search engine. This is\nbecause the vertical search engine does not include web\npages in its index that contain the term china in a\ndifferent sense (e.g., referring to a hard white ceramic),\nbut does include relevant pages even if they do not\nexplicitly mention the term China.\n\n\n\nFinally, the ranking function in ad hoc information\n  retrieval can also be based on a document classifier as we\n  will explain in Section 15.4 (page ).\n\n\n\n\n\nThis list shows the\ngeneral importance of classification in IR. Most retrieval systems today contain multiple\ncomponents that use some form of classifier.\nThe classification task we will\nuse as an example in this book is text\nclassification.\n\n\nA computer is not essential for classification. Many\nclassification tasks have traditionally been solved\nmanually. Books in a library are assigned Library of\nCongress categories by a librarian. But manual\nclassification is expensive to scale. The \nmulticore computer chips\nexample\nillustrates one alternative approach:\nclassification by the use of standing queries - which can\nbe thought of as \n  rules  - most commonly\nwritten by hand. As in our example (multicore\nor multi-core) and (chip or processor\nor microprocessor), rules are sometimes equivalent\nto Boolean expressions.\n\n\nA rule captures a certain combination of keywords that\nindicates a class. Hand-coded rules have good scaling\nproperties, but creating and maintaining them over time is\nlabor intensive. A technically skilled person (e.g., a\ndomain expert who is good at writing regular expressions)\ncan create rule sets that will rival or exceed the accuracy\nof the automatically generated classifiers we will discuss\nshortly; however, it can be hard to find someone with this\nspecialized skill.\n\n\nApart from manual classification and hand-crafted rules,\nthere is\na third approach to text classification, namely, machine\nlearning-based text classification.\nIt is the approach that we focus on in the next several chapters.\nIn machine learning, the set of rules or, more\ngenerally, the decision criterion of the text classifier, is\nlearned automatically from training data. This approach is\nalso called  statistical text classification  if the\nlearning method is statistical. In statistical text\nclassification, we require a number of good example\ndocuments (or training documents) for each class. The need\nfor manual classification is not eliminated because the\ntraining documents come from a person who has labeled them\n- where   labeling  refers to the process of annotating each\ndocument with its class. But labeling is arguably an easier\ntask than writing rules. Almost anybody can look at a\ndocument and decide whether or not it is related to China. Sometimes such labeling is already\nimplicitly part of an existing workflow. For instance, you may\ngo through the news\narticles returned by a standing query each morning and give\nrelevance feedback (cf. Chapter 9 ) by moving\nthe relevant articles to a special folder like multicore-processors.\n\n\nWe begin this chapter with a general introduction to the\ntext classification problem including a formal definition\n(Section 13.1 ); we then cover Naive Bayes,\na particularly simple and effective classification method\n(Sections 13.2-13.4). All\nof the classification algorithms we study represent documents in high-dimensional spaces. To improve the\nefficiency of these algorithms, it is generally desirable to\nreduce the dimensionality of these spaces; to this end, a\ntechnique known as feature selection is commonly\napplied in text classification as discussed in Section 13.5 .\nSection 13.6  covers evaluation of text\nclassification. In the following chapters,\nChapters 14 15 , we look at two other families\nof classification methods, vector space classifiers and\nsupport vector machines.\n\n\n\n\nSubsections\n\nThe text classification problem\nNaive Bayes text classification\n\nRelation to multinomial unigram language model\n\n\nThe Bernoulli model\nProperties of Naive Bayes\n\nA variant of the multinomial model\n\n\nFeature selection\n\nMutual\n  information\n Feature selectionChi2 Feature selection\n\nAssessing\n     as a feature selection\n    methodAssessing chi-square as a feature\n    selection method\n\nFrequency-based feature\n  selection\nFeature selection for multiple classifiers\nComparison of feature selection methods\n\n\nEvaluation of text classification\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The text classification problem\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}