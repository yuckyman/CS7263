{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/cluster-pruning-1.html",
  "title": "Cluster pruning",
  "body": "\n\n\n\n\nCluster pruning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Components of an information\n Up: Efficient scoring and ranking\n Previous: Impact ordering\n    Contents \n    Index\n\n\n\n\n \n\nCluster pruning\n \nIn cluster pruning we have a preprocessing step during which we cluster the document vectors. Then at query time, we consider only documents in a small number of clusters as candidates for which we compute cosine scores. Specifically, the preprocessing step is as follows:\n\n\nPick  documents at random from the collection. Call these leaders.\n\nFor each document that is not a leader, we compute its nearest leader.\n\n\nWe refer to documents that are not leaders as followers. Intuitively, in the partition of the followers induced by the use of  randomly chosen leaders, the expected number of followers for each leader is \n.\nNext, query processing proceeds as follows:\n\n\nGiven a query , find the leader  that is closest to . This entails computing cosine similarities from  to each of the  leaders.\n\nThe candidate set  consists of  together with its followers. We compute the cosine scores for all documents in this candidate set.\n\n\n\nThe use of randomly chosen leaders for clustering is fast and likely to reflect the distribution of the document vectors in the vector space: a region of the vector space that is dense in documents is likely to produce multiple leaders and thus a finer partition into sub-regions.  This illustrated in Figure 7.3 .\n\n\n\n\nFigure 7.3:\nCluster pruning.\n\n\n\n\nVariations of cluster pruning introduce additional parameters  and , both of which are positive integers. In the pre-processing step we attach each follower to its  closest leaders, rather than a single closest leader. At query time we consider the  leaders closest to the query . Clearly, the basic scheme above corresponds to the case . Further, increasing  or  increases the likelihood of finding  documents that are more likely to be in the set of true top-scoring  documents, at the expense of more computation. We reiterate this approach when describing clustering in Chapter 16  (page 16.1 ).\n\n\nExercises.\n\nWe suggested above (Figure 7.2 ) that the postings\nfor static quality ordering be in decreasing order of\n.  Why do we use the decreasing rather than the\nincreasing order?\n\n\n\nWhen discussing champion lists, we simply used the  documents with the largest tf values to create the champion list for . But when considering global champion lists, we used idf as well, identifying documents with the largest values of \n. Why do we differentiate between these two cases?\n\n\n\nIf we were to only have one-term queries, explain\nwhy the use of global champion lists with  suffices for\nidentifying the  highest scoring documents. What is a\nsimple modification to this idea if we were to only have\n-term queries for any fixed integer ?\n\n\n\nExplain how the common global ordering by  values in all high and low lists helps make the score computation efficient.\n\n\n\nConsider again the data of Exercise 6.4.4 with nnn.atc for the query-dependent scoring.  Suppose that we were given static quality scores of 1 for Doc1 and 2 for Doc2.  Determine under Equation 35 what ranges of static quality score for Doc3 result in it being the first, second or third result for the query best car insurance.\n\n\n\nSketch the frequency-ordered postings for the data in Figure 6.9 .\n\n\n\nLet the static quality scores for Doc1, Doc2 and Doc3 in Figure 6.11  be respectively 0.25, 0.5 and 1.  Sketch the postings for impact ordering when each postings list is ordered by the sum of the static quality score and the Euclidean normalized tf values in Figure 6.11 .\n\n\n\nThe nearest-neighbor problem in the plane is the following: given a set of  data points on the plane, we preprocess them into some data structure such that, given a query point , we seek the point in  that is closest to  in Euclidean distance.  Clearly cluster pruning can be used as an approach to the nearest-neighbor problem in the plane, if we wished to avoid computing the distance from  to every one of the query points.  Devise a simple example on the plane so that with two leaders, the answer returned by cluster pruning is incorrect (it is not the data point closest to ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Components of an information\n Up: Efficient scoring and ranking\n Previous: Impact ordering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}