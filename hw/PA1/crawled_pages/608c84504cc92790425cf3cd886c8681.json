{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html",
  "title": "Properties of Naive Bayes",
  "body": "\n\n\n\n\nProperties of Naive Bayes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: A variant of the\n Up: Text classification and Naive\n Previous: The Bernoulli model\n    Contents \n    Index\n\n\n\n\n  \n\nProperties of Naive Bayes\n \nTo gain a better understanding of the two models and the assumptions they make, let\nus go back and examine how we derived their classification\nrules in Chapters 11 12 .\nWe decide class membership of a document\nby assigning it to the class with the\n   probability\n(cf. probtheory), which we compute as\nfollows:\n\n\n\n\n\n\n\n\n(121)\n \n\n\n\n(122)\n \n\n\n\n(123)\n\n\nwhere Bayes' rule (Equation 59, page 59 ) is applied in\n(122) and\nwe drop the denominator in the last step because  is\nthe same for all classes and does not affect the argmax.\n\n\nWe can interpret\nEquation 123  as a  description of the\ngenerative process we assume in Bayesian text\nclassification. To generate a document, we first choose \nclass  with probability  (top nodes in\n and 13.5 ).\nThe two models \ndiffer in the formalization of the second step, the\ngeneration of the document given the class, corresponding to\nthe conditional distribution\n\n:\n\n\n\n\n\n\n\n\n(124)\n\n\n\n\n(125)\n\n\nwhere \n is the sequence of terms as it\noccurs in  (minus terms that were excluded from the vocabulary)\nand \n is a binary vector of\ndimensionality  that indicates for each term whether it\noccurs in  or not.\n\n\nIt should now be clearer why we introduced the\n document space\n in Equation 112 when we defined the classification problem.\nA critical step\nin solving a text classification problem\nis to choose the document\nrepresentation. \n\n and\n\n are two different\n document representations.\nIn the first case, \n is the set of all term sequences (or, more\nprecisely, sequences of term tokens).\nIn the second case, \n is\n.\n\n\nWe cannot use\n and 125  for text\nclassification directly.\nFor the Bernoulli model,\nwe would have to estimate \n different\nparameters, one for each possible combination of \nvalues  and a class. The number of parameters\nin the\nmultinomial case has the same order of\nmagnitude.This being a very large\nquantity, estimating these parameters reliably is\ninfeasible.\n\n\nTo reduce the number of parameters,\nwe make \nthe Naive Bayes   conditional independence\nassumption . We assume that attribute values are independent of\neach other given the class:\n\n\n\n\n\n\n\n\n(126)\n\n\n\n\n(127)\n\n\nWe \nhave introduced two random variables here to make the\ntwo different generative models explicit.\n\n \n  is the random variable for position\n in the document and takes as values terms from the\nvocabulary. \n\n is the probability that in a document of\nclass  the term  will occur in position .\n\n   is the random variable for\nvocabulary term\n and takes as values 0 (absence) and 1 (presence). \n\n \nis the probability that in a document of\nclass  the term  will occur - in any position and possibly\nmultiple times.\n\n\n\n\nFigure 13.4:\nThe multinomial NB model.\n\n\n\n\n\n\nFigure 13.5:\nThe Bernoulli NB model.\n\n\n\n\nWe illustrate\nthe conditional\nindependence assumption in\n and 13.5 . The class\nChina\ngenerates values for each of the five term attributes (multinomial) or\nsix binary attributes (Bernoulli) with a certain\nprobability, independent of the values of the other attributes.\nThe fact that a document in the class\nChina contains the term Taipei does not\nmake it more likely or less likely that it also contains\nBeijing. \n\n\nIn reality, the conditional independence assumption does not\nhold for text data. Terms are conditionally dependent\non each other. But as we will discuss shortly, NB models\nperform well despite the conditional independence\nassumption.\n\n\nEven when assuming conditional independence, we still have\ntoo many parameters for the multinomial model if\nwe assume a different probability distribution\nfor each position  in the\ndocument. The position of a term in a document by itself\ndoes not carry information about the class. Although there is a\ndifference between China sues France and France\nsues China, the occurrence of China in position 1\nversus position 3 of the document is not useful in NB\nclassification because we look at each term separately. \nThe conditional independence assumption commits\nus to this way of processing the evidence.\n\n\nAlso, if we assumed different term distributions for each\nposition , we would have to estimate a different set of\nparameters for each . The probability of bean\nappearing as the first term of a coffee document\ncould be different from it appearing as the second term, and\nso on.\nThis again causes problems in estimation owing to\ndata sparseness.\n\n\nFor these reasons, we make a second\nindependence assumption for the multinomial model,\n\n positional independence :\nThe conditional probabilities for a term are the same\nindependent of position in the document.\n\n\n\n\n\n\n(128)\n\n\nfor all positions \n, terms  and classes\n. Thus, we have a single distribution of\nterms that is valid for all positions  and we can use\n as its symbol.Positional\nindependence is equivalent to adopting the  bag of\nwords  model, which we introduced in the context of ad hoc\nretrieval in Chapter 6  (page 6.2 ).\n\n\nWith conditional and positional independence assumptions, we only need to estimate\n\n parameters \n (multinomial model) or\n\n (Bernoulli model), one for each term-class\ncombination, rather than a number that is at least exponential in\n, the size of the vocabulary.\nThe independence\nassumptions reduce the number of parameters to be estimated\nby several orders of magnitude.\n\n\nTo summarize, we generate a document in the multinomial\nmodel (Figure 13.4 ) by first picking a class \nwith  where    is a\n  random variable taking values\nfrom  as values. Next we generate term \n in position \nwith \n for each of the  positions of the\ndocument. The  all have the same \ndistribution over terms for a given . In the example in\nFigure 13.4 , we show the generation\nof \n, corresponding to\nthe one-sentence document\nBeijing and Taipei join WTO.\n\n\nFor a completely specified document generation model, we\nwould also have to define a distribution \n over\nlengths. Without it, the multinomial model is \na token generation model rather than a document\ngeneration model.\n\n\nWe generate a document in the Bernoulli model\n(Figure 13.5 ) by first picking a class  with\n and then generating a binary indicator  for\neach term  of the vocabulary\n(\n).\nIn the example in\nFigure 13.5 , we show the generation\nof \n, corresponding, again, to the one-sentence document\nBeijing and Taipei join WTO where we have\nassumed that\nand is a stop word.\n\n\n\n\n\n\nTable 13.3:\nMultinomial versus Bernoulli model.  \n\n  \nmultinomial model\nBernoulli model\n \n event model\ngeneration of token\ngeneration of document\n \n random variable(s)\n iff  occurs at given pos\n\n iff  occurs in doc\n \n document representation\n\n\n\n\n \n  \n \n    \n\n \n parameter estimation\n\n\n\n \n \n decision rule: maximize\n\n\n\n\n \n multiple occurrences\ntaken into account\nignored\n \n length of docs\ncan handle\nlonger docs\nworks best for short docs\n \n # features\ncan handle more\nworks best with fewer\n \n estimate for term the\n\n\n\n\n \n\n\n\n\nWe compare the two\nmodels in Table 13.3 , including estimation\nequations and decision rules.\n\n\n   Naive Bayes is so called because the\nindependence assumptions we have just made are indeed very\nnaive for a model of natural language. The conditional\nindependence assumption states that features are independent\nof each other given the class. This is hardly ever true for\nterms in documents. In many cases, the opposite is true.\nThe pairs hong and kong or london and\nenglish in Figure 13.7  are examples of highly\ndependent terms. In addition, the multinomial model makes an\nassumption of positional independence. The Bernoulli model\nignores positions in documents altogether because it only\ncares about absence or presence.  This  bag-of-words \nmodel discards all information that is communicated by the\norder of words in natural language sentences.  \nHow can NB be a good text classifier when its model of natural\nlanguage is so oversimplified?\n\n\n\n\n\n\nTable 13.4:\n  \nCorrect estimation implies accurate prediction, but accurate\nprediction does not imply correct estimation.\n  \n\n\nclass selected\n \n true probability \n0.6\n0.4\n\n \n \n\n (Equation 126)\n0.00099\n0.00001\n \n \n NB estimate \n0.99\n0.01\n\n \n\n\n\n\n  The answer is that\neven though the probability estimates of\nNB are of low quality, its classification\ndecisions are surprisingly good.  Consider a document \nwith true probabilities\n and\n as shown in Table 13.4 .\nAssume that  contains\nmany terms that are positive indicators for \nand many terms that are negative indicators for . \nThus, when using the\nmultinomial model in Equation 126,\n\n\nwill be much larger than \n\n (0.00099 vs. 0.00001 in the table).\nAfter division by 0.001 to get well-formed probabilities\nfor , we end up with one estimate that is close to\n1.0 and one that is close to 0.0. This is common:\nThe winning class in NB classification\nusually has a much larger probability than the other\nclasses and the estimates diverge very significantly from\nthe true probabilities. But the\nclassification decision is based on which class gets the\nhighest score. It does not matter how accurate the\nestimates are. Despite the bad estimates, NB \nestimates a\nhigher probability for  and therefore assigns \nto the correct class in Table 13.4 .  Correct estimation implies\naccurate prediction, but accurate prediction does not imply\ncorrect estimation. NB classifiers estimate badly,\nbut often classify well.\n\n\nEven if it is not the method with the highest accuracy for\ntext, NB has many virtues that make it a strong\ncontender for text classification. It excels if there are\nmany equally important features that jointly contribute to\nthe classification decision. It is also somewhat robust to\nnoise features (as defined in the next\nsection) and  concept\ndrift   - the gradual change over\ntime of the concept underlying a class like US\npresident from Bill Clinton to George W. Bush (see\nSection 13.7 ). Classifiers like\n kNN \nknn\ncan be carefully tuned to idiosyncratic properties of a\nparticular time period. This will then hurt them when\ndocuments in the following time period have slightly\ndifferent properties.\n\n\nThe Bernoulli model is particularly robust with respect to\nconcept drift.\nWe will see in Figure 13.8  that it can\nhave decent performance when using fewer than a dozen\nterms. The most important indicators for a class are less\nlikely to change. Thus, a model that only relies on these\nfeatures is more likely to maintain a certain level of\naccuracy in concept drift.\n\n\nNB's main strength is its efficiency:\nTraining and classification can be accomplished with one\npass over the data. Because it combines efficiency with good\naccuracy it is often used as a baseline in text\nclassification research. It is often the method of choice if\n(i) squeezing out a few extra percentage points of\naccuracy is not worth the trouble in a text classification\napplication, (ii) a very large amount of training data is\navailable and there is more to be gained from training on a\nlot of data than using a better classifier on a smaller\ntraining set, or (iii) if its robustness to concept drift\ncan be exploited.\n\n\n\n\n\n\nTable 13.5:\n   A set of documents for which\nthe NB independence assumptions are problematic.\n (1)\nHe moved from London, Ontario, to London, England.\n \n (2)\nHe moved from London, England, to London, Ontario.\n \n (3)\nHe moved from England to London, Ontario.\n \n\n\n\n\nIn this book, we discuss NB as a classifier for\ntext. The independence assumptions do not hold for\ntext. However, it can be shown that NB is an\n optimal classifier \n(in the sense of minimal error rate on\nnew data) for data where the independence\nassumptions do hold.\n\n\n\n\nSubsections\n\nA variant of the multinomial model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: A variant of the\n Up: Text classification and Naive\n Previous: The Bernoulli model\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}