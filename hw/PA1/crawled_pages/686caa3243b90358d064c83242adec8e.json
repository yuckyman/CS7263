{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/an-example-information-retrieval-problem-1.html",
  "title": "An example information retrieval problem",
  "body": "\n\n\n\n\nAn example information retrieval problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: A first take at\n Up: Boolean retrieval\n Previous: Boolean retrieval\n    Contents \n    Index\n\n\n\n \n\nAn example information retrieval problem\n\n\nA fat book which many people own is Shakespeare's Collected Works.\nSuppose you wanted to determine which plays of Shakespeare contain the\nwords Brutus AND Caesar and NOT\nCalpurnia.  One way\nto do that is to start at the beginning and to read through all the\ntext, noting for each play whether it contains Brutus and\nCaesar and excluding it from consideration if it contains\nCalpurnia.\nThe simplest form of document retrieval is for a computer to do this\nsort of linear scan\nthrough documents.  This process is commonly referred to as\n grepping  through text, after the Unix command grep, which\nperforms this process.  Grepping through text can be a very effective\nprocess, especially given the speed of modern computers, and often\nallows useful possibilities for wildcard\npattern matching through the\nuse of   .\nWith modern computers, for simple querying of modest\ncollections (the size of Shakespeare's Collected Works is a bit under\none million words of text in total), you really need nothing more.\n\n\nBut for many purposes, you do need more:\n\n\nTo process large document collections quickly. The amount of\n  online data has grown at least\nas quickly as the speed of computers, and we would now like to be able\nto search collections that total in the order of billions to trillions\nof words.\n\nTo allow more flexible matching operations.  For example, it is\n  impractical to perform the query\n  Romans NEAR countrymen with grep, where\n  NEAR might be defined as ``within 5 words'' or ``within the\n  same sentence''.\n\nTo allow ranked retrieval: in many cases you want the best answer\n  to an information need among many documents that contain certain words.\n\n\n\nThe way to avoid linearly scanning the texts for each query is\nto  index  the documents in advance.  Let us stick with\nShakespeare's Collected Works, and\nuse it to introduce the basics of the Boolean\nretrieval model.   Suppose we record for each\ndocument - here a play of Shakespeare's - whether it contains each word\nout of all the words Shakespeare used (Shakespeare used about 32,000\ndifferent words).  The result is a binary term-document \n incidence matrix , as in Figure 1.1 .   Terms  are the indexed units (further discussed in Section 2.2 ); they are usually words, and for the moment you can think of them as words, but the information retrieval\nliterature normally speaks of terms because some of them, such as perhaps I-9\nor Hong Kong are not usually thought of as words.\nNow, depending on whether we look at the\nmatrix rows or columns, we can have a vector for each term, which shows\nthe documents it appears in, or a vector for each document, showing the\nterms that occur in it.\n\n\n\n\n\nTo answer the query Brutus AND Caesar AND\nNOT\nCalpurnia, we take the vectors for Brutus, Caesar\nand Calpurnia, complement the last, and then do a bitwise AND:\n\n110100 AND 110111 AND 101111 = 100100\n\n\nThe answers for this query are thus Antony and Cleopatra\nand Hamlet (Figure 1.2 ).\n\n\nThe\n Boolean retrieval model \nis a model for information\nretrieval in which we can pose any query which is in the form of a Boolean expression of terms,\nthat is, in which terms are combined with\nthe operators and, or, and not.\nThe model views each document as just a set of words.\n\n\n\n\nFigure:\nResults from Shakespeare for the query\nBrutus AND Caesar AND NOT\nCalpurnia.\n\n\n\n\nLet us now consider a more realistic scenario, simultaneously using the\nopportunity to introduce some terminology and notation.\nSuppose we have \n documents.  By\n documents \nwe mean whatever units we have decided to build a retrieval system\nover.  They might be individual memos or chapters of a book (see\nSection 2.1.2 (page ) for further discussion).\nWe will refer to the group of documents over which we\nperform retrieval as the (document)  collection .  It is sometimes\nalso referred to as a  corpus  (a body of texts).\nSuppose each document is about 1000 words long (2-3 book pages).\nIf we assume an average of 6 bytes per word including spaces and\npunctuation, then this is a document collection about 6 GB in size.\nTypically, there might be about distinct terms in these\ndocuments.  There is nothing special about the numbers\nwe have chosen, and they might vary by an order of magnitude or more, but they\ngive us some idea of the dimensions of the kinds of problems we need to\nhandle.  We will discuss and model these size assumptions in\nSection 5.1 (page ).\n\n\nOur goal is to develop a system to address\nthe  ad hoc retrieval  task.  This is the most standard IR task.\nIn it, a system aims to provide documents from within the collection that are\nrelevant to an arbitrary user information need, communicated to the\nsystem by means of a one-off, user-initiated query.  An\n information need  is the topic about which the user desires to\nknow more, and is differentiated from a  query , which is what\nthe user conveys to the computer in an attempt to communicate the\ninformation need.\nA document is  relevant  if it is\none that the user perceives as containing information of value with\nrespect to their personal information need.\nOur example above was rather artificial in that the\ninformation need was defined in terms of particular words, whereas\nusually a user is interested in a topic like ``pipeline leaks'' and\nwould like to find relevant documents regardless of whether they\nprecisely use those words or express the concept with other words such\nas pipeline rupture.\nTo assess the  effectiveness  of an IR system (i.e., the\nquality of its search results), a user will usually want to know two key\nstatistics about the system's returned results for a query:\n\n\n Precision : What fraction of\nthe returned results are relevant to the information need?\n\n\n Recall : What fraction of the relevant documents in\n  the collection were returned by the system?\n\n\nDetailed discussion of relevance and evaluation measures including\nprecision and recall is found in Chapter 8 .\n\n\nWe now cannot build a term-document matrix in a naive way.\nA \n matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory.\nBut the crucial observation is that the matrix is extremely sparse, that\nis, it has few non-zero entries.  Because each document is 1000 words\nlong, the matrix has no more than one\nbillion 1's, so a minimum of 99.8% of the cells are zero.\nA much better representation is to record only the things\nthat do occur, that is, the 1 positions.\n\n\n \nThis idea is central to the first major concept in information\nretrieval, the  inverted index .  The name is actually redundant:\nan index always maps back from terms to the parts of a\ndocument where they occur. Nevertheless, inverted index, or\nsometimes  inverted file , has\nbecome the standard term\nin information retrieval.The basic idea of an inverted index is shown\nin Figure 1.3 .  We keep a  dictionary \nof terms (sometimes also referred to as a  vocabulary  or  lexicon ; in this book, we use dictionary for the data structure and vocabulary for the set of terms).\nThen for each term, we have a list that records which documents the\nterm occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called\na  posting .The list is then called a  postings list \n(or   ),\nand all the postings\nlists taken together are referred to as the\n postings .\nThe dictionary in Figure 1.3  has been sorted\nalphabetically and each postings list is\nsorted by document ID.  We will see why this is useful in\nSection 1.3 , below, but later we will\nalso consider\nalternatives to doing this (Section 7.1.5 ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: A first take at\n Up: Boolean retrieval\n Previous: Boolean retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}