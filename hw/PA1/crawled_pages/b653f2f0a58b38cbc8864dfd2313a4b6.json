{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html",
  "title": "Evaluation of ranked retrieval results",
  "body": "\n\n\n\n\nEvaluation of ranked retrieval results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Assessing relevance\n Up: Evaluation in information retrieval\n Previous: Evaluation of unranked retrieval\n    Contents \n    Index\n\n\n\n \n\nEvaluation of ranked retrieval results\n\n\n\n\nFigure 8.2:\nPrecision/recall graph.\n\n\n\n\nPrecision, recall, and the F measure are set-based measures.  They are\ncomputed using unordered sets of documents.  We need to extend these\nmeasures (or to define new measures) if we are to evaluate the ranked\nretrieval results that are now standard with search engines.\nIn a ranked retrieval context, appropriate sets of retrieved documents are\nnaturally given by the top  retrieved documents.  For each such set,\nprecision and recall values can\nbe plotted to give a  precision-recall curve , such as the one shown\nin Figure 8.2 .  Precision-recall curves have a\ndistinctive saw-tooth shape: if the \ndocument retrieved is nonrelevant then recall is the same as for the top\n documents, but precision has dropped.  If it is relevant, then both\nprecision and recall increase, and the curve jags up and to the right.\nIt is often useful to remove these jiggles and the standard way to do\nthis is with an interpolated precision: the   interpolated precision\n at a certain\nrecall level  is defined as the highest precision found for any recall\nlevel :\n\n\n\n\n\n\n(42)\n\n\n\nThe justification is that almost anyone would be\nprepared to look at a few more documents if it would increase the\npercentage of the viewed set that were relevant (that is, if the\nprecision of the larger set is higher).  Interpolated precision\nis shown by a thinner line in Figure 8.2 .  With\nthis definition, the interpolated precision at a recall of 0 is\nwell-defined (Exercise 8.4 ). \n\n\n\n\n\nRecall\nInterp.\n\n \nPrecision\n\n0.0\n1.00\n\n0.1\n0.67\n\n0.2\n0.63\n\n0.3\n0.55\n\n0.4\n0.45\n\n0.5\n0.41\n\n0.6\n0.36\n\n0.7\n0.29\n\n0.8\n0.13\n\n0.9\n0.10\n\n1.0\n0.08\n\n\nCalculation of 11-point Interpolated Average\n  Precision.This is for the precision-recall curve shown in Figure 8.2 . \n\n\n\nExamining the entire precision-recall curve is very informative,\nbut there is often a desire to\nboil this information down to a few numbers, or perhaps even a single\nnumber. The traditional way of\ndoing this (used for instance in the first 8 TREC Ad Hoc evaluations)\nis the  11-point interpolated average precision .  For each information\nneed, the interpolated precision is measured at the 11 recall levels of\n0.0, 0.1, 0.2, ..., 1.0.  For the precision-recall curve in\nFigure 8.2 , these 11 values are shown in Table 8.1 . \nFor each recall level, we then calculate\nthe arithmetic mean of \nthe interpolated precision at that recall level for each information\nneed in the test collection.   A composite precision-recall curve showing 11\npoints can then be graphed.  Figure 8.3  shows an example graph of\nsuch results from a representative good system at TREC 8.\n\n\n\n\nAveraged 11-point precision/recall graph across 50 queries for\n  a representative TREC system.The Mean Average Precision for this system is\n  0.2553.\n\n\nIn recent years, other measures have become more common.  Most\nstandard among the TREC community is \n\n Mean Average Precision\n \n  (MAP), which provides a single-figure measure of quality across recall\nlevels.  Among evaluation measures, MAP has been shown to have\nespecially good discrimination and stability.\nFor a single information need, Average Precision is the average of the precision\nvalue obtained for the set of top \n documents existing after each relevant document is retrieved, and this value is then averaged over information needs.\nThat is, if the set of relevant documents for an information need  is \n and \n is the set of ranked retrieval results from the top result until\nyou get to document , then\n\n\n\n\n\n\n(43)\n\n\nWhen a relevant document is not retrieved at all,the precision value in the above equation is\ntaken to be 0.  For a single information need, the average precision approximates the area under the \nuninterpolated precision-recall curve, and so the MAP is roughly the average area under the precision-recall curve for a set of queries.\n\n\nUsing MAP, fixed recall levels are not chosen,\nand there is no interpolation.  The MAP value for a test collection is\nthe arithmetic mean of average precision values for individual information\nneeds.  (This has the effect of weighting each information\nneed equally in the final reported number, even if many documents are\nrelevant to some queries whereas very few are relevant to other queries.)\nCalculated MAP\nscores normally vary widely across information needs when measured\nwithin a single\nsystem, for instance, between 0.1 and 0.7.  Indeed, there is normally more\nagreement in MAP for an individual information need across systems than\nfor MAP scores for different information needs for the same system.\nThis means that a set of test information needs must be large and\ndiverse enough to be representative of system effectiveness\nacross different queries.\n\n\n   The\nabove measures factor in precision at all recall levels.\nFor many prominent applications, particularly web search,\nthis may not be germane to users.  What matters is\nrather how many good results there are on the first page or\nthe first three pages.  This leads to measuring precision at\nfixed low levels of retrieved results, such as 10 or 30\ndocuments.  This is referred to as ``Precision at '', for\nexample ``Precision at 10''.  It has the advantage of not\nrequiring any estimate of the size of the set of relevant\ndocuments but the disadvantages that it is the least stable of the\ncommonly used evaluation measures and that it does not average\nwell, since the total number of relevant documents for a\nquery has a strong influence on precision at .\n\n\nAn alternative, which alleviates this problem, is  R-precision .\nIt requires having a set of known  \nrelevant documents , from which we calculate the\nprecision of the top  documents returned.\n(The set  may be incomplete, such as when  is formed by\ncreating relevance judgments for the pooled top  results of\nparticular systems in a set of experiments.)  \nR-precision adjusts for the size\nof the set of relevant documents: A perfect system could score\n1 on this metric for each query, whereas, even a perfect system could\nonly achieve a precision at 20 of 0.4 if there were only 8\ndocuments in the collection relevant to an information need.  \nAveraging this measure across queries thus makes more sense.  This measure\nis harder to explain to naive users than Precision at  but easier to explain than MAP.\nIf there are  relevant documents for a query, we examine the top\n results of a system, and find that  are relevant, then by\ndefinition, not only is the precision (and hence R-precision) ,\nbut the recall of this result set is also .  Thus, R-precision\nturns out to be identical to the\n  break-even point , another measure\nwhich is sometimes used, defined in terms of this equality relationship holding. \nLike Precision at , R-precision describes only one point on\nthe precision-recall curve, rather than attempting to summarize\neffectiveness across the curve, and it is somewhat unclear why you should\nbe interested in the break-even point rather than either the best point on the curve (the\npoint with maximal F-measure) or a retrieval level of interest to a particular application (Precision at ).  Nevertheless, R-precision turns out to be highly correlated with MAP empirically, despite measuring only a single point on the curve.\n\n\n\n\nFigure 8.4:\nThe ROC curve corresponding to the precision-recall curve in\n  Figure 8.2 .\n\n.\n\n\n\nAnother concept sometimes used in evaluation is an  ROC curve .\n(``ROC'' stands for ``Receiver Operating \nCharacteristics'', but knowing that doesn't help most people.)\nAn ROC curve plots the true positive rate or sensitivity against the\nfalse positive rate or (\n).  Here,\n sensitivity  is just another term for recall. \nThe false positive rate is given by . \nFigure 8.4  shows the ROC curve\ncorresponding to the precision-recall curve in\nFigure 8.2 .  An ROC curve always goes from the bottom\nleft to the top right of the graph.  For a good system, the graph climbs\nsteeply on the left side.  \nFor unranked result sets,  specificity , given by ,\nwas not seen as a very useful notion. Because the set\nof true negatives is always so large, its\nvalue would be almost 1 for all information needs (and,\ncorrespondingly, the value of \nthe false positive rate would be almost 0). That is, the\n``interesting'' part of Figure 8.2  is\n\n, a part which is compressed to a small\ncorner of Figure 8.4 .  But an ROC curve could make sense when\nlooking over the full retrieval spectrum, and it provides another way of\nlooking at the data. \nIn many fields, a common aggregate measure is to report\nthe area under the ROC curve, which is the ROC analog of MAP.\nPrecision-recall curves \nare sometimes loosely referred to as ROC curves.  This is\nunderstandable, but not accurate.\n\n\nA final approach that has seen increasing adoption,\nespecially when employed with machine learning approaches to\nranking svm-ranking is measures of\n cumulative gain , and in particular  normalized\n  discounted cumulative gain   ( NDCG ).  NDCG is\ndesigned for situations of non-binary notions of relevance\n(cf. Section 8.5.1 ).  Like precision at , it is\nevaluated over some number  of top search results. For a set of\nqueries , let\n be the relevance score assessors gave to document\n for query .  Then,\n\n\n\n\n\n\n(44)\n\n\nwhere  is a normalization factor calculated to make it so that a\nperfect ranking's NDCG at  for query  is 1.  For queries for which  documents are retrieved, the last summation is done up to .\n\n\nExercises.\n\n What are the possible values for\ninterpolated precision at a recall level of 0?\n\n\n\nMust there always be a break-even\npoint between precision and recall?  Either show there must\nbe or give a counter-example.  \n\n\n\nWhat\nis the relationship between the value of  and the\nbreak-even point?\n\n\n\nThe  Dice coefficient  of two sets is a measure of\ntheir intersection scaled by their size (giving a value in the range 0\nto 1): \n\n\n\n\n\n\n(45)\n\n\nShow that the balanced F-measure () is equal to the Dice\ncoefficient of the retrieved and relevant document sets.\n\n\n\nConsider an information need for which there are 4 relevant documents in the collection.\nContrast two systems run on this collection.  Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result):\n\n\nSystem 1\n R\nN\nR\nN\nN\n N\nN\nN\nR\nR\n\nSystem 2\n N\nR\nN\nN\nR\n R\nR\nN\nN\nN\n\n\n\n\nWhat is the MAP of each system?  Which has a higher MAP?\n\nDoes this result intuitively make sense?  What does it say about what is important in getting a good MAP score?\n\nWhat is the R-precision of each system?  (Does it rank the systems the same as MAP?)\n\n\n\n\nThe following list of Rs and Ns represents relevant (R) and\nnonrelevant (N) returned documents in a ranked list of 20 documents\nretrieved in response to a query from a collection of 10,000\ndocuments. The top of the ranked list (the document the system thinks is\nmost likely to be relevant) is on the left of the list. This list shows\n6 relevant documents.  Assume that there are 8 relevant documents in\ntotal in the collection.\n\n\nR\nR\nN\nN\nN\n N\nN\nN\nR\nN\n R\nN\nN\nN\nR\n N\nN\nN\nN\nR\n\n\n\n\nWhat is the precision of the system on the top 20?\n\nWhat is the F on the top 20?\n\nWhat is the uninterpolated precision of the system at 25% recall?\n\nWhat is the interpolated precision at 33% recall?\n\nAssume that these 20 documents are the complete result set of the\n  system. What is the MAP for the query?\n\n\nAssume, now, instead, that the system returned the entire 10,000\ndocuments in a ranked list, and these are the first 20 results returned.\n\nf.\nWhat is the largest possible MAP that this\n  system could have?\n\ng.\nWhat is the smallest possible MAP that this\n  system could have?\n\nh.\nIn a set of experiments, only the top 20 results are evaluated by\n  hand.  The result in (e) is used to approximate the range\n  (f)-(g).  For this example, how large (in absolute terms) can the\n  error for the MAP be by calculating (e) instead of (f) and (g) for\n  this query?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Assessing relevance\n Up: Evaluation in information retrieval\n Previous: Evaluation of unranked retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}