{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-14.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Exercises\n Up: Vector space classification\n Previous: The bias-variance tradeoff\n    Contents \n    Index\n\n\n\n \n\nReferences and further reading\n\n\nAs discussed in Chapter 9 , Rocchio relevance\nfeedback is due to Rocchio (1971).\nJoachims (1997) presents a probabilistic\nanalysis of the method.  Rocchio classification was widely used as a\nclassification method in    in the\n1990s\n(Buckley et al., 1994b;a, Voorhees and Harman, 2005).\nInitially, it was used as a form of  routing . Routing\nmerely ranks documents according to relevance to a class\nwithout assigning them. Early work on  filtering , a\ntrue classification approach that makes an assignment\ndecision on each document, was published by\nIttner et al. (1995) and Schapire et al. (1998).  The\ndefinition of routing we use here should not be confused\nwith another sense. Routing can also refer to the electronic\ndistribution of documents to subscribers, the so-called\n push model  of document distribution.  In a\n pull model , each transfer of a document to the user\nis initiated by the user - for example, by means of search\nor by selecting it from a list of documents on a news\naggregation website.\n\n\nSome authors restrict the name Roccchio\n  classification to two-class problems and use the terms\n   cluster-based \n(Iwayama and Tokunaga, 1995)\nand   centroid-based classification \n  (Han and Karypis, 2000, Tan and Cheng, 2007) for Rocchio classification with .\n\n\nA more detailed treatment of kNN can be found in\n(Hastie et al., 2001), including methods for\ntuning the\nparameter . An example of an approximate fast kNN\nalgorithm is locality-based hashing\n(Andoni et al., 2006).  Kleinberg (1997) presents\nan approximate \n kNN\nalgorithm (where  is the dimensionality of the space and\n the number of data points), but at the cost of\nexponential storage requirements: \n.\nIndyk (2004) surveys nearest neighbor methods in\nhigh-dimensional spaces.\nEarly work on kNN in text classification was motivated by\nthe availability of massively parallel hardware architectures\n(Creecy et al., 1992).\nYang (1994) uses an inverted index to speed up kNN\nclassification.  The optimality result for 1NN (twice the\nBayes error rate asymptotically) is due to\nCover and Hart (1967). \n\n\nThe effectiveness of Rocchio classification and kNN is\nhighly dependent on careful parameter tuning (in particular,\nthe parameters  for Rocchio on page 14.2  and\n for kNN), feature\nengineering svm-text and feature selection\nfeature.\nBuckley and Salton (1995), Yang and Kisiel (2003), Schapire et al. (1998)\nand Moschitti (2003)\naddress these issues for Rocchio and\nYang (2001) \nand Ault and Yang (2002) \nfor kNN.\nZavrel et al. (2000) compare\nfeature selection methods for kNN.\n\n\nThe bias-variance tradeoff was introduced by\nGeman et al. (1992).  \nThe derivation in Section 14.6  is for  \n,\nbut the\ntradeoff applies to \nmany loss functions\n(cf. Friedman (1997), Domingos (2000)).\nSchütze et al. (1995) and\nLewis et al. (1996) discuss linear classifiers for text\nand Hastie et al. (2001) linear classifiers in general.\nReaders interested in the algorithms mentioned, but not\ndescribed in this chapter may wish to consult\nBishop (2006) for neural networks,\nHastie et al. (2001) for linear and logistic\nregression, and Minsky and Papert (1988) for the  perceptron\nalgorithm .  \nAnagnostopoulos et al. (2006) show that an inverted index\ncan be used for highly\nefficient document classification with any linear\nclassifier, provided that the\nclassifier is still effective when trained on a modest\nnumber of features via feature\nselection.\n\n\nWe have only presented the simplest method for combining\ntwo-class classifiers into a one-of classifier. Another\nimportant method is the use of error-correcting codes, where\na vector of decisions of different two-class classifiers is\nconstructed for each document. A test document's decision vector is\nthen ``corrected'' based on the distribution of decision\nvectors in the training set, a procedure that incorporates\ninformation from all two-class classifiers and their\ncorrelations into the final classification decision\n(Dietterich and Bakiri, 1995). \nGhamrawi and McCallum (2005)\nalso exploit dependencies between classes in any-of classification.\nAllwein et al. (2000)\npropose a general framework for combining two-class\nclassifiers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Exercises\n Up: Vector space classification\n Previous: The bias-variance tradeoff\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}