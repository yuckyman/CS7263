{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/zipfs-law-modeling-the-distribution-of-terms-1.html",
  "title": "Zipf's law: Modeling the distribution of terms",
  "body": "\n\n\n\n\nZipf's law: Modeling the distribution of terms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Dictionary compression\n Up: Statistical properties of terms\n Previous: Heaps' law: Estimating the\n    Contents \n    Index\n\n\n\n\n \n\nZipf's law: Modeling the distribution of terms\n \nWe also want to understand\nhow terms are distributed across documents. This\nhelps us to characterize the properties of the\nalgorithms for compressing postings lists in Section 5.3 .\n\n\nA commonly used model of the distribution of terms in a\ncollection is  Zipf's law . It states that, if\n is the\nmost common term in the collection,  is the next most\ncommon, and so on, then the collection frequency\n of the th most common\nterm is proportional to :\n\n\n\n\n\n \n \n\n(3)\n\n\nSo if the most frequent term occurs  times, then the\nsecond most frequent term has half as many occurrences, the third\nmost frequent term a third as many occurrences, and so on. The\nintuition is that frequency decreases very rapidly with\nrank. Equation 3  is one of the simplest ways of\nformalizing such a rapid decrease and it has been found to\nbe a reasonably good model.\n\n\nEquivalently, we can write Zipf's law as \n or\nas \n where  and  is\na constant to be defined in Section 5.3.2 . It is therefore a\n power law  with exponent . See\nChapter 19 , page 19.2.1 , for another\npower law, a law characterizing the distribution of links on web\npages.\n\n\n\n\n \nZipf's law for Reuters-RCV1.\nFrequency is plotted as a function of frequency rank for\nthe terms in the \ncollection. The line is the distribution predicted by Zipf's\nlaw (weighted least-squares fit; intercept is 6.95).\n\n\n\n\nThe log-log graph in Figure 5.2  plots the\ncollection frequency of a term as a function of its rank for\nReuters-RCV1. A line with slope -1, corresponding to\nthe Zipf function \n, is also shown. \nThe fit of the data to the law is not particularly good, but\ngood enough to serve as a model for term distributions in\nour calculations in Section 5.3 .\n\n\nExercises.\n\nAssuming one machine word per posting, what is the\nsize of the uncompressed (nonpositional) index for\ndifferent tokenizations based on Table 5.1 ? How do these\nnumbers compare with Table 5.6 ?\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n Next: Dictionary compression\n Up: Statistical properties of terms\n Previous: Heaps' law: Estimating the\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}