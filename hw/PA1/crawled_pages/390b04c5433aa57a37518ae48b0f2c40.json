{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/result-ranking-by-machine-learning-1.html",
  "title": "Result ranking by machine learning",
  "body": "\n\n\n\n\nResult ranking by machine learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Machine learning methods in\n Previous: A simple example of\n    Contents \n    Index\n\n\n\n \n\nResult ranking by machine learning\n\n\nThe above ideas can be readily generalized to \nfunctions of many more than two variables.  There are lots of other\nscores that are indicative of the relevance of a document to a query,\nincluding static quality (PageRank-style measures, discussed in\nChapter 21 ), document age, zone contributions, document length,\nand so on.\nProviding that these measures can be calculated for a training\ndocument collection with relevance judgments, any number of such\nmeasures can be used to train a machine learning classifier.  For\ninstance, we could train an SVM over binary relevance judgments, and\norder documents based on their probability of relevance, which is\nmonotonic with the documents' signed distance from the decision\nboundary.  \n\n\nHowever, approaching IR result ranking like this is not necessarily the right way to \nthink about the problem.  Statisticians normally first divide problems\ninto  classification  problems (where a categorical variable is\npredicted) versus  regression  problems (where a real number is\npredicted).  In between is the specialized field of  ordinal\nregression  where a ranking is predicted.  Machine learning for ad hoc\nretrieval is most properly thought of as an ordinal\nregression problem, where the goal is to rank a set of documents for a\nquery, given training data of the same sort.  This formulation gives\nsome additional power, since documents can be evaluated relative to\nother candidate documents for the same query, rather than having to be\nmapped to a global scale of goodness, while also weakening the problem\nspace, since just a ranking is required rather than an absolute\nmeasure of relevance.  Issues of ranking are especially germane in web\nsearch, where the ranking at the very top of the results list is\nexceedingly important, whereas decisions of relevance of a document to\na query may be much less important.  Such work can and has been\npursued using the  structural SVM  framework which we mentioned in\nSection 15.2.2 , where the class being predicted is a ranking\nof results for a query, but here we will present the slightly simpler\nranking SVM. \n\n\nThe construction of a  ranking SVM  proceeds as follows.  We\nbegin with a set of judged queries.  For each training query , we\nhave a set of documents returned in response to the query, which have\nbeen totally ordered by a person for relevance to the query.\nWe construct a vector of features \n for each document/query pair, using features such as those\ndiscussed in Section 15.4.1 , and many more.  For two\ndocuments  and , we then form the vector of feature differences:\n\n\n\n\n\n\n(180)\n\n\n\nBy hypothesis, one of  and  has been judged more relevant.\nIf  is judged more relevant than , denoted \n( should precede  in the results ordering), then we will assign the vector \n\n the class ; otherwise .   The goal then is\nto build a classifier which will return\n\n\n\n\n\n\n(181)\n\n\nThis SVM learning task is formalized in a manner much like the other\nexamples that we saw before:\n\n\n\nWe can leave out  in the statement of the\nconstraint, since we only need to consider the constraint for document\npairs ordered in one direction, since \nis antisymmetric.  These constraints are then solved, as before, to give a linear\nclassifier which can rank pairs of documents.  This approach has been\nused to build ranking functions which outperform standard hand-built\nranking functions in IR evaluations on standard data sets; see the\nreferences for papers that present such results.\n\n\nBoth of the methods that we have just looked at use \na linear weighting of document features that are\nindicators of relevance, as has most work in this area.  It is\ntherefore perhaps interesting to note that much of traditional IR\nweighting involves nonlinear scaling of basic measurements\n(such as log-weighting of term frequency, or idf).   At the present\ntime, machine learning is very good at producing optimal weights for\nfeatures in a linear combination (or other similar restricted model\nclasses), but it is not good at coming up with good nonlinear scalings\nof basic measurements.  This area remains the domain of\nhuman feature engineering.\n\n\nThe idea of learning ranking functions has been around for a number of\nyears, but it is only very recently that sufficient machine learning\nknowledge, training document collections, and computational power have\ncome together to make this method practical and exciting.  It is thus\ntoo early to write something definitive on machine learning\napproaches to ranking in information retrieval, but there is every\nreason to expect the use and importance of machine learned ranking\napproaches to grow over time.  While skilled humans can do a very\ngood job at defining ranking functions by hand, hand tuning is\ndifficult, and it has to be done again for each new document\ncollection and class of users.\n\n\nExercises.\n\nPlot the first 7 rows of Table 15.3  in\nthe - plane to produce a figure like that in\nFigure 15.7 .\n\n\n\nWrite down the equation of a line in the - plane separating the Rs from the Ns.\n\n\n\nGive a training example (consisting of values for \n and the relevance judgment) that when added to the training set makes it impossible to separate the R's from the N's using a line in the - plane.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Machine learning methods in\n Previous: A simple example of\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}