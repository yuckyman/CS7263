{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-5.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Scoring, term weighting and\n Up: Index compression\n Previous: Gamma codes\n    Contents \n    Index\n\n\n\n\n \n\nReferences and further reading\n\n\nHeaps' law was discovered by Heaps (1978). See\nalso Baeza-Yates and Ribeiro-Neto (1999). A detailed study of vocabulary\ngrowth in large collections is (Williams and Zobel, 2005).\nZipf's law is due to Zipf (1949).\nWitten and Bell (1990) investigate the quality of the fit\nobtained by the law.  Other term distribution models,\nincluding K mixture and two-poisson model, are discussed by\nManning and Schütze (1999, Chapter 15).\nCarmel et al. (2001), Büttcher and Clarke (2006),\nBlanco and Barreiro (2007), and Ntoulas and Cho (2007) show that lossy compression can\nachieve good compression with no or no significant decrease\nin retrieval effectiveness.\n\n\nDictionary compression is covered in detail by\nWitten et al. (1999, Chapter 4), which is recommended as\nadditional reading.\n\n\nSubsection 5.3.1 is based on\n(Scholer et al., 2002). The authors find that variable byte codes\nprocess queries two times faster than either bit-level compressed\nindexes or uncompressed indexes with a 30% penalty in\ncompression ratio compared with the best bit-level compression\nmethod. They also show that compressed indexes can be\nsuperior to uncompressed indexes not only in disk usage, but\nalso in query processing speed.  Compared with VB codes,\n``variable nibble'' codes showed 5% to 10% better\ncompression and up to one third worse effectiveness\nin one experiment (Anh and Moffat, 2005).\nTrotman (2003) also recommends using VB codes\nunless disk space is at a premium. In recent work,\nAnh and Moffat (2006a;2005) and Zukowski et al. (2006)\nhave constructed word-aligned binary codes that are both\nfaster in decompression and at least as efficient as VB\ncodes.\nZhang et al. (2007) investigate \nthe increased effectiveness of caching when a number of\ndifferent compression techniques for postings lists are used\non modern hardware.\n\n\n codes (Exercise 5.3.2 ) and  codes\nwere introduced by Elias (1975), who proved that\nboth codes are universal. In addition,  codes are\nasymptotically optimal for \n.\n codes perform better than  codes if large\nnumbers (greater than 15) dominate.  A good introduction to\ninformation theory, including the concept of\n entropy , is (Cover and Thomas, 1991).\nWhile Elias codes are only asymptotically optimal,\narithmetic codes \n(Witten et al., 1999, Section 2.4)\ncan be constructed to be\narbitrarily close to the optimum  for any\n. \n\n\nSeveral additional index compression techniques are covered\nby Witten et al. (1999; Sections 3.3 and 3.4 and Chapter 5). \nThey recommend using \n\n parameterized codes \nfor\nindex compression, codes that explicitly model the\nprobability distribution of gaps for each term. For example,\nthey show that   Golomb codes  achieve better\ncompression ratios than  codes for large\ncollections.  Moffat and Zobel (1992) compare several\nparameterized methods, including LLRUN\n(Fraenkel and Klein, 1985).\n\n\nThe distribution of gaps in a postings list depends on the\nassignment of docIDs to documents. A number of researchers\nhave looked into assigning docIDs in a way that is conducive\nto the efficient compression of gap sequences\n(Moffat and Stuiver, 1996; Blandford and Blelloch, 2002; Silvestri et al., 2004; Blanco and Barreiro, 2006; Silvestri, 2007).\nThese techniques assign docIDs in a small range to documents\nin a cluster where a cluster can consist of all documents in\na given time period,  on a particular web site, or sharing\nanother property.\nAs a result,\nwhen a sequence of documents from a cluster occurs in a\npostings list, their gaps are small and can be more\neffectively compressed.\n\n\nDifferent considerations apply to the compression of term\nfrequencies and word positions than to the compression of\ndocIDs in postings lists. See Scholer et al. (2002) and\nZobel and Moffat (2006).  Zobel and Moffat (2006) is\nrecommended in general as an in-depth and up-to-date tutorial\non inverted indexes, including index compression.\n\n\nThis chapter only looks at index compression for Boolean\nretrieval. For  ranked retrieval \n(Chapter 6 ), it is advantageous to order postings\naccording to term frequency instead of docID. During query\nprocessing, the scanning of many postings lists can then be\nterminated early because smaller weights do not change the\nranking of the highest ranked  documents found so\nfar. \nIt is not a good idea to precompute and\nstore weights in the index (as opposed to frequencies)\nbecause they cannot be compressed as well as integers\n(see impactordered).\n\n\nDocument compression can also be important in an\nefficient information retrieval system.  de Moura et al. (2000)\nand Brisaboa et al. (2007) describe\ncompression schemes that allow direct searching of terms and\nphrases in the compressed text, which is infeasible with\nstandard text compression utilities like gzip and\ncompress.\n\n\nExercises.\n\nWe have defined unary codes as being ``10'': sequences of 1s\nterminated by a 0. Interchanging the roles of 0s and 1s\nyields an equivalent ``01'' unary code. When this 01 unary\ncode is used, the construction of a\n code can be stated as follows: (1) Write\n down in binary using \n bits. (2) Prepend  0s.\n(i) Encode the numbers in Table 5.5  in this alternative\n code.\n(ii)\nShow that this method produces \na well-defined alternative\n code in the sense that it has the same length and can\nbe uniquely decoded.\n\n\n\nUnary code is not a universal code in the sense\ndefined above. However, there exists a distribution over gaps\nfor which unary code is optimal. Which distribution is this?\n\n\n\nGive some examples of terms that violate the\nassumption that gaps all have the same size (which we made\nwhen estimating the space requirements of a -encoded\nindex). What are\ngeneral characteristics of these terms?\n\n\n\nConsider a term whose postings list has size ,\nsay, . Compare the size of the\n-compressed gap-encoded postings list if the distribution of the\nterm is uniform (i.e., all gaps have the same size) versus\nits size when the distribution is not uniform. Which\ncompressed postings list is smaller?\n\n\n\nWork out the sum \nin Equation 12\nand show it adds up to about\n251 MB. Use the numbers in Table 4.2 , but do not round , , and the number of vocabulary blocks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Scoring, term weighting and\n Up: Index compression\n Previous: Gamma codes\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}