{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/probabilistic-information-retrieval-1.html",
  "title": "Probabilistic information retrieval",
  "body": "\n\n\n\n\nProbabilistic information retrieval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Review of basic probability\n Up: irbook\n Previous: Exercises\n    Contents \n    Index\n\n\n\n\n\nProbabilistic information retrieval\n\n\nDuring the discussion of relevance feedback in Section 9.1.2 , we observed that if we have some known relevant\nand nonrelevant documents, then we can straightforwardly start to\nestimate the probability of a term  appearing in a relevant document\n, and that this could be the basis of a classifier that\ndecides whether documents are relevant or not. In this chapter, we\nmore systematically introduce this probabilistic approach to IR, which provides\na different formal basis for a retrieval model and results in different techniques for setting term\nweights.\n\n\nUsers start with information needs, which they translate into query representations. Similarly, there are documents, which are converted into document representations (the latter differing at least by how text is tokenized, but perhaps containing fundamentally less information, as when a non-positional index is used). Based on these two representations, a system tries to determine how well documents satisfy information needs. In the Boolean or vector space models of IR, matching is done in a formally defined but semantically imprecise calculus of index terms. Given only a query, an IR system has an uncertain understanding of the information need. Given the query and document representations, a system has an uncertain guess of\nwhether a document has content relevant to the information need. Probability theory provides a principled foundation for such reasoning under uncertainty. This chapter provides one answer as to how to exploit this foundation to estimate how likely it is that a document is relevant to an information need.\n\n\nThere is more than one possible retrieval model which has a\nprobabilistic basis. Here, we will introduce probability theory\nand the Probability Ranking Principle\n(Sections 11.1 -11.2 ), and then concentrate on the\n Binary Independence Model  (Section 11.3 ), which is the\noriginal and still most influential\nprobabilistic retrieval model.  Finally, we will introduce\nrelated but extended methods which use term counts, including the\nempirically successful Okapi BM25 weighting scheme, and Bayesian Network\nmodels for IR (Section 11.4 ).  In Chapter 12 ,\nwe then present the alternative probabilistic language modeling\napproach to IR,\nwhich has been developed with considerable success in recent years.\n\n\n\n\nSubsections\n\nReview of basic probability theory\nThe Probability Ranking Principle\n\nThe 1/0 loss case\nThe PRP with retrieval costs\n\n\nThe Binary Independence Model\n\nDeriving a ranking function for query terms\nProbability estimates in theory\nProbability estimates in practice\nProbabilistic approaches to relevance feedback\n\n\nAn appraisal and some extensions\n\nAn appraisal of probabilistic models\nTree-structured dependencies between terms\nOkapi BM25: a non-binary model\nBayesian network approaches to IR\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Review of basic probability\n Up: irbook\n Previous: Exercises\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}