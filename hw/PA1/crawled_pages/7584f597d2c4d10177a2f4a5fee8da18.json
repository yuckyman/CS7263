{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/information-retrieval-system-evaluation-1.html",
  "title": "Information retrieval system evaluation",
  "body": "\n\n\n\n\nInformation retrieval system evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Standard test collections\n Up: Evaluation in information retrieval\n Previous: Evaluation in information retrieval\n    Contents \n    Index\n\n\n\n \n\nInformation retrieval system evaluation\n\n\nTo measure ad hoc information retrieval effectiveness in the standard way, we\nneed a test collection consisting of three things: \n\n\nA document collection\n\nA test suite of information needs, expressible as queries\n\nA set of relevance judgments, standardly a binary assessment of\n  either relevant or nonrelevant for each query-document pair.\n\n\nThe standard approach to information retrieval system evaluation\nrevolves around the notion of  relevant  and\nnonrelevant documents.  With respect to a user information need, a\ndocument in the test collection is given a binary classification as either\nrelevant or nonrelevant. \nThis decision is referred to as the   gold standard  or  ground truth  judgment of relevance.\nThe test document collection and suite of information needs have to be of a\nreasonable size: you need to average performance \nover fairly large test sets, as results are highly\nvariable over different documents and information needs.  As a rule of\nthumb, 50 information needs has usually been found to be a\nsufficient minimum.\n\n\nRelevance is assessed relative to an\n  ,\nnot a query.  For example, an information need \nmight be:\n\nInformation on whether drinking red wine is more\neffective at reducing your risk of heart attacks than white wine.\n\n\nThis might be translated into a query such as:\n\nwine and red and white and\nheart and attack and effective\n\n\nA document is relevant if it addresses the stated information need, not\nbecause it just happens to contain all the words in the query.  This\ndistinction is often misunderstood in practice, because the information\nneed is not overt.  But, nevertheless, an information need is\npresent.  If a user types python into a web search engine, \nthey might be wanting to know where they can purchase a pet python.  Or they\nmight be wanting information on the programming language Python.  From a\none word query, it is very difficult for a system to know what the\ninformation need is.  But, nevertheless, the user has one, and can judge the returned\nresults on the basis of their relevance to it.  \nTo evaluate a system, we require an overt expression of an information\nneed, which can be used for judging returned documents as relevant or\nnonrelevant.\nAt this point, we make a simplification: relevance can reasonably\nbe thought of as a scale, with some documents highly relevant and\nothers marginally so.  But for the moment, we will use just a binary\ndecision of relevance.  We discuss the reasons for using \nbinary relevance judgments and alternatives in Section 8.5.1 .\n\n\nMany systems contain various weights (often known as\nparameters) that can be adjusted to tune system\nperformance. It is wrong to report results on a test\ncollection which were obtained by tuning these parameters to\nmaximize performance on that collection. That is because\nsuch tuning overstates the expected performance of the\nsystem, because the weights will be set to maximize\nperformance on one particular set of queries rather than for\na random sample of queries.  In such cases, the correct\nprocedure is to have one or more  development test\ncollections ,  and to tune the\nparameters on the development test collection.  The tester\nthen runs the system with those weights on the test\ncollection and reports the results on that collection as an\nunbiased estimate of performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Standard test collections\n Up: Evaluation in information retrieval\n Previous: Evaluation in information retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}