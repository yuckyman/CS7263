{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/features-a-crawler-should-provide-1.html",
  "title": "Features a crawler should provide",
  "body": "\n\n\n\n\nFeatures a crawler should provide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Crawling\n Up: Overview\n Previous: Features a crawler must\n    Contents \n    Index\n\n\n\n\nFeatures a crawler should provide\n\n\nDistributed:\nThe crawler should have the ability to execute in a distributed fashion across multiple machines.\n \n\nScalable:\nThe crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth.\n \n\nPerformance and efficiency:\nThe crawl system should make\nefficient use of various system resources including processor,\nstorage and network bandwidth.\n \n\nQuality:\nGiven that a significant fraction of all web pages are of poor utility for serving user query needs, the crawler should be biased towards fetching ``useful'' pages first.\n \n\nFreshness:\nIn many applications, the crawler should\noperate in continuous mode: it should obtain fresh copies of\npreviously fetched pages. A search engine crawler, for instance,\ncan thus ensure that the search engine's index contains a fairly\ncurrent representation of each indexed web page. For such continuous crawling, a crawler should be able to crawl a page with a frequency that approximates the rate of change of that page.\n \n\nExtensible:\nCrawlers should be designed to be extensible in many ways - to cope with new data formats, new fetch protocols,\nand so on. This demands that the crawler architecture be modular.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Crawling\n Up: Overview\n Previous: Features a crawler must\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}