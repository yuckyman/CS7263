{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html",
  "title": "Vector space classification",
  "body": "\n\n\n\n\nVector space classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Document representations and measures\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n \n\n\nVector space classification\n\n\nThe document representation in Naive Bayes is a sequence of\nterms or a binary vector \n. In this chapter we adopt a different\nrepresentation for text classification, the vector space\nmodel, developed in Chapter 6 . It represents each\ndocument as a vector with one real-valued component, usually\na tf-idf weight, for each term.  Thus, the document space\n, the domain of the classification function\n, is \n.  This chapter introduces a\nnumber of classification methods that operate on real-valued\nvectors.\n\n\nThe basic hypothesis in using the vector space model for\nclassification is the \n contiguity hypothesis .\n\nContiguity hypothesis. Documents in the same\nclass form a contiguous region and regions of different\nclasses do not overlap.  \n\n\nThere are many classification\ntasks, in particular the type of text classification that we\nencountered in Chapter 13 , where classes can be\ndistinguished by word patterns.  For example, documents in\nthe class China tend to have high values on\ndimensions like Chinese, Beijing, and\nMao whereas documents in the class UK tend to\nhave high values for London, British and\nQueen. Documents of the two classes therefore form\ndistinct contiguous regions as shown in\nFigure 14.1  and we can draw boundaries that\nseparate them and classify new documents. How exactly\nthis is done is the topic of this chapter.\n\n\n\n\nFigure 14.1:\nVector space classification into three classes.\n\n\n\n\nWhether or not a set of documents is mapped into a\ncontiguous region depends on the particular choices we make\nfor the document representation: type of weighting, stop\nlist etc. To see that the document representation is\ncrucial, consider the two classes written by a group\nvs. written by a single person. Frequent occurrence\nof the first person pronoun I is evidence for the single-person\nclass. But that information is likely deleted from the document\nrepresentation if we use a stop list. If the\ndocument representation chosen is unfavorable, the contiguity hypothesis\nwill not hold and successful vector space classification is\nnot possible.\n\n\nThe same\nconsiderations that led us to prefer weighted\nrepresentations, in particular length-normalized tf-idf\nrepresentations, in Chapters 6 7  also apply\nhere. For example, a term with 5 occurrences in a document\nshould get a higher weight than a term with one occurrence, but a\nweight 5 times larger would give too much emphasis to the\nterm. Unweighted and unnormalized counts should not be used\nin vector space classification.\n\n\nWe introduce two vector space classification\nmethods in this chapter, Rocchio and kNN. Rocchio classification\n(Section 14.2 ) divides the vector space into\nregions centered on centroids or  prototypes , one for each class, computed as\nthe center of mass of all documents in the class. Rocchio\nclassification is simple and efficient, but inaccurate if\nclasses are not approximately spheres with similar radii.\n\n\nkNN or   nearest neighbor\nclassification (Section 14.3 ) assigns the majority class of\nthe  nearest neighbors to a test document. kNN requires\nno explicit training and can use the unprocessed training set directly\nin classification.\nIt is less efficient than other classification\nmethods in classifying documents.  If the training set is large,\nthen kNN can handle non-spherical and other complex classes\nbetter than Rocchio.\n\n\nA large number of text classifiers can be viewed as linear\nclassifiers - classifiers that classify based on a simple\nlinear combination of the features (Section 14.4 ).  Such classifiers\npartition the space of features into regions separated by\nlinear  decision\nhyperplanes , in a manner to be detailed below.  Because of\nthe bias-variance tradeoff (Section 14.6 ) more\ncomplex nonlinear models are not systematically better than\nlinear models.  Nonlinear models have more parameters to fit\non a limited amount of training data and are more\nlikely to make mistakes for small and noisy data sets.\n\n\nWhen applying two-class classifiers to problems with more than\ntwo classes, there are one-of tasks - a\ndocument must be assigned to exactly one of several mutually\nexclusive classes - and any-of tasks - a document\ncan be assigned to any number of classes\nas we will explain in Section 14.5 . Two-class classifiers solve any-of\nproblems and can be combined to solve one-of problems.\n\n\n\n\nSubsections\n\nDocument representations and measures of\n  relatedness in vector spaces\nRocchio classification\nk nearest neighbor\n\nTime complexity and optimality of kNN\n\n\nLinear versus nonlinear classifiers\nClassification with more than two classes\nThe bias-variance tradeoff\nReferences and further reading\nExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Document representations and measures\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}