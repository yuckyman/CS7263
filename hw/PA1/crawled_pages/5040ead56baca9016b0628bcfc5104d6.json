{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/list-of-tables-1.html",
  "title": "List of Tables",
  "body": "\n\n\n\n\nList of Tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: List of Figures\n Up: irbook\n Previous: Contents\n    Contents \n    Index\n\n\n\n\n\nList of Tables\n\nTypical system parameters\nin 2007.\nThe seek time is the time needed to position the disk head in\na new position. The transfer time per byte is the rate\nof transfer from disk to memory when the head is in the right position. \n\nCollection statistics for Reuters-RCV1. Values are\nrounded for the computations in this book.\nThe unrounded\nvalues are:\n806,791 documents, 222\ntokens per document, 391,523 (distinct) terms,\n6.04\nbytes per token with spaces and punctuation, \n4.5 bytes per token without spaces and punctuation,\n7.5 bytes per term,\nand 96,969,056 tokens.\nThe\nnumbers in this table correspond to the third line (``case\nfolding'') in icompresstb5. \nThe five steps in constructing an\nindex for Reuters-RCV1 in blocked sort-based indexing. Line numbers refer to Figure 4.2 .\nCollection statistics for a large collection.\nThe effect of preprocessing on \nthe number of terms,\nnonpositional postings, and tokens for Reuters-RCV1.\n``'' indicates the reduction in size from the\nprevious line, except that ``30 stop words'' and ``150 stop\nwords'' both use ``case folding'' as their reference\nline. ``T%'' is the cumulative (``total'') reduction from unfiltered.\nWe performed stemming with the Porter stemmer\n(Chapter 2 , page 2.2.4 ).\nDictionary compression for Reuters-RCV1.\nEncoding gaps instead of document IDs. For example,\nwe store \ngaps 107, 5, 43, ..., instead of\ndocIDs 283154, 283159, 283202, ... for\ncomputer.\nThe first docID is left unchanged (only\nshown for arachnocentric).\n\nSome examples of unary and  codes.\nUnary codes are only shown for the smaller numbers.\nCommas in  codes are for readability only and are not part of the actual codes.\nIndex and dictionary compression for Reuters-RCV1.\nThe compression ratio depends on the proportion of actual text\nin the collection. Reuters-RCV1 contains a\nlarge amount of XML\nmarkup. Using the two best\ncompression schemes,  encoding and blocking with\nfront coding, the \nratio compressed index to collection size is therefore\nespecially small for Reuters-RCV1:\n\n.\n.\n\nTwo gap sequences to be merged in blocked sort-based indexing\n\nCosine computation for Exercise 6.4.4 .\nCalculating the kappa statistic.\nINEX 2002 collection statistics.\nINEX 2002 results of the vector space model in Section 10.3  for\ncontent-and-structure (CAS) queries and the quantization function Q.\nA comparison of content-only and full-structure\nsearch in INEX 2003/2004.\nData for parameter\nestimation examples.  \nTraining and test times for\nNB.  \n\nMultinomial versus Bernoulli model.  \n\nCorrect estimation implies accurate prediction, but accurate\nprediction does not imply correct estimation.\nA set of documents for which\nthe NB independence assumptions are problematic.\nCritical values of the \ndistribution with one degree of freedom. For example, if\n  the two events are\nindependent, then \n. So for \n\nthe assumption of independence can be rejected with 99% confidence.  \n\nThe ten largest classes in the\nReuters-21578\ncollection with number of documents in training and test sets.  \nMacro- and microaveraging.\n``Truth'' is the true class and\n``call'' the\ndecision of the classifier. In this example, macroaveraged precision is\n\n. Microaveraged precision is\n\n.  \n\nText  classification effectiveness numbers on Reuters-21578\n  for F (in percent). Results from \nLi and Yang (2003) (a), Joachims (1998) (b: kNN)\nand Dumais et al. (1998) (b: NB, Rocchio, trees, SVM).  \n\nData for parameter estimation exercise.  \nVectors and class centroids for the data in\n  Table 13.1 .\nTraining examples for machine-learned scoring.\nSome applications of clustering in information\nretrieval.\nThe four external evaluation measures applied to\nthe clustering in Figure 16.4 .\nComparison of HAC algorithms.\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}