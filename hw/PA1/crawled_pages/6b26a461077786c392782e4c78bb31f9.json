{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/heaps-law-estimating-the-number-of-terms-1.html",
  "title": "Heaps' law: Estimating the number of terms",
  "body": "\n\n\n\n\nHeaps' law: Estimating the number of terms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Zipf's law: Modeling the\n Up: Statistical properties of terms\n Previous: Statistical properties of terms\n    Contents \n    Index\n\n\n\n\nHeaps' law: Estimating the number of terms\n\n\n\n\nHeaps' law.Vocabulary size  as a function of\ncollection size   (number of tokens) for Reuters-RCV1.\nFor these data, the dashed line \n\nis the best least-squares fit. Thus, \n\nand .\n\n\n\nA better way of getting a handle on  is  Heaps'\nlaw , which\nestimates vocabulary size as a function of collection size:\n\n\n\n\n\n\n(1)\n\n\nwhere  is the number of tokens in the collection. Typical\nvalues for the parameters  and  are: \n and . \nThe motivation for Heaps' law is that the simplest possible\nrelationship between collection size and vocabulary size is linear in log-log space\nand the assumption of linearity is usually born out in\npractice as shown in Figure 5.1  for Reuters-RCV1.\nIn this case, the fit is excellent for\n\n, for the parameter values  and\n. For example, for the first 1,000,020 tokens Heaps'\nlaw predicts 38,323 terms:\n\n\n\n\n\n\n(2)\n\n\nThe actual number is\n38,365 terms, very close to the prediction.\n\n\nThe parameter  is quite variable because vocabulary\ngrowth depends a lot on the nature of the collection and how\nit is processed. Case-folding and stemming reduce the growth\nrate of the vocabulary, whereas including numbers and\nspelling errors increase it. Regardless\nof the values of the parameters for a particular collection,\nHeaps' law suggests that (i) the dictionary size \ncontinues to increase with more documents in the collection,\nrather than a maximum vocabulary size being reached, and\n(ii) the size of the dictionary is quite large for\nlarge collections.  These two hypotheses have been\nempirically shown to be true of large text collections\n(Section 5.4 ).  So dictionary compression\nis important for an effective information retrieval system.\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n Next: Zipf's law: Modeling the\n Up: Statistical properties of terms\n Previous: Statistical properties of terms\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}