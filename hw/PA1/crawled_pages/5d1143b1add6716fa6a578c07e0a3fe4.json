{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html",
  "title": "The Binary Independence Model",
  "body": "\n\n\n\n\nThe Binary Independence Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Deriving a ranking function\n Up: Probabilistic information retrieval\n Previous: The PRP with retrieval\n    Contents \n    Index\n\n\n\n\n \n\nThe Binary Independence Model\n\n\nThe  Binary Independence Model  (BIM)\nwe present in this section is the model that has\ntraditionally been used with the PRP.\nIt introduces some simple assumptions, which make estimating the probability function  practical.\nHere, ``binary''\nis equivalent to Boolean: documents and queries are both represented as binary term incidence vectors.\nThat is, a document  is represented by the vector\n \n where\n if term  is present in document \nand  if  is not present in .\nWith this representation, many possible documents have\nthe same vector representation.\nSimilarly, we represent  by the incidence vector  (the distinction between  and  is less central since commonly  is in the form of a set of words).  ``Independence'' means that\nterms are modeled as occurring in documents\nindependently. The model recognizes no association between\nterms. This assumption is far from correct, but it nevertheless often gives satisfactory results in practice; it is the ``naive'' assumption of Naive Bayes models, discussed further in Section 13.4 (page ). Indeed, the Binary Independence Model is exactly the same as the multivariate Bernoulli Naive Bayes model presented in Section 13.3 (page ). In a sense this assumption is equivalent to an assumption of the vector space model, where each term is a dimension that is orthogonal to all other terms.\n\n\nWe will first present a model which assumes that the user has a single step information need. As discussed in Chapter 9 , seeing a range of results might let the user refine their information need. Fortunately, as mentioned there, it is straightforward to extend the Binary Independence Model so as to provide a framework for relevance feedback, and we present this model in Section 11.3.4 .\n\n\nTo make a probabilistic retrieval strategy precise, we need to estimate how terms in documents contribute to relevance, specifically, we wish to know how term frequency, document frequency, document length, and other statistics that we can compute influence judgments about document relevance, and how they can be reasonably combined to estimate the probability of document relevance. We then order documents by decreasing estimated probability of relevance.\n\n\nWe assume here that the relevance of each document is independent of the relevance of other documents. As we noted in Section 8.5.1 (page ), this is incorrect: the assumption is especially harmful in practice if it allows a system to return duplicate or near duplicate documents. Under the BIM, we model the probability  that a document is relevant via the probability in terms of term incidence vectors \n.\nThen, using Bayes rule, we have:\n\n\n\n\n\n\n\n\n(63)\n\n\n\n\n(64)\n\n\nHere, \n and \n are the probability that if a relevant or nonrelevant, respectively, document is retrieved, then that document's representation is . You should think of this quantity as defined with respect to a space of possible documents in a domain.  How do we compute all these probabilities? We never know the exact probabilities, and so we have to use estimates: Statistics about the actual document collection are used to estimate these probabilities.  \n and \n indicate the prior probability of retrieving a relevant or nonrelevant document respectively for a query . Again, if we knew the percentage of relevant documents in the collection, then we could use this number to estimate \n and \n. Since a document is either relevant or nonrelevant to a query, we must have that:\n\n\n\n\n\n\n(65)\n\n\n\n\n\nSubsections\n\nDeriving a ranking function for query terms\nProbability estimates in theory\nProbability estimates in practice\nProbabilistic approaches to relevance feedback\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Deriving a ranking function\n Up: Probabilistic information retrieval\n Previous: The PRP with retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}