{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/model-based-clustering-1.html",
  "title": "Model-based clustering",
  "body": "\n\n\n\n\nModel-based clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Flat clustering\n Previous: Cluster cardinality in K-means\n    Contents \n    Index\n\n\n\n\n \n\nModel-based clustering\n\n\nIn this section, we describe a generalization of\n -means, the EM algorithm. It can be applied to a\nlarger variety of document representations and distributions\nthan  -means.\n\n\nIn  -means, we attempt to find centroids that are good\nrepresentatives. We can view the set of  centroids as a model that\ngenerates the data. Generating a document in this model consists of\nfirst picking a centroid at random and then adding some noise. If the\nnoise is normally distributed, this procedure will result in\nclusters of spherical shape. \n Model-based clustering \nassumes\nthat the data were generated by a model and tries to\nrecover the original model from the data. The model that we\nrecover from the data then\ndefines clusters and an assignment of documents to clusters.\n\n\nA commonly used criterion for estimating the model parameters\nis maximum\nlikelihood. In  -means, \nthe quantity \n is proportional to the\nlikelihood that a particular model (i.e., a set of centroids) generated\nthe data. For  -means, \nmaximum likelihood \nand minimal RSS are equivalent criteria.\nWe denote the model parameters by .\nIn  -means, \n\n. \n\n\nMore generally, the\nmaximum likelihood criterion is\nto select the parameters  that maximize the log-likelihood\nof generating the data :\n\n\n\n\n\n\n(198)\n\n\n is the objective function that measures\nthe goodness of the clustering. Given two\nclusterings with the same number of clusters, we prefer the\none with higher .\n\n\nThis is the same approach we took in Chapter 12 \n(page 12.1.1 ) for language modeling and in\nSection 13.1  (page 13.4 ) for text\nclassification. In text classification, we chose the class\nthat maximizes the likelihood of generating a particular\ndocument. Here, we choose the clustering  that\nmaximizes the likelihood of generating a given set of\ndocuments.  Once we have , we\ncan compute an assignment probability\n\n for each document-cluster pair. This\nset of assignment probabilities defines a soft clustering.\n\n\nAn example of a soft assignment is\nthat\na document about\nChinese cars may have a fractional membership of 0.5 in each of the\ntwo clusters China and automobiles, reflecting the fact\nthat both topics are pertinent. A hard clustering like\n -means cannot\nmodel this simultaneous relevance to two topics.\n\n\nModel-based clustering provides a\nframework for incorporating our knowledge about a domain.\n -means and the\nhierarchical algorithms in Chapter 17 \nmake fairly rigid assumptions about the data. For example,\nclusters in  -means are assumed to be spheres.\nModel-based clustering offers more flexibility. The\nclustering model can be adapted to what we know about the\nunderlying distribution of the data, be it\nBernoulli (as in\nthe example in Table 16.3 ), Gaussian with non-spherical variance\n(another model that is important\nin document clustering) or a member of a different family.\n\n\nA commonly used algorithm for model-based clustering \nis the\n Expectation-Maximization algorithm  or \n EM algorithm .\nEM clustering is an iterative algorithm that maximizes\n.\nEM can be applied to many different types of probabilistic\nmodeling.  \nWe will work with a mixture\nof multivariate Bernoulli distributions here, the distribution we know from\nSection 11.3  \n(page 11.3 )\nand Section 13.3  (page 13.3 ):\n\n\n\n\n\n \n \n\n(199)\n\n\nwhere \n,\n\n,\nand  \n\nare the parameters of the\nmodel.\n\nis the probability that\na document from cluster  contains term .\nThe probability  is the\nprior  of cluster : the probability that a\ndocument  is in \nif we\nhave no information about . \n\n\nThe mixture model then is:\n\n\n\n\n\n \n \n\n(200)\n\n\nIn this model, we generate a document by\nfirst picking a cluster  with probability  and\nthen generating the terms of the document according to the\nparameters .\nRecall that the document\nrepresentation of the multivariate Bernoulli is a vector of\n Boolean values (and not a real-valued vector).\n\n\nHow do we use EM to infer the parameters\nof the clustering\nfrom the data? That is, how do we choose parameters  that\nmaximize \n?\nEM is similar to  -means\nin that it alternates between an  expectation step ,\ncorresponding to reassignment, and a  maximization step ,\ncorresponding to recomputation of the parameters of the model. The\nparameters of  -means are the centroids, the parameters of the\ninstance of EM in this section are the  and \n.\n\n\nThe maximization step\nrecomputes the conditional parameters \n\nand the priors \nas follows:\n\n\n\n\n\n\n\n \n \n\n(201)\n\n\nwhere \n if \n and\n0 otherwise and\n is the soft\nassignment of document  to cluster  as computed in \nthe preceding iteration. (We'll address the issue of\ninitialization in a moment.)\nThese are the maximum likelihood estimates for the\nparameters of the multivariate Bernoulli  from\nTable 13.3  (page 13.3 )\nexcept that  documents are assigned fractionally\nto clusters here. \nThese maximum likelihood estimates\nmaximize the likelihood of the data given the model.\n\n\nThe expectation step computes the soft assignment of\ndocuments to clusters given the current parameters \nand :\n\n\n\n\n\n\n\n \n \n\n(202)\n\n\nThis expectation step applies \n and 200  to computing the likelihood that  generated document\n. It is the classification procedure for\nthe multivariate Bernoulli in Table 13.3 . Thus, the\nexpectation step is nothing else but Bernoulli Naive Bayes\nclassification (including normalization, i.e. dividing by\nthe denominator, to get a\nprobability distribution over clusters).\n\n\n\n\n\n\n(a)\ndocID\ndocument text\ndocID\ndocument text\n\n \n1\nhot chocolate cocoa beans\n7\nsweet sugar\n\n \n2\ncocoa ghana africa\n8\nsugar cane brazil\n\n \n3\nbeans harvest ghana\n9\nsweet sugar beet\n\n \n4\ncocoa butter\n10\nsweet cake icing\n\n \n5\nbutter truffles\n11\ncake black forest\n\n \n6\nsweet chocolate\n \n \n\n\n\n\n\n\n\n(b)\nParameter\nIteration of clustering\n\n \n \n0\n1\n2\n3\n4\n5\n15\n25\n\n \n\n \n0.50\n0.45\n0.53\n0.57\n0.58\n0.54\n0.45\n\n \n\n \n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n\n \n\n \n0.50\n0.79\n0.99\n1.00\n1.00\n1.00\n1.00\n\n \n\n \n0.50\n0.84\n1.00\n1.00\n1.00\n1.00\n1.00\n\n \n\n \n0.50\n0.75\n0.94\n1.00\n1.00\n1.00\n1.00\n\n \n\n \n0.50\n0.52\n0.66\n0.91\n1.00\n1.00\n1.00\n\n \n\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.83\n0.00\n\n \n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n \n\n \n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n \n\n \n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n \n\n \n0.50\n0.40\n0.14\n0.01\n0.00\n0.00\n0.00\n\n \n\n \n0.50\n0.57\n0.58\n0.41\n0.07\n0.00\n0.00\n\n \n\n \n0.000\n0.100\n0.134\n0.158\n0.158\n0.169\n0.200\n\n \n\n \n0.000\n0.083\n0.042\n0.001\n0.000\n0.000\n0.000\n\n \n\n \n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n \n\n \n0.000\n0.167\n0.195\n0.213\n0.214\n0.196\n0.167\n\n \n\n \n0.000\n0.400\n0.432\n0.465\n0.474\n0.508\n0.600\n\n \n\n \n0.000\n0.167\n0.090\n0.014\n0.001\n0.000\n0.000\n\n \n\n \n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n\n \n\n \n1.000\n0.500\n0.585\n0.640\n0.642\n0.589\n0.500\n\n \n\n \n1.000\n0.300\n0.238\n0.180\n0.159\n0.153\n0.000\n\n \n\n \n1.000\n0.417\n0.507\n0.610\n0.640\n0.608\n0.667\n\n\n\nThe EM clustering algorithm.The table shows a set of documents (a)\nand parameter values for selected iterations during EM\nclustering (b).\nParameters shown are prior\n, soft assignment scores\n (both omitted for cluster 2), and lexical\nparameters  for a few terms.\nThe authors initially assigned document 6 to cluster 1\nand document 7 to cluster 2  (iteration 0). EM converges after 25 iterations.\nFor smoothing, the  in\nEquation 201 were replaced with \n where\n\n.\n\n \n\n\n\nWe clustered a set of 11 documents into two clusters using\nEM in Table 16.3 .\nAfter convergence in iteration 25, the first 5 documents are\nassigned to cluster 1 (\n) and the last 6 to\ncluster 2 (). Somewhat atypically, the final assignment is a hard\nassignment here.\nEM usually converges to a soft assignment.\nIn iteration 25, the prior  for cluster 1 is\n\n because 5 of the 11 documents\nare in cluster 1. Some terms are quickly associated with one\ncluster because the initial assignment can ``spread'' to them\nunambiguously. For example, \nmembership in cluster 2 spreads \nfrom document 7 to document 8 in the first iteration because they share\nsugar ( in iteration 1).\n\n\nFor parameters of terms occurring in\nambiguous contexts, convergence takes longer. Seed\ndocuments 6 and 7 both contain sweet. As a result, it\ntakes 25 iterations for the term to be unambiguously\nassociated with cluster 2. ( in iteration 25.)\n\n\nFinding good seeds is even more critical for EM than for\n -means. EM is prone to get stuck in  local optima if the\nseeds are not chosen well. This is a general problem\nthat also occurs in other applications of EM.Therefore, as with  -means, the initial assignment of\ndocuments to clusters is often computed by a different\nalgorithm. For example, a hard  -means clustering may\nprovide the initial assignment, which EM can then ``soften up.''\n\n\nExercises.\n\nWe saw above that the time complexity of  -means is\n. What is the time complexity of EM?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Flat clustering\n Previous: Cluster cardinality in K-means\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}