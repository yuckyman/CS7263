{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/classification-with-more-than-two-classes-1.html",
  "title": "Classification with more than two classes",
  "body": "\n\n\n\n\nClassification with more than two classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The bias-variance tradeoff\n Up: Vector space classification\n Previous: Linear versus nonlinear classifiers\n    Contents \n    Index\n\n\n\n \n \n\nClassification with more than two classes\n \nWe can extend two-class linear classifiers to\n classes. The method to use\ndepends on whether the classes are mutually exclusive\nor not.\n\n\nClassification for classes that are not mutually exclusive\nis called  any-of ,  multilabel , or  multivalue classification .  In this\ncase, a document can belong to several classes\nsimultaneously, or to a single class, or to none of the\nclasses.  A decision on one class leaves all options open\nfor the others.  It is sometimes said that the classes are\nindependent of each other, but this is misleading\nsince the classes are rarely statistically independent in\nthe sense defined on page 13.5.2 .  In terms\nof the formal definition of the classification problem in\nEquation 112 (page 112 ), we learn \ndifferent classifiers  in any-of classification,\neach returning either  or \n:\n\n.\n\n\n \nSolving an any-of classification task with linear\nclassifiers is straightforward:\n\n\nBuild a classifier for each class,\nwhere the training set consists of the set of documents in\nthe class (positive labels) and its complement (negative\nlabels). \n\nGiven the test document, apply each\nclassifier separately. The decision of one classifier has no\ninfluence on the decisions of the other classifiers.\n\n\n\nThe second type of classification with more than two classes\nis \n one-of classification . Here, the classes are\nmutually exclusive.\nEach document must belong to exactly one of\nthe classes. \nOne-of classification is also called\n multinomial ,\n polytomous ,\n multiclass ,\nor\n single-label classification .\nFormally, there is a single classification function  in\none-of classification whose range is , i.e.,\n\n.\nkNN is a (nonlinear) one-of classifier.\n\n\nTrue one-of problems are less common in text classification\nthan any-of problems.  With classes like UK,\nChina, poultry, or coffee, a\ndocument can be relevant to many topics simultaneously - as\nwhen the prime minister of the UK visits China to talk about\nthe coffee and poultry trade.\n\n\nNevertheless, we will often make a one-of assumption, as we\ndid in Figure 14.1 , even if classes are not\nreally mutually exclusive. For the classification problem of\nidentifying the language of a document, the one-of\nassumption is a good approximation as most text is written\nin only one language. In such cases, imposing a one-of constraint\ncan increase the classifier's effectiveness because errors\nthat are due to the fact that the any-of classifiers\nassigned a document to either no class or more than one class are eliminated.\n\n\n\n\nFigure 14.12:\n hyperplanes\ndo not divide space into  disjoint regions.\n\n\n\n hyperplanes do not divide \n \ninto \ndistinct regions as illustrated in Figure 14.12 . Thus, we must use\na combination method when using two-class linear classifiers for one-of classification.\nThe simplest method is to rank classes\nand then select the top-ranked\nclass. Geometrically, the ranking can be with\nrespect to the distances from the  linear separators.\nDocuments close to\na class's separator are more likely to be misclassified, so the\ngreater the distance from the separator, the more plausible\nit is that a positive classification decision is correct.\nAlternatively, we can use a direct measure of confidence to\nrank classes, e.g., probability of class membership.\nWe can state this algorithm\nfor one-of classification with linear classifiers\nas follows:\n\n\n\nBuild a classifier for each class,\nwhere the training\nset consists of the set of documents in the class (positive labels) and its\ncomplement (negative labels).\n\nGiven the test document, \napply each classifier separately.\n\nAssign the document to the class with\n\n\nthe maximum score,\n\nthe maximum confidence value,\n\nor the maximum probability.\n\n\n\n\n\n\n\n\n \nassigned class\nmoney-fx\ntrade\ninterest\nwheat\ncorn\ngrain\n\ntrue class\n \n \n \n \n \n \n \n\nmoney-fx\n \n95\n0\n10\n0\n0\n0\n\ntrade\n \n1\n1\n90\n0\n1\n0\n\ninterest\n \n13\n0\n0\n0\n0\n0\n\nwheat\n \n0\n0\n1\n34\n3\n7\n\ncorn\n \n1\n0\n2\n13\n26\n5\n\ngrain\n \n0\n0\n2\n14\n5\n10\n\n\nA confusion matrix for Reuters-21578.For example, 14 documents from\ngrain were incorrectly assigned to wheat.\nAdapted from Picca et al. (2006).\n\n \n\n\n\nAn important tool for analyzing the performance of a \nclassifier for  classes is the  confusion matrix . The\nconfusion matrix shows for each pair of classes \n, how many\ndocuments from  were incorrectly assigned to . In\nTable 14.5 , \nthe classifier manages to distinguish the three\nfinancial classes money-fx,\ntrade, and\ninterest from the three agricultural classes\nwheat,\ncorn, and\ngrain, but makes many errors within these two\ngroups. \nThe confusion matrix can help pinpoint opportunities\nfor improving \nthe accuracy of the system. For example, to address the\nsecond largest error in Table 14.5  (14 in the row grain), one could attempt to\nintroduce features that distinguish wheat documents\nfrom grain documents.\n\n\nExercises.\n\nCreate a training set of 300 documents, 100 each\nfrom three different languages (e.g., English, French,\nSpanish). Create a test set by the same procedure, but also add\n100 documents from a fourth language. \nTrain (i) a one-of\nclassifier (ii) an any-of\nclassifier on this training set and\nevaluate it on the test set.\n(iii) Are there any interesting\ndifferences in how the two classifiers behave on this task?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The bias-variance tradeoff\n Up: Vector space classification\n Previous: Linear versus nonlinear classifiers\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}