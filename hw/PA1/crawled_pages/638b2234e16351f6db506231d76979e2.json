{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/optimality-of-hac-1.html",
  "title": "Optimality of HAC",
  "body": "\n\n\n\n\nOptimality of HAC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Divisive clustering\n Up: Hierarchical clustering\n Previous: Centroid clustering\n    Contents \n    Index\n\n\n\n\n \n\nOptimality of HAC\n\n\nTo state the optimality conditions of hierarchical\nclustering precisely, we first define the  combination\nsimilarity COMB-SIM of a clustering \n as the \nsmallest combination\nsimilarity of any of its  clusters:\n\n\n\n\n\n\n(210)\n\n\nRecall that  the combination similarity of a cluster\n that was created as the merge of  and\n is the\nsimilarity of  and\n \n(page 17.1 ). \n\n\nWe then define\n\n to be\n  optimal \nif all clusterings  with\n clusters, , have lower combination\nsimilarities:\n\n\n\n\n\n\n(211)\n\n\n\nFigure 17.12  shows that\ncentroid clustering is not optimal.  The clustering\n\n (for ) has combination\nsimilarity  and\n\n (for ) has combination similarity -3.46.\nSo the clustering \n produced in the first merge\nis not optimal since there is a clustering with\nfewer clusters (\n) that has higher combination\nsimilarity. Centroid clustering is not optimal because\ninversions can occur.\n\n\nThe above definition of optimality would be of limited use if it\nwas only applicable to a clustering together with its\nmerge history. However, we can show\n(Exercise 17.5 ) that \n  \nfor the three non-inversion algorithms can be\nread off from the cluster without knowing its history. \n These direct definitions of\ncombination similarity are as follows.\n\nsingle-link\nThe combination similarity of a cluster\n is the smallest similarity of any bipartition of\n  the cluster, where the similarity of a bipartition is the\n  largest similarity between any two documents from the two parts:\n\n\n\n\n\n\n(212)\n\n\nwhere each \n is a\nbipartition of .\n\ncomplete-link\nThe combination similarity of a cluster\n is the smallest similarity\nof any two points in :\n\n.\n\nGAAC\nThe combination similarity of a cluster\n is the\naverage of all pairwise similarities in  \n(where\nself-similarities are not included in the average):\nEquation 205.\n\n\nIf we use these definitions of combination similarity, then\noptimality is a property of a set of clusters and not of a\nprocess that produces a set of clusters.\n\n\nWe can now prove the optimality of single-link\nclustering by induction over the number of clusters .  We\nwill give a proof for the case where no two pairs of\ndocuments have the same similarity,\nbut it can easily be extended to the case with ties.\n\n\nThe inductive basis of the proof is that a clustering with\n clusters has combination similarity 1.0, which\nis the largest value possible. The\ninduction hypothesis is that a single-link clustering  with  clusters\nis optimal:\n\n\n\n for all .\nAssume for contradiction that the clustering  we obtain by merging\nthe two most similar clusters in  is not optimal\nand that instead a different sequence of merges\n\n leads to the optimal clustering\nwith  clusters.\nWe can write the assumption that  is optimal\nand that  is not\nas\n\n. \n\n\nCase 1: The two documents linked by \n are in the same cluster in  . They can only be in the same cluster if a\nmerge with similarity smaller than \nhas occurred in the merge sequence producing\n. \nThis implies \n. Thus,\n\n. Contradiction.\n\n\nCase 2: The two documents linked by \n are not in the same cluster in  . But\n\n, so\nthe single-link merging rule should have merged these two\nclusters when processing\n. Contradiction.\n\n\nThus,  is optimal.\n\n\nIn contrast to single-link clustering, complete-link\nclustering and GAAC are not optimal as this example shows:\n\n\n\n\n\n\nBoth algorithms merge the two points with distance 1 (\nand ) first and thus cannot find the two-cluster\nclustering \n. \nBut \n is optimal on the\noptimality criteria of complete-link clustering and GAAC.\n\n\nHowever, the merge criteria of complete-link clustering and\nGAAC approximate the desideratum of approximate sphericity\nbetter than the merge criterion of single-link clustering.\nIn many applications, we want spherical clusters.  Thus,\neven though single-link clustering may seem preferable at\nfirst because of its optimality, it is optimal with respect\nto the wrong criterion in many document clustering\napplications.\n\n\n\n\n\nTable 17.1:\nComparison of HAC algorithms.\n\nmethod\ncombination similarity\ntime compl.\noptimal?\ncomment\n\nsingle-link\nmax inter-similarity of any 2\ndocs\n\nyes\nchaining effect\n\ncomplete-link\nmin inter-similarity of any 2 docs\n\n\nno\nsensitive to outliers\n\ngroup-average\naverage of all sims\n\n\nno\nbest choice for \nmost applications\n\ncentroid\naverage inter-similarity\n\n\nno\ninversions can occur\n\n\n \n\n\n\n\n\nTable 17.1  summarizes the properties of the four HAC\nalgorithms introduced in this chapter.  We recommend GAAC\nfor document clustering because it is generally the method\nthat produces the clustering with the best properties for\napplications.  It does not suffer from chaining, from sensitivity\nto outliers and from inversions.  \n\n\nThere are two exceptions to this\nrecommendation. First, for non-vector representations,\nGAAC is not applicable and clustering should typically\nbe performed with the complete-link method. \n\n\nSecond, in\nsome applications\nthe purpose of clustering is not to create a complete\nhierarchy or exhaustive partition of the entire document set.\nFor instance,  first story detection  or \n novelty detection  \nis the task of\ndetecting the first occurrence of an event in a stream of news stories. One\napproach to this task is to find a tight cluster\nwithin the documents that were sent across the wire in a\nshort period of time and are dissimilar from all previous\ndocuments. For example, the documents sent over the wire in\nthe minutes after the World Trade Center attack on September\n11, 2001 form\nsuch a cluster. Variations of single-link clustering can do\nwell on this task since it is the structure of small parts\nof the vector space - and not global structure - that is important in this case.\n\n\nSimilarly, we will describe an approach to duplicate\ndetection on the web in Section 19.6 \n(page 19.6 ) where single-link clustering is used\nin the guise of the  union-find\n  algorithm . Again, the decision whether a group of\ndocuments are duplicates of each other is not influenced by\ndocuments that are located far away and single-link\nclustering is a good choice for duplicate detection.\n\n\nExercises.\n\n   Show the equivalence\nof the two definitions of combination similarity: the\nprocess definition\non page 17.1 \nand the static definition\non page 17.5 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Divisive clustering\n Up: Hierarchical clustering\n Previous: Centroid clustering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}