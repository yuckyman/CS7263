{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html",
  "title": "The Bernoulli model",
  "body": "\n\n\n\n\nThe Bernoulli model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Properties of Naive Bayes\n Up: Text classification and Naive\n Previous: Relation to multinomial unigram\n    Contents \n    Index\n\n\n\n\n \n\nThe Bernoulli model\n\n\nThere are two different ways we can set up an NB\nclassifier. The model we introduced in the previous section\nis the\n multinomial model . It generates one term from the\nvocabulary in each position of the document, where we assume\na generative model that will be discussed in more detail in\nSection 13.4 \n(see also\npage 12.1.1 ).\n\n\nAn alternative to the multinomial model\nis the \n multivariate Bernoulli model  \nor\n Bernoulli model . It is equivalent to the\nbinary independence model\nof Section 11.3 (page ), which generates an\nindicator for each term of the vocabulary, either \n indicating presence of the term in\nthe document\nor \nindicating absence.  Figure 13.3  presents training and\ntesting algorithms for the Bernoulli model. The Bernoulli model\nhas the same time complexity as the multinomial model.\n\n\n\n\n\n\nThe different generation models imply different estimation\nstrategies and different classification rules. The Bernoulli model estimates\n\n as the fraction of documents of\nclass  that contain term  (Figure 13.3 ,\nTRAINBERNOULLINB, line 8).  In contrast, the\nmultinomial model estimates \n as the\nfraction of tokens or fraction of positions in\ndocuments of class  that contain term \n(Equation 119).  \nWhen classifying a test document, the\nBernoulli model uses binary occurrence information, ignoring\nthe number of occurrences, whereas the multinomial model\nkeeps track of multiple occurrences. As a result, the\nBernoulli model typically makes many mistakes when\nclassifying long documents. For example, it may assign an\nentire book to the class China because of a single\noccurrence of the term China.\n\n\nThe models also differ in how nonoccurring terms are used\nin classification. They do not affect the classification\ndecision in the multinomial model; but in the Bernoulli model\nthe probability of nonoccurrence is factored in when\ncomputing  (Figure 13.3 , APPLYBERNOULLINB, Line 7).  This is because only the\nBernoulli NB model models absence of terms explicitly.\n\n\nWorked example. Applying the Bernoulli model to\nthe example in Table 13.1 , we have the same estimates\nfor the priors as before:\n\n,\n\n. The conditional probabilities are:\n\n\n\n\n\n\n\n\nThe denominators are  and  because \nthere are three documents in  and one document in \nand because \nthe constant  in\nEquation 119 is 2 - there are two cases to consider for\neach term, occurrence and nonoccurrence.\n\n\nThe scores of the\ntest document for the two classes are \n\n\n\n\n\nand, analogously,\n\n\n\n\n\nThus,\nthe classifier assigns the test document to \n\nnot-China.\nWhen looking only at binary occurrence and not at term\nfrequency,\nJapan and Tokyo are indicators for  ()\nand the conditional probabilities of\nChinese for  and  are not different enough\n(4/5 vs. 2/3) to affect the classification decision. End worked example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Properties of Naive Bayes\n Up: Text classification and Naive\n Previous: Relation to multinomial unigram\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}