{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/when-does-relevance-feedback-work-1.html",
  "title": "When does relevance feedback work?",
  "body": "\n\n\n\n\nWhen does relevance feedback work?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Relevance feedback on the\n Up: Relevance feedback and pseudo\n Previous: Probabilistic relevance feedback\n    Contents \n    Index\n\n\n\n\nWhen does relevance feedback work?\n\n\nThe success of relevance feedback depends on certain assumptions.\nFirstly, the user has to have sufficient knowledge to be able to make an\ninitial\nquery which is at least somewhere close to the documents they desire.\nThis is needed anyhow for successful information retrieval in the basic\ncase, but it is important to see the kinds of problems that relevance\nfeedback cannot solve alone. Cases where relevance feedback alone is not\nsufficient include:\n\n\nMisspellings. If the user spells a term in a different way to the\n  way it is spelled in any document in the collection, then relevance\n  feedback is unlikely to be effective. This can be addressed by the spelling\n  correction techniques of Chapter 3 .\n\nCross-language information retrieval. Documents in another\n  language are not nearby in a vector space based on term distribution.\n  Rather, documents in the same language cluster more closely together.\n\nMismatch of searcher's vocabulary versus collection vocabulary. If\n  the user searches for laptop but all the documents use the\n  term notebook computer, then the query will fail, and relevance\n  feedback is again most likely ineffective.\n\n\n\n\n\nSecondly, the relevance feedback approach requires\nrelevant documents to be similar to each other. That is, they should cluster.\nIdeally, the term distribution in all relevant documents\nwill be similar to that in the documents marked by the users, while the\nterm distribution in all nonrelevant documents will be different from those\nin relevant documents. Things will work well if\nall relevant documents are tightly clustered around a single\nprototype, or, at least, if there are different prototypes, if the relevant\ndocuments have\nsignificant vocabulary overlap, while similarities between relevant and\nnonrelevant documents are small. Implicitly, the Rocchio relevance\nfeedback model treats relevant documents as a single\ncluster, which it models via the centroid of the cluster.\nThis approach does not work as well if the relevant documents are a\nmultimodal class, \nthat is, they consist\nof several clusters of documents\nwithin the vector space. This\ncan happen with:\n\n\nSubsets of the documents using different vocabulary, such as\n  Burma vs. Myanmar\n\nA query for which the answer set is inherently disjunctive, such\n  as Pop stars who once worked at Burger King.\n\nInstances of a general concept, which often appear as a disjunction\n  of more specific concepts, for example, felines.\n\n\nGood editorial content in the collection can often provide a solution to\nthis problem. For example, an article on the attitudes of\ndifferent groups to the situation in Burma could introduce the\nterminology used by different parties, thus linking the document clusters.\n\n\nRelevance feedback is not necessarily popular with users. Users are\noften reluctant to provide explicit feedback, or in general do not wish\nto prolong the search interaction. Furthermore, it is often harder to\nunderstand why a particular document was retrieved after relevance\nfeedback is applied.\n\n\nRelevance feedback can also have practical problems. The long queries\nthat are generated by straightforward application of relevance feedback\ntechniques are inefficient for a typical IR system. This results in a\nhigh computing cost for the retrieval and potentially long response\ntimes for the user. A partial solution to this is to only reweight\ncertain prominent terms in the relevant documents, such as perhaps the\ntop 20 terms by term frequency. Some experimental results have also\nsuggested that using a limited number of terms like this may give better\nresults (Harman, 1992) though other work has suggested that\nusing more terms is better in terms of retrieved document quality\n(Buckley et al., 1994b).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Relevance feedback on the\n Up: Relevance feedback and pseudo\n Previous: Probabilistic relevance feedback\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}