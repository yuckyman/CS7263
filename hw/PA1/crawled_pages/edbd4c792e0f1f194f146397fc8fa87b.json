{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html",
  "title": "A first take at building an inverted index",
  "body": "\n\n\n\n\nA first take at building an inverted index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Processing Boolean queries\n Up: Boolean retrieval\n Previous: An example information retrieval\n    Contents \n    Index\n\n\n\n \n\nA first take at building an inverted index\n\n\nTo gain the speed benefits of indexing at retrieval time, we\nhave to build the index in advance.  The major steps in this are:\n\n\nCollect the documents to be indexed:\n\n\n\n ...\n\nTokenize the text, turning each document into a list of tokens:\n\n \n\n\n \n ...\n\nDo linguistic preprocessing, producing a\n  list of normalized tokens, which are the indexing terms:\n\n \n\n\n \n ...\n\nIndex the documents that each term occurs in by creating an inverted index,\n  consisting of a dictionary and postings.\n\n\nWe will define and discuss the earlier stages of processing, that is,\nsteps 1-3,\nin Section 2.2 (page ).  Until then you can\nthink of tokens and normalized tokens as also loosely equivalent to\nwords.\nHere, we assume that the first 3 steps have already been done, and we\nexamine building a basic inverted index by\n sort-based indexing .\n\n\n\n\n\n\nWithin a document collection, we assume that each document\nhas a\nunique serial number, known as the document identifier\n ( docID ). During index construction, we can simply\nassign successive integers to each new document\nwhen it is first encountered.\nThe input to indexing is a list of normalized\ntokens for each document, which we can equally think of as a list of\npairs of term and docID, as in Figure 1.4 .  The core indexing\nstep is \n sorting \nthis list so that the terms are alphabetical,\ngiving us the representation in the middle column of\nFigure 1.4 .  Multiple occurrences of the same term from the\nsame document are then merged.Instances of the same term are then grouped, and the result is split\ninto a  dictionary  and\n postings , as shown in the right column of\nFigure 1.4 .  Since\na term generally occurs in a number of\ndocuments, this data organization already reduces the storage requirements of\nthe index.  The dictionary also records some statistics, such as the\nnumber of documents\nwhich contain each term (the  document frequency , which is here\nalso the length of each postings list).  This information is not\nvital for a basic Boolean search engine, but it allows us to\nimprove the efficiency of the search engine at query time, and it\nis a statistic later used in many ranked retrieval models.\nThe postings are secondarily sorted by docID.  This provides the basis\nfor efficient query processing.\nThis inverted index structure is essentially without rivals as\nthe most efficient structure for supporting ad hoc text search.\n\n\nIn the resulting index, we pay for storage of both\nthe dictionary and the postings lists.\nThe latter are much larger, but the dictionary is commonly kept in memory,\nwhile postings lists are normally kept on disk, so the size of each is\nimportant, and in Chapter 5  we will examine how\neach can be optimized for storage and access efficiency.\nWhat data structure should be used for a postings list?  A fixed length\narray would be wasteful as some words occur in many documents, and others\nin very few.\nFor an in-memory postings list, two good alternatives are singly linked\nlists or variable length arrays.  Singly linked lists allow cheap\ninsertion of documents into postings lists (following updates, such as\nwhen recrawling the web for updated documents), and naturally extend to\nmore advanced indexing strategies such as skip lists\n(Section 2.3 ), which require additional pointers.\nVariable length arrays win in space requirements by avoiding the\noverhead for pointers and in time requirements because their use of\ncontiguous memory increases speed on modern\nprocessors with memory caches.  Extra pointers can in practice be encoded\ninto the lists as offsets.  If updates are relatively infrequent,\nvariable length arrays will be more compact and faster to traverse.\nWe can also use a hybrid scheme with a linked list of fixed\nlength arrays for each term.  When postings lists are stored on disk,\nthey are stored (perhaps compressed) as a contiguous run of postings\nwithout explicit pointers (as in Figure 1.3 ), so as\nto minimize the size of the postings list and the number of disk seeks to\nread a postings list into memory.\n\n\nExercises.\n\nDraw the inverted index that would be built for the following document collection.  (See Figure 1.3  for an example.)\n\nDoc 1    new home sales top forecasts \nDoc 2    home sales rise in july \nDoc 3    increase in home sales in july \nDoc 4    july new home sales rise\n\n\n\n\n  \nConsider these documents:\n\nDoc 1    breakthrough drug for schizophrenia\nDoc 2    new schizophrenia drug\nDoc 3    new approach for treatment of schizophrenia\nDoc 4    new hopes for schizophrenia patients\n\n\n\nDraw the term-document incidence matrix for this document\n  collection.\n\nDraw the inverted index representation for this\n  collection, as in Figure 1.3 (page ).\n\n\n\n\nFor the document collection shown in\nExercise 1.2 , what are the returned results for\nthese queries:\n\n\nschizophrenia AND drug\n\nfor AND NOT(drug OR approach)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Processing Boolean queries\n Up: Boolean retrieval\n Previous: An example information retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}