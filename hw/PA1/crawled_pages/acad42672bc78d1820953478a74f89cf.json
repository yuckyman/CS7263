{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/results-snippets-1.html",
  "title": "Results snippets",
  "body": "\n\n\n\n\nResults snippets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Evaluation in information retrieval\n Previous: Refining a deployed system\n    Contents \n    Index\n\n\n\n \n\nResults snippets\n\n\nHaving chosen or ranked the documents matching a query, we wish to\npresent a results list that will be informative to the user.  In many\ncases the user will not want to examine all the returned documents and\nso we want to make the results list informative enough that the user\ncan do a final ranking of the documents for themselves based on\nrelevance to their information need.The standard way of doing this is to provide a  snippet , a short\nsummary of the document, which is designed so as to allow the user to\ndecide its relevance.  Typically, the snippet consists of the document title\nand a short summary, which is automatically extracted. The question is\nhow to design the summary so as to maximize its usefulness to the\nuser. \n\n\nThe two basic kinds of summaries are \n static , which are always the\nsame regardless of the query, and  dynamic  (or\nquery-dependent), which are\ncustomized according to the user's information need as deduced from a\nquery.  Dynamic summaries attempt to explain why a particular document\nwas retrieved \nfor the query at hand. \n\n\nA static summary is generally comprised of either or both a subset of\nthe document and  metadata  associated with the document.  The simplest\nform of summary takes the first two sentences or 50 words of a document,\nor extracts particular zones of a document, such as the title and\nauthor.  Instead of zones of a document, the summary can instead use\nmetadata associated with the document.  This may be an alternative way\nto provide an author or date, or may include elements which are designed\nto give a summary, such as the description metadata\nwhich can appear in the meta element \nof a web HTML page.  This summary is typically\nextracted and cached at indexing time, in such a way that it can be\nretrieved and presented quickly when displaying search results, whereas\nhaving to access the actual document content might be a relatively\nexpensive operation.\n\n\n\nThere has been extensive work within  natural language processing  (NLP)\non better ways to do  text summarization . Most such work still aims only\nto choose sentences from the original document to present and\nconcentrates on how to select good sentences. \n\n\nThe models typically\ncombine positional factors, favoring the first and last paragraphs of\ndocuments and the first and last sentences of paragraphs, with content\nfactors, emphasizing sentences with key terms, which have low\ndocument frequency in the collection as a whole, but high frequency and good\ndistribution across the particular document being returned.  In \nsophisticated NLP approaches, the system synthesizes sentences for a\nsummary, either by doing full text generation or by editing and perhaps\ncombining sentences used in the document.  For example, it might delete\na relative clause or replace a pronoun with the noun phrase that it\nrefers to.  This last class of methods\nremains in the realm of research and is seldom used for search results:\nit is easier, safer, and often even better to just use sentences from\nthe original document. \n\n\nDynamic summaries display one or more ``windows'' on the\ndocument, aiming to present the pieces that have the most utility to the\nuser in evaluating the document with respect to their information\nneed. Usually these windows contain one or several of the query \nterms, and so are often referred to as  keyword-in-context  (  )\nsnippets, though sometimes they may still be pieces of the text such as the\ntitle \nthat are selected for their query-independent information value just as in the case of static\nsummarization.  Dynamic summaries are generated in conjunction with\nscoring. \nIf the query is found as a phrase, occurrences of the phrase in the\ndocument will be shown as the summary.  If not, windows within the document\nthat contain multiple query terms will be selected. \nCommonly these windows may just stretch some number of words to the left\nand right of the query terms. This is a place where NLP\ntechniques can usefully be employed: users prefer snippets that read\nwell because they contain complete phrases.\n\n\n\n\n\n\nDynamic summaries are generally regarded as greatly improving the\nusability of IR systems, but they \npresent a complication for IR system design.  A\ndynamic summary cannot be precomputed, but, on the other hand, \nif a system has only a positional index, then it cannot easily \nreconstruct the context surrounding search engine hits in order to\ngenerate such a dynamic summary.  This is one reason for \nusing static summaries.  The standard solution to this in a\nworld of large and cheap disk drives is to \nlocally cache all the documents at index time (notwithstanding that this\napproach raises various legal, information security and control\nissues that are far from resolved) as shown in Figure 7.5 (page ). Then, a system can simply scan a document\nwhich is about to appear in a\ndisplayed results list to find snippets containing the query words.\nBeyond simply access to the text, \nproducing a good KWIC snippet requires some care.  Given a variety of\nkeyword occurrences in a document, the goal is to choose fragments which\nare: (i) maximally informative about the discussion of those terms in the\ndocument, (ii) self-contained enough to be easy to read, and\n(iii) short enough to fit within the normally strict constraints on the\nspace available for summaries.\n\n\nGenerating snippets must be fast since the\nsystem is typically generating many snippets for each query that it handles.\nRather than caching an\nentire document, it is common to cache only a generous but fixed size\nprefix of the document, such as perhaps 10,000 characters.  For most\ncommon, short documents, the entire document is thus cached, but huge\namounts of local storage will not be wasted on potentially vast\ndocuments.  Summaries of documents whose length exceeds the\nprefix size will be based on material in the\nprefix only, which is in general a useful zone in which to look for a\ndocument summary  \nanyway.  \n\n\nIf a document has been updated since it was last processed\nby a crawler and indexer, these\nchanges will be neither in the cache nor\nin the index. In these circumstances, neither the index nor the summary\nwill accurately reflect the current contents of the document, but it is the\ndifferences between the summary and the actual document content that will\nbe more glaringly obvious to the end user.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Evaluation in information retrieval\n Previous: Refining a deployed system\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}