{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/ponte-and-crofts-experiments-1.html",
  "title": "Ponte and Croft's Experiments",
  "body": "\n\n\n\n\nPonte and Croft's Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Language modeling versus other\n Up: The query likelihood model\n Previous: Estimating the query generation\n    Contents \n    Index\n\n\n\n\nPonte and Croft's Experiments\n\n\n\n\n\n\nPonte and Croft (1998) present the first experiments on the\nlanguage modeling approach to information retrieval.  Their basic\napproach is the model\nthat we have presented until now.  However, we have presented an\napproach where the language model is a mixture of two multinomials, much\nas in (Miller et al., 1999, Hiemstra, 2000) rather than Ponte and\nCroft's multivariate \nBernoulli model.  The use of multinomials has been standard in most\nsubsequent work in the LM approach and experimental results in IR, as\nwell as evidence from text classification which we consider in \nSection 13.3 (page ), suggests that it is superior.\nPonte and Croft argued strongly for the effectiveness of the\nterm weights that come from the language modeling approach over\ntraditional tf-idf weights.  We present a subset of their results in\nFigure 12.4  where they compare tf-idf to language modeling by\nevaluating TREC topics 202-250 over TREC disks 2 and 3.  The\nqueries are sentence-length natural language queries.  The language\nmodeling approach yields significantly better results than their\nbaseline tf-idf \nbased term weighting approach.  And indeed the gains shown here have\nbeen extended in subsequent work.\n\n\nExercises.\n\nConsider making a language model from the following training text:\n\nthe martian has landed on the latin pop sensation ricky martin\n\n\n\nUnder a MLE-estimated unigram probability model, what are\n   and \n?\n\n\n\nUnder a MLE-estimated bigram model, what are\n  \n and \n?\n\n\n\n\nSuppose we have a collection that consists of the 4 documents given in\nthe below table.   \n\n\ndocID\nDocument text\n\n1\nclick go the shears boys click click click\n\n2\nclick click\n\n3\nmetal here\n\n4\nmetal shears click here\n\n\n\nBuild a query likelihood language model for this document collection.\nAssume a mixture model between the documents and the collection, with\nboth weighted at 0.5. Maximum likelihood estimation (mle) is used to\nestimate both as unigram models.  Work out the model probabilities of\nthe queries click, shears, and hence click\n  shears for each document, and use those probabilities to rank\nthe documents returned by each query.  Fill in these probabilities in\nthe below table:\n\n\nQuery\nDoc 1\nDoc 2\nDoc 3\nDoc 4\n\nclick\n \n \n \n \n\nshears\n \n \n \n \n\nclick shears\n \n \n \n \n\n\n\nWhat is the final ranking of the documents for the query click\n  shears?\n\n\n\nUsing the calculations in Exercise 12.2.3  as inspiration or as\nexamples where appropriate, write one sentence each describing the\ntreatment that \nthe model in Equation 102\ngives\nto each of the following quantities.  Include whether it is present in\nthe model or not and whether the effect is raw or scaled.\n\n\nTerm frequency in a document\n\nCollection frequency of a term\n\nDocument frequency of a term\n\nLength normalization of a term\n\n\n\n\nIn the mixture model approach to the query likelihood model (Equation 104), the\nprobability estimate of a term is based on the term frequency of a\nword in a document, and the collection frequency of the word. Doing\nthis certainly guarantees that each term of a query (in the\nvocabulary) has a non-zero chance of being generated by each\ndocument. But it has a more subtle but important effect of\nimplementing a form of term weighting, related to what we saw in\nChapter 6 . \nExplain how this works. In particular, include in your answer a\nconcrete numeric example showing this term weighting at work. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Language modeling versus other\n Up: The query likelihood model\n Previous: Estimating the query generation\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}