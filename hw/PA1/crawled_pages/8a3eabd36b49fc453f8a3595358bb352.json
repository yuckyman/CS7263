{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html",
  "title": "Hierarchical clustering",
  "body": "\n\n\n\n\nHierarchical clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Hierarchical agglomerative clustering\n Up: irbook\n Previous: Exercises\n    Contents \n    Index\n\n\n\n\n\nHierarchical clustering\n\n\nFlat clustering is efficient and conceptually simple, but as\nwe saw in Chapter 16  it has a number of\ndrawbacks. \nThe algorithms introduced in Chapter 16 \nreturn a flat unstructured set of clusters,\nrequire a prespecified number of clusters as input and are\nnondeterministic.  \n Hierarchical clustering  (or\n hierarchic clustering ) outputs a hierarchy, a structure that is more\ninformative than the unstructured set of clusters returned by flat\nclustering.Hierarchical clustering does not require us to prespecify\nthe number of clusters and most hierarchical algorithms that\nhave been used in IR are\ndeterministic.  These advantages of hierarchical clustering\ncome at the cost of lower efficiency. The most common\nhierarchical clustering algorithms have a complexity that is\nat least quadratic in the number of documents compared to\nthe linear complexity of  -means and EM (cf. Section 16.4 , page 16.4 ).\n\n\nThis chapter first introduces agglomerative\nhierarchical clustering (Section 17.1 ) and presents\nfour different agglomerative algorithms, in\nSections 17.2 -17.4 , which differ in the\nsimilarity measures they employ: single-link, complete-link,\ngroup-average, and centroid similarity.  We then discuss the\noptimality conditions of hierarchical clustering in\nSection 17.5 .  \nSection 17.6  introduces\ntop-down (or divisive) hierarchical\nclustering.\nSection 17.7  looks at\nlabeling clusters automatically, a problem that must be solved whenever\nhumans interact with the output of clustering. We\ndiscuss \nimplementation issues in\nSection 17.8 .  Section 17.9  provides\npointers to further reading, including references to\nsoft\nhierarchical clustering, which we do\nnot cover in this book. \n\n\nThere are few differences between the\napplications of flat and hierarchical clustering in\ninformation retrieval. In particular, \nhierarchical clustering is appropriate for any of\nthe applications shown in\nTable 16.1  (page 16.1 ; see also\nSection 16.6 , page 16.6 ). In fact, the\nexample we gave for\ncollection clustering is hierarchical. In general, we\nselect flat\nclustering when \nefficiency is important and hierarchical clustering when one\nof the potential problems of flat clustering (not enough\nstructure, predetermined number of clusters, non-determinism)\nis a concern. In addition, many researchers\nbelieve that hierarchical clustering produces better\nclusters than flat clustering. However, there is no\nconsensus on this issue (see references in Section 17.9 ).\n\n\n\n\nSubsections\n\nHierarchical agglomerative clustering\nSingle-link and complete-link clustering\n\nTime complexity of HAC\n\n\nGroup-average agglomerative clustering\nCentroid clustering\nOptimality of HAC\nDivisive clustering\nCluster labeling\nImplementation notes\nReferences and further reading\nExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Hierarchical agglomerative clustering\n Up: irbook\n Previous: Exercises\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}