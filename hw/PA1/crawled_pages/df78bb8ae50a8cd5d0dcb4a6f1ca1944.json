{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html",
  "title": "K-means",
  "body": "\n\n\n\n\nK-means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Cluster cardinality in K-means\n Up: Flat clustering\n Previous: Evaluation of clustering\n    Contents \n    Index\n\n\n\n\n \n\nK-means\n -means is the most important flat\nclustering algorithm. Its objective is to minimize the\naverage squared Euclidean distance \n(Chapter 6 , page 6.4.4 )\nof documents from their cluster\ncenters where a cluster center is defined as the mean or\n centroid  of the documents in a cluster\n:\n\n\n\n\n\n\n(190)\n\n\n\nThe definition assumes that documents are represented as\nlength-normalized vectors in a real-valued space in the familiar way. We used\ncentroids for Rocchio classification in\nChapter 14  (page 14.2 ). They play a\nsimilar role here.  The ideal cluster in  -means\nis a sphere with the centroid as its center of gravity.\nIdeally, the clusters should not overlap.  Our desiderata\nfor classes in Rocchio classification were the same.  The\ndifference is that we have no labeled training set in\nclustering for which we know which documents should be in\nthe same cluster.\n\n\nA measure of how well the centroids represent the members of\ntheir clusters is the  residual sum of squares  or\n RSS , the squared distance of each\nvector from its centroid summed over all vectors:\n\n\n\n\n\n\n\n\n\n\n \n \n\n(191)\n\n\nRSS is the  objective function  in\n -means and our goal is to minimize it. Since  is fixed,\nminimizing RSS is equivalent to\nminimizing the average squared distance, a measure of how\nwell centroids represent their documents.\n\n\n\n\n\n\n\n\n\nThe first step of \n -means is to select as initial cluster\ncenters  randomly selected documents, the\n seeds .  The algorithm then moves the cluster\ncenters around in space in order to minimize RSS.  As shown\nin Figure 16.5 , this is done iteratively by repeating\ntwo steps until a stopping criterion is met: reassigning\ndocuments to the cluster with the closest centroid; and\nrecomputing each centroid based on the current members of\nits cluster.  Figure 16.6  shows snapshots from nine iterations of the\n -means algorithm for a set of points. The ``centroid'' column\nof Table 17.2  (page 17.2 ) shows examples of\ncentroids.\n\n\n  We can apply one of the\nfollowing termination conditions.\n\n\nA fixed number of iterations  has been\ncompleted. This condition limits the runtime of the\nclustering algorithm, but in some cases the quality of the\nclustering will be poor because of an insufficient number of\niterations.\n\nAssignment of documents to clusters (the partitioning\nfunction ) does not change between iterations.\nExcept for cases with a bad local minimum, this produces a\ngood clustering, but runtimes may be unacceptably long.\n\nCentroids  do not change between\niterations. This is equivalent to  not changing\n(Exercise 16.4.1 ).\n\n Terminate when RSS falls below a\nthreshold. This criterion ensures that the clustering is\nof a desired quality after termination.  In practice, we\nneed to combine it with a bound on the number of iterations\nto guarantee termination.\n\nTerminate when the decrease in RSS falls below a\nthreshold . For small , this indicates that\nwe are close to convergence.  Again, we need to combine it\nwith a bound on the number of iterations to prevent very\nlong runtimes.\n\n\n\nWe now show that  -means converges by proving that  \n monotonically decreases in each\niteration. We will use decrease in the meaning\ndecrease or does not change in this section.\nFirst, RSS decreases in the reassignment step\nsince each vector is assigned to the closest centroid, so\nthe distance it contributes to  decreases.\nSecond, it decreases in the recomputation step\nbecause the new centroid is the vector \nfor which  reaches its minimum.\n\n\n\n\n\n\n\n\n(192)\n\n\n\n\n(193)\n\n\nwhere  and  \nare the\n components of their respective vectors.\nSetting the partial derivative to zero, we get:\n\n\n\n\n\n \n \n\n(194)\n\n\nwhich is the componentwise definition of the centroid. Thus,\nwe minimize\n when the old centroid is replaced with the\nnew centroid.\n, the sum of the , must then also decrease during\nrecomputation. \n\n\nSince there is only a finite set of possible clusterings, a\nmonotonically decreasing algorithm will eventually arrive at\na (local) minimum. Take care, however, to break ties\nconsistently, e.g., by assigning a document to the cluster\nwith the lowest index if there are several equidistant\ncentroids.  Otherwise, the algorithm can cycle forever in a\nloop of clusterings that have the same cost.\n\n\n\n\n\n\nWhile this proves the convergence of\n -means, there is unfortunately no guarantee that a \nglobal minimum in the objective function will be reached. This is a particular\nproblem if a document set contains many\n outliers , documents that are far from any other documents and\ntherefore do not fit well into any cluster. Frequently, if an\noutlier is chosen as an initial seed, then\nno other vector is assigned to it during\nsubsequent iterations. Thus, we end up with a  singleton\ncluster  (a cluster with only one document) even though\nthere is probably a clustering with lower RSS.\nFigure 16.7  shows an example of a suboptimal\nclustering resulting from a bad choice of initial seeds.\n\n\nAnother type of suboptimal clustering that frequently occurs\nis one with empty clusters (Exercise 16.7 ).\n\n\n Effective heuristics for seed\nselection include (i) excluding outliers from the seed set;\n(ii) trying out multiple starting points and choosing the\nclustering with lowest cost; and (iii) obtaining seeds from\nanother method such as hierarchical clustering. Since\ndeterministic hierarchical clustering methods are more\npredictable than  -means, a hierarchical\nclustering of a small random sample of size  (e.g., for\n or ) often provides good seeds (see the\ndescription of the Buckshot algorithm, Chapter 17 ,\npage 17.8 ).\n\n\nOther initialization methods compute seeds that are not\nselected from the vectors to be clustered. A robust method\nthat works well for a large variety of document\ndistributions is to select  (e.g., ) random vectors\nfor each cluster and use their centroid as the seed for this\ncluster. See Section 16.6  for \nmore sophisticated initializations.\n\n\nWhat is the time complexity of  -means? Most of\nthe time is spent on computing vector distances.  One such\noperation costs . The reassignment step computes\n distances, so its overall complexity is\n. In the recomputation step, each vector gets\nadded to a centroid once, so the complexity of this step is\n. For a fixed number of iterations , the\noverall complexity is therefore\n . Thus,  -means\nis linear in all relevant factors: iterations, number of\nclusters, number of vectors and dimensionality of the\nspace. This means that  -means is more efficient than the\nhierarchical algorithms in Chapter 17 .  We had to\nfix the number of iterations , which can be tricky\nin practice.  But in most cases,  -means \nquickly reaches either complete convergence or a clustering that is\nclose to convergence. In the latter case, \na few documents would switch membership if further\niterations were computed, but this has a small effect on the\noverall quality of the clustering.\n\n\nThere is one subtlety in the preceding argument. Even a linear\nalgorithm can be quite slow if one of the arguments of \n is\nlarge, and  usually is large. \nHigh\ndimensionality is not a problem for computing the distance\nbetween\ntwo documents. Their vectors are sparse, so\nthat only a small fraction of the theoretically\npossible  componentwise differences need to be\ncomputed. \nCentroids, however, are dense since they pool all\nterms that occur in any of the documents of their\nclusters. As a result, distance computations are time consuming in a\nnaive implementation of  -means. \nHowever,\nthere are simple and\neffective heuristics for making centroid-document\nsimilarities as fast to compute as document-document\nsimilarities. Truncating\ncentroids to the most significant  terms (e.g., )\nhardly  decreases cluster quality while achieving a\nsignificant speedup of the reassignment step (see references\nin Section 16.6 ).\n\n\n \nThe same efficiency problem is addressed by  K-medoids , a variant\nof  -means that computes medoids instead of centroids as\ncluster centers. We define the  medoid  of a cluster as\nthe\ndocument vector that is\nclosest to the centroid. Since medoids are sparse document\nvectors, distance computations are fast.\n\n\n\n\n\nEstimated minimal residual sum of squares as a function of the number\nof clusters in  -means.\nIn this clustering of 1203 Reuters-RCV1 documents, there are two\npoints where the \n\n curve flattens: at 4 clusters and at\n9 clusters. The documents were\nselected from the categories China,\nGermany, Russia and Sports, so\nthe   clustering is closest to the\nReuters classification.\n\n\n\n\n\nSubsections\n\nCluster cardinality in K-means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Cluster cardinality in K-means\n Up: Flat clustering\n Previous: Evaluation of clustering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}