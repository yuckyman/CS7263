{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/exercises-3.html",
  "title": "Exercises",
  "body": "\n\n\n\n\nExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Hierarchical clustering\n Up: Flat clustering\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n\nExercises\n\n\nExercises.\n\n   Let  be a clustering\nthat exactly reproduces a class structure  and \na clustering that further subdivides some clusters in . Show\nthat  \n.\n\n\n\n Show that \n.\n\n\n\nMutual information is symmetric in the sense that\nits value\ndoes not change if the roles of clusters and\nclasses are switched: \n\n.\nWhich of the other three evaluation measures are symmetric\nin this sense?\n\n\n\nCompute RSS for the two clusterings in Figure 16.7 .\n\n\n\n   (i) Give an example of a\nset of points and three initial centroids (which need not be\nmembers of the set of points) for which 3-means\nconverges to a clustering with an empty cluster. (ii) Can\na clustering with an empty cluster be the global\noptimum with respect to RSS?\n\n\n\nDownload \nReuters-21578. \nDiscard documents that do not occur in one of the 10 classes\nacquisitions,\ncorn,\ncrude,\nearn,\ngrain,\ninterest,\nmoney-fx,\nship,\ntrade, and\nwheat. Discard documents that occur in two of these\n10 classes.\n(i) Compute a  -means clustering of this subset\ninto 10 clusters. \nThere are a number of software packages that implement\n -means, such as WEKA (Witten and Frank, 2005) and R\n(R Development Core Team, 2005).\n(ii) Compute purity, normalized mutual\ninformation,  and RI for the clustering with respect to\nthe 10 classes.\n(iii)\nCompile a confusion matrix\n(Table 14.5 , page 14.5 ) for the 10\nclasses and 10 clusters.\nIdentify classes that give rise to false\npositives \nand false negatives.\n\n\n\n   Prove that\n\n\nis monotonically decreasing in .\n\n\n\nThere is a soft version of  -means\nthat computes the fractional membership of a document in a\ncluster as a monotonically decreasing function of the\ndistance  from its centroid, e.g., as .\nModify reassignment and\nrecomputation steps of hard  -means for this soft version.\n\n\n\nIn the last iteration in Table 16.3 , document 6 is in cluster 2 even\nthough it was the initial seed for cluster 1. Why does the\ndocument change membership?\n\n\n\nThe values of the parameters  in iteration\n25 in Table 16.3  are rounded. What are the exact values\nthat EM will converge to?\n\n\n\nPerform a  -means clustering for the documents in\nTable 16.3 . \nAfter how many iterations does  -means converge?\nCompare the result with the EM clustering\nin Table 16.3  and discuss the differences.\n\n\n\nModify the expectation and maximization steps of EM\nfor a Gaussian mixture. The\nmaximization step computes the maximum likelihood parameter\nestimates , , and  for each\nof the clusters. The expectation step computes for each\nvector a soft assignment to clusters (Gaussians) based on their current\nparameters.\nWrite down the equations for Gaussian mixtures corresponding to\n and 202 .\n\n\n\nShow that  -means can be viewed as the limiting case\nof\nEM for Gaussian mixtures if variance is very small and all\ncovariances are 0. \n\n\n\nThe  within-point scatter  of a clustering\nis\ndefined as\n\n.\nShow that minimizing RSS and minimizing within-point scatter\nare equivalent.\n\n\n\nDerive an AIC criterion\nfor the multivariate Bernoulli\nmixture model \nfrom Equation 196.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Hierarchical clustering\n Up: Flat clustering\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}