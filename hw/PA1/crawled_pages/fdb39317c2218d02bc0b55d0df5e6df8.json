{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/dot-products-1.html",
  "title": "Dot products",
  "body": "\n\n\n\n\nDot products\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Queries as vectors\n Up: The vector space model\n Previous: The vector space model\n    Contents \n    Index\n\n\n\n\n \n\nDot products\n \nWe denote by   the vector derived from document , with one component in the vector for each dictionary term. Unless otherwise specified, the reader may assume that the components are computed using the tf-idf weighting scheme, although the particular weighting scheme is immaterial to the discussion that follows. The set of documents in a collection then may be viewed as a set of vectors in a vector space, in which there is one axis for each term. This representation loses the relative ordering of the terms in each document; recall our example from Section 6.2 (page ), where we pointed out that the documents Mary is quicker than John and John is quicker than Mary are identical in such a bag of words representation.\n\n\nHow do we quantify the similarity between two documents in this vector space? A first attempt might consider the magnitude of the vector difference between two document vectors. This measure suffers from a drawback: two documents with very similar content can have a significant vector difference simply because one is much longer than the other. Thus the relative distributions of terms may be identical in the two documents, but the absolute term frequencies of one may be far larger.\n\n\n\n\n\n  \nTo compensate for the effect of document length, the standard way of quantifying the similarity between two documents  and  is to compute the  cosine similarity  of their vector representations  and \n\n\n\n\n\n\n\n(24)\n\n\nwhere the numerator represents the  dot product  (also known as the  inner product ) of the vectors  and , while the denominator is the product of\ntheir   Euclidean lengths . The dot product \n of two vectors is defined as \n. Let  denote the document vector for , with  components \n.  The Euclidean length of  is defined to be \n.\n\n\nThe effect of the denominator of Equation 24 is thus to  length-normalize  the vectors  and  to unit vectors \n and\n\n. We can then rewrite (24) as\n \n\n\n\n\n\n(25)\n\n\n\nWorked example.\nConsider the documents in Figure 6.9 . We now apply Euclidean normalization to the tf values from the table, for each of the three documents in the table.  The quantity \n has the values 30.56, 46.84 and 41.30 respectively for Doc1, Doc2 and Doc3.  The resulting Euclidean normalized tf values for these documents are shown in Figure 6.11 .\n\n\n\n\nFigure 6.11:\nEuclidean normalized tf values for documents in Figure 6.9 .\n\n\n\nEnd worked example.\n\n Thus, (25) can be viewed as the dot product of the normalized versions of the two document vectors. This measure is the cosine of the angle  between the two vectors, shown in Figure 6.10 .  What use is the similarity measure \n? Given a document  (potentially one of the  in the collection), consider searching for the documents in the collection most similar to . Such a search is useful in a system where a user may identify a document and seek others like it - a feature available in the results lists of search engines as a more like this feature. We reduce the problem of finding the document(s) most similar to  to that of finding the  with the highest dot products ( values) \n. We could do this by computing the dot products between  and each of \n, then picking off the highest resulting  values.\n\n\n\n\n\n\nWorked example.\nFigure 6.12 shows the number of occurrences of three terms (affection, jealous and gossip) in each of the following three novels: Jane Austen's Sense and Sensibility (SaS) and Pride and Prejudice (PaP) and Emily Brontë's Wuthering Heights (WH). Of course, there are many other terms occurring in each of these novels. In this example we represent each of these novels as a unit vector in three dimensions, corresponding to these three terms (only); we use raw term frequencies here, with no idf multiplier. The resulting weights are as shown in Figure 6.13.\n\n\n\n\n\n\nNow consider the cosine similarities between pairs of the\nresulting three-dimensional vectors. A simple computation\nshows that sim((SAS), (PAP)) is 0.999,\nwhereas sim((SAS), (WH)) is 0.888; thus,\nthe two books authored by Austen (SaS and PaP) are\nconsiderably closer to each other than to\nBrontë's Wuthering Heights.\nIn fact, the similarity between the first two is almost perfect (when restricted to the three terms we consider).  Here we have considered tf weights, but we could of course use other term weight functions.\nEnd worked example.\n\nViewing a collection of  documents as a collection of vectors leads to a natural view of a collection as a   term-document matrix  and jealousy would under stemming be considered as a single dimension.  This matrix view will prove to be useful in Chapter 18 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Queries as vectors\n Up: The vector space model\n Previous: The vector space model\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}