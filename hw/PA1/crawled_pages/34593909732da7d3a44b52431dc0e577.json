{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html",
  "title": "Support vector machines: The linearly separable case",
  "body": "\n\n\n\n\nSupport vector machines: The linearly separable case\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Extensions to the SVM\n Up: Support vector machines and\n Previous: Support vector machines and\n    Contents \n    Index\n\n\n\n \n\nSupport vector machines: The linearly separable case\n\n\n\n\nFigure 15.1:\nThe support vectors are the 5 points right up against the margin of\n  the classifier.\n\n\n\n\nFor two-class, separable training data sets, such as the one in\nFigure 14.8 (page ), there are lots of \npossible linear separators.  Intuitively, a decision boundary\ndrawn in the middle of\nthe void between data items of the two classes seems better\nthan one which approaches very close to examples of one or both classes.\nWhile some learning methods such as the\nperceptron algorithm (see references in vclassfurther)\nfind just any linear separator, others, like Naive Bayes, search for \nthe best linear separator according to some criterion.  The SVM in\nparticular defines the criterion to be looking for a decision surface that is\nmaximally far away from any data point.  This distance from the\ndecision surface to the closest data point determines the\n margin  of the classifier.  \nThis method of construction necessarily means\nthat the decision function for an SVM is fully specified by a (usually\nsmall) subset of the data which defines the position of the separator.\nThese points are referred to as the  support\nvectors  (in a vector space, a point can be thought of as a vector\nbetween the origin and that point).  Figure 15.1  shows the margin and support\nvectors for a sample problem.  Other data points play no part in\ndetermining the decision surface that is chosen.\n\n\n\n\nAn intuition for large-margin classification.Insisting on a\n  large margin reduces the capacity of the model: the range of angles\n  at which the fat decision surface can be placed is smaller than for a\n  decision hyperplane (cf. vclassline).\n\n\nMaximizing the margin seems good because points near\nthe decision surface represent very uncertain classification decisions:\nthere is almost a 50% chance of the classifier deciding either way.  A\nclassifier with a large margin makes no low certainty classification\ndecisions.  This gives you a classification safety margin: a\nslight error in measurement or a slight document variation will not\ncause a misclassification.  Another intuition motivating SVMs is shown in\nFigure 15.2 .  By construction, an SVM classifier insists on a\nlarge margin around the decision boundary.  Compared to a decision\nhyperplane, if you have to place a fat separator between classes, you\nhave fewer choices of where it can be put.  As a result of this, the\nmemory capacity of the model has been decreased, and hence we expect that its\nability to correctly generalize to test data is increased (cf. the\ndiscussion of the  bias-variance tradeoff  in\nChapter 14 , page 14.6 ). \n\n\nLet us formalize an SVM with algebra.  A decision hyperplane\n(page 14.4 ) can be \ndefined by an intercept term  and a decision hyperplane normal vector\n which is \nperpendicular to the hyperplane.  This vector is commonly referred to\nin the machine learning literature as the  weight vector .  \nTo choose among \nall the hyperplanes that are perpendicular to the normal vector, we\nspecify the intercept term . \nBecause the hyperplane is perpendicular to the normal vector, all\npoints  on the hyperplane satisfy \n.\nNow suppose that we have a set of\ntraining data points \n, where each member is a pair of a point  and a class label  corresponding to it.For SVMs, the two data classes are always named \nand  (rather than 1 and 0), and the intercept term is always\nexplicitly represented as  (rather than being folded into the weight\nvector  by adding an extra always-on feature).  The\nmath works out much more cleanly if you do things this way, as we\nwill see almost immediately in the definition of functional margin.  The\nlinear classifier is then:\n\n\n\n\n\n\n(165)\n\n\nA value of  indicates one class, and a value of  the other class.\n\n\nWe are confident in the classification of a point if it is far away from\nthe decision boundary.  For a given data set and decision hyperplane, we\ndefine the  functional margin  of the \n example  with respect to a hyperplane\n\n as the quantity \n\n.  The\nfunctional margin of a data set with respect to a decision surface is then twice the\nfunctional margin of any of the points in the data set with minimal functional\nmargin (the factor of 2 comes from measuring across\nthe whole width of the margin, as in Figure 15.3 ).\nHowever, there is a problem with using this \ndefinition as is: the value is underconstrained, because \nwe can always make the functional margin as big as we wish\nby simply scaling up  and .  For example, if we replace \nby  and  by  then the functional margin\n\n \nis five times as large.  This suggests that we need to place\nsome constraint on the size of the  vector.  To get a sense of\nhow to do that, let us look at the actual geometry.\n\n\n\n\nFigure 15.3:\nThe geometric margin of a point () and a decision boundary\n  ().\n\n\n\n\nWhat is the Euclidean distance from a point  to the decision\nboundary?  In Figure 15.3 , we denote by  this distance.\nWe know that the shortest distance between a point and a \nhyperplane is perpendicular to the plane, and hence, parallel to\n.   A unit vector in this direction is \n.\nThe dotted line in the diagram is then a translation of the vector\n\n.\nLet us label the point on the hyperplane closest to  as\n.  Then:\n\n\n\n\n\n\n(166)\n\n\nwhere multiplying by  just changes the sign for the two cases\nof  being on either side of the decision surface.\nMoreover,  lies on the decision boundary and so satisfies\n\n.  Hence:\n\n\n\n\n\n\n(167)\n\n\nSolving for  gives:\n\n\n\n\n\n\n(168)\n\n\nAgain, the points closest to the separating hyperplane are\nsupport vectors.  The  geometric margin  of the classifier is the\nmaximum width of the band that can be drawn separating the support\nvectors of the two classes. \nThat is, it is twice the minimum value over data points for  given\nin Equation 168, or, equivalently,\nthe maximal width of one of the \nfat separators shown in Figure 15.2 .  The geometric margin is\nclearly invariant to scaling of parameters: if we replace  by\n and  by \n, then the geometric margin is the same, because it is inherently\nnormalized by the\nlength of .  This means that we can impose any scaling constraint we\nwish on  without affecting the geometric margin.\nAmong other choices, we could use unit vectors, as in Chapter 6 ,\nby requiring that .  This would have the effect of making\nthe geometric margin the same as the functional margin.\n\n\nSince we can scale the functional margin as we please, for convenience\nin solving large SVMs, let us choose to require\nthat the functional margin of \nall data points is at least 1 and that it is equal to 1 for at least\none data vector.  That is, for all items in the data:\n\n\n\n\n\n\n(169)\n\n\nand there exist support vectors for which the inequality\nis an equality.  Since each \nexample's distance from the hyperplane is \n\n, the geometric margin\nis \n.   Our desire is still to maximize this geometric\nmargin.\nThat is, we want to find  and  such that:\n\n\n\n is maximized\n\nFor all \n, \n\n\n\nMaximizing  is the same as minimizing \n.  This gives the final standard formulation of an SVM \nas a minimization problem:\n\n\n\n\nWe are now optimizing a quadratic function subject to linear\nconstraints.   Quadratic optimization \nproblems are a standard, well-known \nclass of mathematical optimization problems, and many algorithms exist\nfor solving them.  We could in principle build our SVM using standard\nquadratic programming (QP) libraries, but there has been much recent research \nin this area aiming to exploit the structure of the kind\nof QP that emerges from an SVM.  As a result, \nthere are more\nintricate but much faster and more scalable libraries available\nespecially for building SVMs, which almost everyone uses to build models.\nWe will not present the details of such algorithms here.  \n\n\nHowever, it will be helpful to what follows to understand the shape of\nthe solution of such an optimization problem.  \nThe solution involves constructing a dual problem where a \nLagrange multiplier  is associated with each constraint \n\n in the\nprimal problem:\n\n\n\nThe solution is then of the form:\n\n\n\nIn the solution, most of the  are zero.  Each non-zero\n indicates that the corresponding  is a support vector.\nThe classification function is then:\n\n\n\n\n\n\n(170)\n\n\nBoth the term to be maximized in the dual problem and the\nclassifying function involve a dot product between pairs of\npoints ( and  or  and ),\nand that is the only way the data are\nused - we will return to the significance of this later. \n\n\nTo recap, we start with a training data set.  The data set\nuniquely defines the best separating hyperplane, and we feed\nthe data through a quadratic optimization procedure to find\nthis plane.  Given a new point  to classify, the\nclassification function  in either\nEquation 165 or Equation 170 is\ncomputing the projection of the point onto the hyperplane\nnormal.  The sign of this function determines the \nclass to assign to the point.  If the point is within\nthe margin of the classifier (or another confidence\nthreshold  that we might have determined to minimize\nclassification mistakes) then the classifier can return\n``don't know'' rather than one of the two classes.  The\nvalue of  may also be transformed into a\nprobability of classification; fitting a sigmoid to\ntransform the values is standard\n(Platt, 2000).  Also, since the\nmargin is constant, if the model includes dimensions from various\nsources, careful rescaling of some dimensions may be\nrequired. However, this is not a problem if our documents (points) are\non the unit hypersphere.\n\n\n\n\nFigure 15.4:\nA tiny 3 data point training set for an SVM.\n\n\n\n\nWorked example. Consider building an SVM over the (very little) data set shown in\nFigure 15.4 . Working geometrically, for an example like this,\nthe maximum margin weight \nvector will be parallel to the shortest line connecting points of the two\nclasses, that is, the line between  and , giving a\nweight vector of .\nThe optimal decision surface is orthogonal to that line and intersects\nit at the halfway point. Therefore, it passes through . So,\nthe SVM decision boundary is: \n\n\n\n\n\n\n(171)\n\n\n\nWorking algebraically, with the standard constraint that\n\n, we seek\nto minimize \n.  This happens when this constraint is satisfied with\nequality by the two support vectors.  Further we know that the\nsolution is \n for some .  So we have that:\n\n\n\n\n\nTherefore,  and .  So the optimal hyperplane is\ngiven by \n and .\n\n\nThe margin  is \n.  This answer can be confirmed geometrically by examining Figure 15.4 .\n\n\nEnd worked example.\n\nExercises.\n\nWhat is the minimum number of support vectors\nthat there can \nbe for a data set (which contains instances of each class)?\n\n\n\nThe basis of being able to use kernels in SVMs (see Section 15.2.3 ) is that the\nclassification function can be written in the form of\nEquation 170 (where, for large problems, most \nare 0).  Show explicitly how the classification function could be\nwritten in this form for the data set from\nsmall-svm-eg.  That is, write  as a function where\nthe data points appear and the only variable is . \n\n\n\nInstall an SVM package such as SVMlight\n(http://svmlight.joachims.org/), and build an SVM for the data\nset discussed in small-svm-eg.  Confirm that the program\ngives the same solution as the text.  For SVMlight, or another package\nthat accepts the same training data format, the training file would\nbe:\n\n1 1:2 2:3\n1 1:2 2:0\n1 1:1 2:1\n\n\nThe training command for SVMlight is then:\n\nsvm_learn -c 1 -a alphas.dat train.dat model.dat\n\nThe -c 1 option is needed to turn off use of the slack variables\nthat we discuss in Section 15.2.1 .  Check that the norm of\nthe weight vector agrees with what we found in\nsmall-svm-eg.  Examine the file alphas.dat\nwhich contains\nthe  values, and check that they agree with your answers in\nExercise 15.1 . \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Extensions to the SVM\n Up: Support vector machines and\n Previous: Support vector machines and\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}