{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/frequency-based-feature-selection-1.html",
  "title": "Frequency-based feature selection",
  "body": "\n\n\n\n\nFrequency-based feature selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Feature selection for multiple\n Up: Feature selection\n Previous: Assessing as a feature\n    Contents \n    Index\n\n\n\n\n\nFrequency-based feature\n  selection\n\nA third feature selection method is\n frequency-based\nfeature selection , that is, selecting the terms that are\nmost common in the class. \nFrequency can be either defined as document frequency (the number of documents\nin the class  that contain the term ) or as\ncollection frequency (the number of\ntokens of  that occur in documents in\n). Document frequency is\nmore appropriate for the Bernoulli model, collection frequency for the\nmultinomial model.\n\n\nFrequency-based feature selection\nselects some\nfrequent terms that have no specific information about the\nclass, for example, the days of the week (Monday,\nTuesday, ...), which are frequent across classes\nin newswire text. When many thousands of features are selected,\nthen frequency-based feature selection often does\nwell.\nThus, if somewhat suboptimal accuracy is acceptable, then\nfrequency-based feature selection can be a good alternative to\nmore complex methods. However, Figure 13.8  is a case where\nfrequency-based feature selection performs a lot worse than\nMI and  and should not be\nused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Feature selection for multiple\n Up: Feature selection\n Previous: Assessing as a feature\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}