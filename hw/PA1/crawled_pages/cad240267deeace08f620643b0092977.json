{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/k-nearest-neighbor-1.html",
  "title": "k nearest neighbor",
  "body": "\n\n\n\n\nk nearest neighbor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Time complexity and optimality\n Up: Vector space classification\n Previous: Rocchio classification\n    Contents \n    Index\n\n\n\n\n \n\nk nearest neighbor\n\n\nUnlike Rocchio,  \nnearest neighbor  or  kNN\nclassification  determines the decision boundary\nlocally. For 1NN we assign each document to the class of its\nclosest neighbor. For kNN we assign each document to the majority class of its\n closest neighbors where  is a parameter.  The rationale of kNN classification\nis that, based on the contiguity hypothesis, \nwe expect a test document \nto have the same label as the\ntraining documents located in the local region surrounding .\n\n\nDecision boundaries in 1NN are concatenated\nsegments of the  Voronoi tessellation  as shown in\nFigure 14.6 .  The Voronoi tessellation of a\nset of objects decomposes space into Voronoi cells, where\neach object's cell consists of all points that are closer to\nthe object than to other objects. In our case, the objects\nare documents.  \nThe Voronoi tessellation then partitions\nthe plane into \n convex polygons, each containing its\ncorresponding document (and no other)\nas shown in Figure 14.6 , where\na convex polygon is a convex region in\n2-dimensional space bounded by\nlines. \n\n\nFor general \n in kNN,\nconsider the region in the space for which the set\nof  nearest neighbors is the same. This again is a convex\npolygon and the space is partitioned into convex\npolygons , within each of which the\nset of  nearest neighbors is invariant (Exercise 14.8 ).\n\n1NN is not very robust. The classification decision\nof each test document\nrelies on the class of a single training document, which may be incorrectly\nlabeled or \natypical. kNN for  is more robust. It\nassigns documents to the majority class of their \nclosest neighbors, with ties\nbroken randomly.\n\n\nThere is a probabilistic\nversion of this kNN classification algorithm. We can estimate the probability of\nmembership in  class \nas the proportion of the  nearest neighbors\nin . \nFigure 14.6  gives an example for\n. Probability estimates for class membership of the\nstar are \n,\n\n, and\n\n.  \nThe \n3nn estimate (\n) \nand \nthe 1nn estimate (\n)\ndiffer with \n3nn preferring the X class\nand \n1nn preferring the circle class .\n\n\nThe parameter  in kNN is often chosen based on experience or\nknowledge about the classification problem at hand. It is\ndesirable for  to be odd to make ties less likely. \nand  are common choices, but much larger values between\n50 and 100 are also used.  An alternative way of setting\nthe parameter is to select the  that gives best results\non a  held-out  portion of the training set.\n\n\n\n\n\n\nWe can also weight the ``votes'' of the  nearest\nneighbors by their cosine similarity. In this scheme, a class's\nscore is computed as:\n\n\n\n\n\n\n(143)\n\n\nwhere  is the set of 's  nearest neighbors\nand \n iff  is in class  and 0 otherwise.\nWe then assign the document to the class with the highest\nscore. Weighting by similarities is often more accurate than\nsimple voting. For\nexample, if two classes have the same number of neighbors in\nthe top , the class with the more similar neighbors wins.\n\n\nFigure 14.7  summarizes the kNN algorithm.\n\n\nWorked example.\nThe distances of the test document from the four training\ndocuments in \nTable 14.1  are\n\n and\n\n. 's nearest neighbor\nis therefore  and 1NN assigns  to 's class,\n.\nEnd worked example.\n\n\n\nSubsections\n\nTime complexity and optimality of kNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Time complexity and optimality\n Up: Vector space classification\n Previous: Rocchio classification\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}