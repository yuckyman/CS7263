{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/a-vector-space-model-for-xml-retrieval-1.html",
  "title": "A vector space model for XML retrieval",
  "body": "\n\n\n\n\nA vector space model for XML retrieval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Evaluation of XML retrieval\n Up: XML retrieval\n Previous: Challenges in XML retrieval\n    Contents \n    Index\n\n\n\n\n \n\nA vector space model for XML retrieval\n \nIn this section, we present a simple vector space model for\nXML retrieval. It is not intended to be a complete\ndescription of a state-of-the-art system. Instead, we want\nto give the reader a flavor of how documents can be\nrepresented and retrieved in XML retrieval.\n\n\n\n\nFigure 10.8:\nA mapping of an XML document (left) to a set of\nlexicalized subtrees (right).\n\n\n\n\nTo take account of structure in retrieval\nin\nFigure 10.4 , we want\na book entitled Julius Caesar to be a match for \nand no match (or a lower weighted match) for .\nIn unstructured retrieval, there would be a single dimension of\nthe vector space for Caesar. In XML retrieval, we must\nseparate the title word Caesar from the author name\nCaesar. One way of doing this is to have each\ndimension of the vector space encode a word together with its position within\nthe XML tree.\n\n\nFigure 10.8  illustrates this representation.\nWe first take each text node (which in our setup is always a\nleaf) and break it into multiple nodes, one for each word.\nSo the leaf node Bill Gates is split into two leaves\nBill and\nGates.\nNext we define\nthe dimensions of the vector space to be\n lexicalized subtrees \n of documents - subtrees that contain at least one vocabulary\nterm.\nA subset of these possible lexicalized\nsubtrees is shown in the figure, but there are\nothers - e.g., the subtree corresponding to the whole\ndocument with the leaf node Gates removed.\nWe can now represent queries and documents as vectors in\nthis space of lexicalized subtrees and compute matches between them.\nThis means that we can use the vector space formalism from\nChapter 6  for XML retrieval. The main difference\nis that the dimensions of vector space in unstructured\nretrieval are vocabulary terms  whereas they are \nlexicalized subtrees in XML retrieval.\n\n\nThere is a  tradeoff between the\ndimensionality of the space and accuracy of query results.\nIf we trivially restrict dimensions to vocabulary terms, then\nwe have a standard vector space retrieval system that will\nretrieve many documents that do not match the structure of\nthe query (e.g., Gates in the title as opposed to the\nauthor element). If we create a separate dimension for each\nlexicalized subtree occurring in the collection, the dimensionality of\nthe space becomes too large.\nA compromise is to index all\npaths that end in a single vocabulary term, in other words,\nall XML-contextterm pairs.  We call such an\nXML-contextterm pair a\n  structural term  and denote\nit by \n: a pair of XML-context  and vocabulary term\n.  The\ndocument in Figure 10.8  has nine structural\nterms. Seven are shown (e.g.,\n\"Bill\" and Author#\"Bill\") and two are not\nshown: /Book/Author#\"Bill\" and\n/Book/Author#\"Gates\". \nThe tree with the leaves\nBill and Gates is a lexicalized subtree that is not a\nstructural term.\nWe use the previously introduced pseudo-XPath notation for\nstructural terms.\n\n\nAs we discussed in the last section\nusers are bad at remembering\ndetails about the schema and at constructing queries that\ncomply with the schema.\nWe will therefore interpret all queries as\nextended queries - that is, there can be an arbitrary\nnumber of intervening nodes  in the document for any\nparent-child node pair in the query.\nFor example, we interpret  in Figure 10.7  as .\n\n\nBut we still\nprefer documents that match the query structure closely by\ninserting fewer additional nodes. We ensure that retrieval\nresults respect this preference by computing a weight for\neach match. A simple measure of the similarity of \na path  in a query\nand \na path  in a document \nis the following  context resemblance  function CR:\n\n\n\n\n\n\n(52)\n\n\nwhere  and  are the number of nodes in  the query path\nand document path, respectively, and  matches  iff\nwe can transform\n\ninto  by inserting additional nodes.\nTwo examples from  Figure 10.6  are\n\n and\n\n where\n\n and  are the relevant paths from top to\nleaf node in ,  and , respectively.\nThe value of \n\nis  if  and  are identical.\n\n\nThe final score for a document is computed as a variant of\nthe cosine measure (Equation 24,\npage 6.3.1 ), \nwhich we call SIMNOMERGE for reasons that will\nbecome clear shortly.\nSIMNOMERGE is defined as follows:\n\n\n\n\n\n\n(53)\n\n\nwhere  is the vocabulary of non-structural terms;  is\nthe set of all XML contexts; and\n\n\nand\n\n\nare\nthe weights of term  in XML context  in query  and\ndocument , respectively. We compute the weights\nusing one of the weightings from Chapter 6 , such as\n\n.\nThe inverse document frequency \ndepends on which elements we use to compute  as\ndiscussed in Section 10.2 .\nThe similarity measure \n is not a true cosine\nmeasure since its value can be larger than 1.0\n(Exercise 10.7 ).\nWe divide by \n to normalize for document length\n(Section 6.3.1 , page 6.3.1 ).\nWe have omitted query length\nnormalization to simplify the formula. It has no effect on\nranking since, for a given query, the normalizer\n\n is the same for all documents.\n\n\n\n\nFigure 10.9:\nThe algorithm for scoring documents with SIMNOMERGE.\n\n\n\nThe algorithm for computing\nSIMNOMERGE for all documents in the collection is shown in Figure 10.9 .\n The array normalizer in Figure 10.9 \n  contains \n\n\nfrom Equation 53 for each document.\n\n\n\n\nFigure 10.10:\n\nScoring of a query with one structural term in SIMNOMERGE.\n\n\n\n\n\nWe give an example of how SIMNOMERGE computes\nquery-document similarities\nin Figure 10.10 .\n\n is one of the structural terms in the query.\nWe successively retrieve all postings lists for structural\nterms \n with the same vocabulary term .\nThree example postings lists are shown.\nFor the first one, \nwe have\n\n since the two contexts are identical.\nThe next context has no context resemblance with :\n\n and the corresponding postings\nlist is ignored.\nThe context match of  with  is 0.63>0 and it will\nbe processed.\nIn this example, the highest ranking document is  with a\nsimilarity of \n. To\nsimplify the figure, the query\nweight of \n is assumed to be 1.0.\n\n\nThe query-document similarity function in Figure 10.9 \nis called SIMNOMERGE because different XML contexts\nare kept separate for the purpose of weighting.  An\nalternative  similarity function is\nSIMMERGE which relaxes the matching conditions of\nquery and document further in the following three ways.\n\n\nWe collect the \nstatistics used for computing\n\n and\n\n from all contexts that have a\nnon-zero resemblance to  (as opposed to just from  as\nin \nSIMNOMERGE).  For instance, for\ncomputing the document frequency of the structural term\natl#\"recognition\", we also count\noccurrences of\nrecognition in XML contexts fm/atl,\narticle//atl etc.\n\nWe modify  Equation 53\nby merging  all structural terms in the document that have a\nnon-zero context resemblance to a given query structural term.\nFor example, the contexts\n/play/act/scene/title and\n/play/title in the document will be merged when matching against the\nquery term\n/play/title#\"Macbeth\". \n\nThe context resemblance function is further relaxed:\n  Contexts have a non-zero resemblance in many cases where\n  the definition of CR in\n  Equation 52 returns 0.\n\n\nSee the references in Section 10.6  for details.\n\n\nThese three changes alleviate the problem of sparse term\nstatistics discussed in Section 10.2  and increase\nthe robustness of the matching function against poorly posed\nstructural queries. The evaluation of\nSIMNOMERGE and SIMMERGE\nin the\nnext section shows that the relaxed\nmatching conditions of SIMMERGE increase the\neffectiveness of XML retrieval.\n\n\nExercises.\n\nConsider computing df for a structural term as the\nnumber of times that the structural term occurs under a\nparticular parent node.  Assume the following: the\nstructural term \n author#\"Herbert\" occurs once as\nthe child of the node squib; there are 10\nsquib nodes in the collection; \n occurs 1000\ntimes as the child of article; there are 1,000,000\narticle nodes in the collection.  The idf weight of\n\n then is \n when occurring as\nthe child of squib and \n when occurring as the child of\narticle. (i) Explain why this is not an appropriate\nweighting for \n. Why should \n not receive a weight that\nis three times higher in articles than in squibs?  (ii) \nSuggest a better way of computing idf.\n\n\n\nWrite down all the structural terms occurring in\nthe XML document in Figure 10.8 .\n\n\n\nHow many structural terms does the document in\nFigure 10.1  yield?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Evaluation of XML retrieval\n Up: XML retrieval\n Previous: Challenges in XML retrieval\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}