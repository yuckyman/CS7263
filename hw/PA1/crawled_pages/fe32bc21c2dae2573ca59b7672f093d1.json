{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/blocked-sort-based-indexing-1.html",
  "title": "Blocked sort-based indexing",
  "body": "\n\n\n\n\nBlocked sort-based indexing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Single-pass in-memory indexing\n Up: Index construction\n Previous: Hardware basics\n    Contents \n    Index\n\n\n\n\n \n\nBlocked sort-based indexing\n\n\n\nThe basic steps in constructing a nonpositional index are\ndepicted in Figure 1.4 (page ). We first make a pass\nthrough the collection assembling all \nterm-docID pairs.  We then sort the pairs with the term\nas the dominant key and docID as the secondary key. Finally,\nwe organize the docIDs for each term into a postings list and\ncompute statistics like term and document frequency.  For\nsmall collections, all this can be done in memory. In this\nchapter, we describe methods for large collections that\nrequire the use of secondary storage.\n\n\nTo make index construction more efficient, we represent\nterms as termIDs (instead of strings\nas we did in Figure 1.4 ), where each\n termID \nis a unique serial number.\nWe can build\nthe mapping from terms to termIDs on the fly while we are\nprocessing the collection; or, in a two-pass\napproach, we compile the vocabulary in the first pass and\nconstruct the inverted index in the second pass. The\nindex construction algorithms described in this chapter all\ndo a single pass through the data.\nSection 4.7  gives references to multipass\nalgorithms that are preferable in certain applications, for\nexample, when disk space is scarce.\n\n\n We work with the\n Reuters-RCV1  collection as our model collection in\nthis chapter, a collection with roughly 1 GB of\ntext.  It consists of about 800,000 documents that were sent\nover the Reuters newswire during a 1-year period between\nAugust 20, 1996, and August 19, 1997. A typical document is\nshown in Figure 4.1 , but note that we ignore\nmultimedia information like images in this book and are only\nconcerned with text.  Reuters-RCV1 covers a wide range of\ninternational topics, including politics, business, sports,\nand (as in this example) science. Some key statistics of the\ncollection are shown in Table 4.2 .\n\n\nReuters-RCV1 has 100 million tokens.  Collecting all\ntermID-docID pairs of the collection using 4 bytes each for termID and docID\ntherefore requires 0.8 GB of storage.\nTypical collections today are often one or two orders of\nmagnitude larger than Reuters-RCV1.  You can easily see how\nsuch collections overwhelm even large computers if we\ntry to sort their termID-docID pairs in memory. If the size of\nthe intermediate files during index construction is within a small factor of available\nmemory, then the compression techniques introduced in\nChapter 5  can help; however, the postings file of many\nlarge collections cannot fit into memory even after\ncompression.\n\n\n\n\n\n\nTable:\nCollection statistics for Reuters-RCV1. Values are\nrounded for the computations in this book.\nThe unrounded\nvalues are:\n806,791 documents, 222\ntokens per document, 391,523 (distinct) terms,\n6.04\nbytes per token with spaces and punctuation, \n4.5 bytes per token without spaces and punctuation,\n7.5 bytes per term,\nand 96,969,056 tokens.\nThe\nnumbers in this table correspond to the third line (``case\nfolding'') in icompresstb5. \n Symbol\nStatistic\nValue\n \n \ndocuments\n800,000\n \n \navg.  # tokens per document\n200\n \n \nterms\n400,000\n \n  \navg.  # bytes per token (incl. spaces/punct.)\n6\n \n  \navg.  # bytes per token (without spaces/punct.)\n4.5\n \n  \navg.  # bytes per term\n7.5\n \n \ntokens\n100,000,000\n \n\n \n\n\n\n\n\nFigure 4.1:\nDocument from the Reuters newswire.\n\n\n\n\nWith main memory insufficient, we need to use an\n external sorting algorithm , that is, one that uses\ndisk.  For acceptable speed, the central requirement of such\nan algorithm is that it minimize the number of random disk\nseeks during sorting - sequential disk reads are far faster\nthan seeks as we explained in Section 4.1 .  One solution is the\n blocked sort-based indexing algorithm  or\n BSBI \nin Figure 4.2 . BSBI \n(i) segments the collection into parts of equal size,\n(ii)\nsorts the termID-docID pairs of each part in memory,\n(iii) stores intermediate sorted results on disk, and (iv) merges all\nintermediate results into the final index.\n\n\nThe algorithm parses documents into\ntermID-docID pairs and accumulates the pairs in memory until a block\nof a fixed size is full (PARSENEXTBLOCK in Figure 4.2 ).  We\nchoose the block size to fit comfortably into memory to\npermit a fast in-memory sort.  The block is then inverted\nand written to disk.  Inversion  involves two steps. First, we\nsort the termID-docID pairs. Next, we collect all\ntermID-docID pairs with the\nsame termID into a postings list, where a  posting  is simply\na docID. The result, an inverted\nindex for the block we have just read, is then written to\ndisk.  Applying this to Reuters-RCV1 and assuming we can fit\n10 million termID-docID pairs into memory, we end up with ten blocks,\neach an inverted index of one part of the collection.\n\n\n\n\n\n\n\n\n\n\n\nMerging in blocked sort-based indexing.Two blocks (``postings lists to be merged'') are loaded from disk into\nmemory, merged in memory (``merged postings lists'') and  written\nback to disk. We show terms instead of termIDs for better readability.\n\n\n\nIn the final step, the algorithm simultaneously merges the\nten blocks into one large merged index. An example with two\nblocks is shown in Figure 4.3 , where we use   to\ndenote the  document of the collection.  To do\nthe merging, we open\nall block files simultaneously, and maintain small read\nbuffers for the ten blocks we are reading and a write buffer for\nthe final merged index we are writing.  \nIn each iteration, we select the lowest termID that has not\nbeen processed yet using a priority queue or a similar data\nstructure. All postings lists for this termID are read\nand merged, and the merged list is written back to disk.\nEach read buffer is refilled from its file when\nnecessary.\n\n\nHow expensive is BSBI? Its time complexity\nis \n because the step with the highest time\ncomplexity is sorting and  is an upper bound for the\nnumber of items we must sort (i.e., the number of\ntermID-docID pairs). But the actual indexing time is\nusually dominated by\nthe time it takes to parse the documents (PARSENEXTBLOCK) and to do the final merge (MERGEBLOCKS).  Exercise 4.6  asks you to\ncompute the total index construction time for RCV1 that\nincludes these steps as well as inverting the blocks and writing them\nto disk. \n\n\nNotice that Reuters-RCV1 is not\nparticularly large in an age when one or more GB of memory\nare standard on personal computers.  With appropriate\ncompression (Chapter 5 ), we could have created an\ninverted index for RCV1 in memory on a not overly beefy\nserver.  The techniques we have described are needed,\nhowever, for collections that are several orders of\nmagnitude larger.\n\n\nExercises.\n\n  \nIf we need  comparisons (where  is\nthe number of termID-docID pairs) and two disk seeks for each\ncomparison, how much time would index construction for\nReuters-RCV1 take if we used \ndisk instead of memory for storage and an unoptimized\nsorting algorithm (i.e., not an external sorting algorithm)?\nUse the system parameters in Table 4.1 .\n\n\n\nHow would you create the dictionary in\nblocked sort-based indexing on the fly to avoid an extra pass\nthrough the data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Single-pass in-memory indexing\n Up: Index construction\n Previous: Hardware basics\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}