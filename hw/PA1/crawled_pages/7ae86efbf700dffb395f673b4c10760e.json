{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/feature-selection-1.html",
  "title": "Feature selection",
  "body": "\n\n\n\n\nFeature selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Mutual information\n Up: Text classification and Naive\n Previous: A variant of the\n    Contents \n    Index\n\n\n\n\n \n \n\n\nFeature selection\n Feature selection \nis the process of selecting a subset of the terms\noccurring in\nthe training set and using only this subset as features\nin text classification. Feature selection\nserves two main purposes. First, it makes training and\napplying a classifier more efficient by decreasing the size\nof the effective vocabulary. This is of particular\nimportance for classifiers that, unlike NB, are\nexpensive to train. Second, feature selection often\nincreases classification accuracy by eliminating noise\nfeatures. A    noise feature \nis one that, when added to the document representation,\nincreases the classification error on new data. Suppose a\nrare term, say arachnocentric, has no information\nabout a class, say China, but all instances of\narachnocentric happen to occur in\nChina documents in our training set. Then the learning method might\nproduce a classifier that misassigns test documents containing\narachnocentric to China. Such an incorrect\ngeneralization from an accidental property of the training\nset is called   overfitting . \n\n\n\n\nFigure:\nBasic feature selection algorithm for selecting the\n best features.\n\n\n\n\nWe can view feature selection as a method for replacing a\ncomplex classifier (using all features) with a\nsimpler one (using a subset of the\nfeatures). It may appear counterintuitive at first that a\nseemingly weaker classifier is advantageous in statistical text\nclassification,\nbut when discussing \nthe bias-variance tradeoff in\nSection 14.6 (page ), we will see\nthat weaker models are often preferable when limited\ntraining data are available.\n\n\nThe basic feature selection algorithm is shown in\nFigure 13.6 . \nFor a given class ,\nwe compute a utility measure  for each\nterm of the vocabulary and select the  terms that have the highest values of . All\nother terms are discarded and not used in classification. We will\nintroduce three different utility measures in this section: mutual information,\n\n; the  test,\n\n; and frequency,\n\n.\n\n\nOf the two NB models, the Bernoulli model is particularly\nsensitive to noise features. A Bernoulli NB classifier\nrequires some form of feature selection or else its accuracy will\nbe low.\n\n\nThis section mainly addresses feature selection for two-class\nclassification tasks like\nChina versus\nnot-China. Section 13.5.5  briefly discusses\noptimizations for systems with more than two\nclasses.\n\n\n\n\nSubsections\n\nMutual\n  information\n Feature selectionChi2 Feature selection\n\nAssessing\n     as a feature selection\n    methodAssessing chi-square as a feature\n    selection method\n\n\nFrequency-based feature\n  selection\nFeature selection for multiple classifiers\nComparison of feature selection methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Mutual information\n Up: Text classification and Naive\n Previous: A variant of the\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}