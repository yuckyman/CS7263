{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-12.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Text classification and Naive\n Up: Language models for information\n Previous: Extended language modeling approaches\n    Contents \n    Index\n\n\n\n \n\nReferences and further reading\n\n\nFor more details on the basic concepts of probabilistic language\nmodels and techniques for smoothing, see either  \nManning and Schütze (1999, Chapter 6) or Jurafsky and Martin (2008, Chapter 4).\n\n\nThe important initial papers that originated the language modeling\napproach to IR are:\n(Berger and Lafferty, 1999, Ponte and Croft, 1998, Miller et al., 1999, Hiemstra, 1998).\nOther relevant papers can be found in the next several years of SIGIR\nproceedings.  (Croft and Lafferty, 2003) contains a collection of papers from a\nworkshop on\nlanguage modeling approaches and Hiemstra and Kraaij (2005) review one\nprominent thread of work on using language modeling approaches for TREC tasks.\nZhai and Lafferty (2001b) clarify the role of smoothing in LMs for IR\nand present detailed empirical \ncomparisons of different \nsmoothing methods.  Zaragoza et al. (2003) advocate using full\nBayesian predictive distributions rather than MAP point estimates, but\nwhile they outperform Bayesian smoothing, they fail to outperform a\nlinear interpolation.  Zhai and Lafferty (2002) argue that a two-stage\nsmoothing model with first Bayesian smoothing followed by linear\ninterpolation gives a good model of the task, and performs better and\nmore stably than a single form of smoothing.  A nice feature of the LM\napproach is that it provides a convenient and principled way to put\nvarious kinds of prior information into the model; Kraaij et al. (2002)\ndemonstrate this by showing the value of link information as a prior\nin improving web entry page retrieval performance.\nAs briefly discussed in Chapter 16  (page 16.1 ), Liu and Croft (2004) show some gains by smoothing a document LM with\nestimates from a cluster of similar documents; Tao et al. (2006)\nreport larger gains by doing document-similarity based smoothing.\n\n\nHiemstra and Kraaij (2005) present TREC results showing a LM approach\nbeating use of BM25 weights.  \nRecent\nwork has achieved some gains by going beyond the unigram model,\nproviding the higher order models are smoothed with lower order models\n(Cao et al., 2005, Gao et al., 2004), though the gains to date\nremain modest.\nSpärck Jones (2004) presents a critical viewpoint on the\nrationale for the language modeling approach, but\nLafferty and Zhai (2003) argue that a unified account can be\ngiven of the probabilistic semantics underlying both the language\nmodeling approach presented in this chapter and the classical\nprobabilistic information retrieval approach of Chapter 11 .\nThe Lemur Toolkit (http://www.lemurproject.org/) provides a\nflexible open source framework for investigating language modeling\napproaches to IR.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Text classification and Naive\n Up: Language models for information\n Previous: Extended language modeling approaches\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}