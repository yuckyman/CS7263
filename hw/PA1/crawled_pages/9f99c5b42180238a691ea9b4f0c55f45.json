{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-18.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Web search basics\n Up: Matrix decompositions and latent\n Previous: Latent semantic indexing\n    Contents \n    Index\n\n\n\n\n \n\nReferences and further reading\n\n\nStrang (1986) provides an excellent introductory overview\nof matrix decompositions including the singular value\ndecomposition. Theorem 18.3 is due to\nEckart and Young (1936). The connection between information\nretrieval and low-rank approximations of the term-document\nmatrix was introduced in Deerwester et al. (1990), with a subsequent\nsurvey of results in Berry et al. (1995). Dumais (1993)\nand Dumais (1995) describe experiments on TREC benchmarks\ngiving evidence that at least on some benchmarks, LSI can\nproduce better precision and recall than standard\nvector-space\nretrieval. http://www.cs.utk.edu/~berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.htmloffer comprehensive pointers to the literature and software\nof LSI. Schütze and Silverstein (1997) evaluate\nLSI and truncated representations of centroids for efficient\n -means clustering\n(Section 16.4 ).\nBast and Majumdar (2005) detail the role of the reduced\ndimension  in LSI and how different pairs of terms get\ncoalesced together at differing values of .  Applications\nof LSI to  cross-language information retrieval \n(where documents in two or more different languages are\nindexed, and a query posed in one language is expected to\nretrieve documents in other languages) are developed in\nBerry and Young (1995) and Littman et al. (1998).  LSI\n(referred to as LSA in more general settings) has been\napplied to host of other problems in computer science\nranging from memory modeling to computer vision.\n\n\nHofmann (1999a;b) provides an initial\nprobabilistic extension of the basic latent semantic indexing\ntechnique. A more satisfactory formal basis for a probabilistic latent\nvariable model for dimensionality reduction is the  Latent\n  Dirichlet Allocation  ( LDA ) model\n(Blei et al., 2003), which is generative and assigns probabilities to\ndocuments outside of the training set.  This model is extended to a\nhierarchical clustering by Rosen-Zvi et al. (2004).\nWei and Croft (2006) present the first large scale evaluation of LDA,\nfinding it to significantly outperform the query likelihood model of\nSection 12.2 (page ), but to not perform quite as well as the relevance\nmodel mentioned in Section 12.4 (page ) - but the latter does\nadditional per-query processing unlike LDA.  Teh et al. (2006)\ngeneralize further by presenting  Hierarchical Dirichlet\nProcesses , a probabilistic model which allows a group (for us, a\ndocument) to be drawn from an infinite mixture of latent topics, while\nstill allowing these topics to be shared across documents.\n\n\nExercises.\n\nAssume you have a set of documents each of which is in either English or in Spanish. The collection is given in Figure 18.4 .\n\n\n\n\nFigure:\nDocuments for Exercise 18.5.\n\n\n\n\nFigure 18.5 gives a glossary relating the Spanish and English words above for your own information. This glossary is NOT available to the retrieval system:\n\n\n\n\nFigure 18.5:\nGlossary for Exercise 18.5.\n\n\n\n\n\nConstruct the appropriate term-document matrix  to use for a collection consisting of these documents. For simplicity, use raw term frequencies rather than normalized tf-idf weights. Make sure to clearly label the dimensions of your matrix.\n\nWrite down the matrices  and  and from these derive the rank 2 approximation .\n\nState succinctly what the  entry in the matrix \n represents.\n\nState succinctly what the  entry in the matrix \n represents, and why it differs from that in \n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Web search basics\n Up: Matrix decompositions and latent\n Previous: Latent semantic indexing\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}