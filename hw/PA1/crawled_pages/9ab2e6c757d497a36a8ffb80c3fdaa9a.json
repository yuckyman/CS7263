{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/postings-file-compression-1.html",
  "title": "Postings file compression",
  "body": "\n\n\n\n\nPostings file compression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Variable byte codes\n Up: Index compression\n Previous: Blocked storage\n    Contents \n    Index\n\n\n\n\n \n\nPostings file compression\n\n\n\n\n\n\nTable:\nEncoding gaps instead of document IDs. For example,\nwe store \ngaps 107, 5, 43, ..., instead of\ndocIDs 283154, 283159, 283202, ... for\ncomputer.\nThe first docID is left unchanged (only\nshown for arachnocentric).\n\n  \nencoding\npostings list\n \n \n \n \n \n \n \n \n the\ndocIDs\n...\n \n283042\n \n283043\n \n283044\n \n283045\n...\n \n  \ngaps\n \n \n \n1\n \n1\n \n1\n \n...\n \n computer\ndocIDs\n...\n \n283047\n \n283154\n \n283159\n \n283202\n...\n \n  \ngaps\n \n \n \n107\n \n5\n \n43\n \n...\n \n arachnocentric\ndocIDs\n252000\n \n500100\n \n \n \n \n \n \n \n \n  \ngaps\n252000\n248100\n \n \n \n \n \n \n \n \n \n\n \n\n\n\nRecall from Table 4.2  (page 4.2 )\nthat Reuters-RCV1 has 800,000 documents, 200 tokens per\ndocument, six characters per token, and 100,000,000\npostings where we define a posting in this chapter as a docID in a\npostings list, that is, excluding frequency and position\ninformation. \nThese numbers correspond to line 3 (``case\nfolding'') in Table 5.1 .  \nDocument identifiers are\n\n bits long.  \nThus, the size of the\ncollection is about \n and\nthe size of\nthe uncompressed postings file is \n. \n\n\nTo devise a more efficient representation of the postings\nfile, one that uses fewer\nthan 20 bits per document, we observe that\nthe postings for frequent terms are close together.  Imagine\ngoing through the documents of a collection one by one and\nlooking for a frequent term like computer.  We will\nfind a document containing computer, then we skip a\nfew documents that do not contain it, then there is again a\ndocument with the term and so on (see\nTable 5.3 ).  The key idea is that the\ngaps between postings are short, requiring a lot less\nspace than 20 bits to store. In fact, gaps for the most\nfrequent terms such as the and for are mostly\nequal to 1. But the gaps for a rare term that occurs only\nonce or twice in a collection (e.g., arachnocentric\nin Table 5.3 ) have the same order of magnitude\nas the docIDs and need 20 bits.  For an economical\nrepresentation of this distribution of gaps, we need a\nvariable encoding method that uses fewer bits for\nshort gaps.\n\n\nTo encode small\nnumbers in less space than large numbers,\nwe look at two types of methods: bytewise compression and\nbitwise compression. As the names suggest, these methods\nattempt to encode gaps with the minimum number of bytes and\nbits, respectively.\n\n\n \n\nSubsections\n\nVariable byte codes\nGamma codes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Variable byte codes\n Up: Index compression\n Previous: Blocked storage\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}