{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html",
  "title": "Estimating the query generation probability",
  "body": "\n\n\n\n\nEstimating the query generation probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Ponte and Croft's Experiments\n Up: The query likelihood model\n Previous: Using query likelihood language\n    Contents \n    Index\n\n\n\n   \n\nEstimating the query generation probability\n\n\nIn this section we describe how to estimate .  The probability\nof producing the query given the LM  of document  using\n maximum likelihood estimation  ( MLE ) and\nthe unigram assumption is: \n\n\n\n\n\n\n(100)\n\n\nwhere  is the language model of document ,  is the\n(raw) term frequency of term  in document , and  is the\nnumber of tokens in document .  That is, we just count up how often\neach word occurred, and divide through by the total number of words in\nthe document .  This is the same method of calculating an MLE as we\nsaw in Section 11.3.2 (page ), but now using a multinomial over word counts.\n\n\n \nThe classic problem with using language models is one of estimation\n(the \n \nsymbol on the P's is used above to stress that the model is estimated): terms\nappear very  sparsely  in documents.  In particular,\nsome words will not have appeared in the document at all, but are\npossible words for the information need, which the user may have used in\nthe query.  If we estimate \n for a term missing from a\ndocument , then we get a strict conjunctive semantics: documents will\nonly give a query non-zero probability if all of the query terms appear\nin the document.  Zero probabilities are clearly a problem in other\nuses of language models, such as when predicting the next word in a\nspeech recognition application, because many words will be sparsely\nrepresented in the training data.  It may seem rather less clear\nwhether this is problematic in an IR application.  This could be\nthought of as a \nhuman-computer interface issue: vector space systems have generally\npreferred more lenient matching, though recent web search developments\nhave tended more in the direction of doing searches with such\nconjunctive semantics.  Regardless of the approach here, there is a\nmore general problem of estimation: occurring words are also badly\nestimated; in particular, the probability of words occurring once in the\ndocument is normally overestimated, since their one occurrence was\npartly by chance.  The answer to this (as we saw in\nprobtheory) is smoothing.  But as people have come to\nunderstand the LM approach better, it has become apparent that the\nrole of smoothing in this model is not only to avoid zero\nprobabilities.  The smoothing of terms actually implements major parts of\nthe term weighting component (Exercise 12.2.3 ).  It is\nnot just that an unsmoothed model \nhas conjunctive semantics; an unsmoothed model works badly because it\nlacks parts of the term weighting component.\n\n\n \nThus, we need to smooth\nprobabilities in our document language models: to discount non-zero\nprobabilities and to give some \nprobability mass to unseen words.\nThere's a wide space of approaches to smoothing probability\ndistributions to deal with this problem.  In Section 11.3.2 (page ),\nwe already discussed adding a number (1,\n1/2, or a small ) to the observed counts and renormalizing to\ngive a probability distribution.In this section we will mention a\ncouple of other smoothing methods, which involve combining observed counts with a\nmore general reference probability distribution.\nThe general approach is that a non-occurring term should be\npossible in a query, but its probability should be somewhat close to\nbut no more likely than would be expected by\nchance from the whole collection.  That is, if \n then\n\n\n\n\n\n\n(101)\n\n\nwhere  is the raw count of the term in the collection, and  is\nthe raw size (number of tokens) of the entire collection. \nA simple idea that works\nwell in practice is to use a mixture between a document-specific multinomial\ndistribution and a multinomial distribution estimated from the entire collection:\n\n\n\n\n\n\n(102)\n\n\nwhere \n and  is a language model built from the\nentire document collection. This mixes the probability from the\ndocument with the general collection frequency of the word.  \nSuch a model is referred to as a  linear interpolation \nlanguage model.Correctly\nsetting  is important to the good performance of this model. \n\n\nAn alternative is to use a language model built from the whole\ncollection as a prior distribution\nin a  Bayesian updating process \n(rather than a uniform distribution, as we saw in\nSection 11.3.2 ).  We then get the following equation:\n\n\n\n\n\n\n(103)\n\n\n\nBoth of these smoothing methods have been shown to perform well in IR\nexperiments; we will stick with the linear interpolation smoothing\nmethod for the rest of this section.  While different in detail, they\nare both conceptually similar: in both cases the probability estimate for a word\npresent in the document combines a discounted MLE and a fraction of\nthe estimate of \nits prevalence in the whole collection, while for words not present in\na document, the estimate is just a fraction of the estimate of the\nprevalence of the word in the whole collection.\n\n\nThe role of smoothing in LMs for IR is not\nsimply or principally to avoid estimation problems.  This was not\nclear when the models were first proposed, but it is now understood that\nsmoothing is essential to the good\nproperties of the models.  The reason for this is explored in\nExercise 12.2.3 .  The extent of smoothing in these two\nmodels is controlled by the  and  parameters: a small\nvalue of  or a large value of  means more smoothing.\nThis parameter can be tuned to optimize performance using\na line search (or, for the linear interpolation\nmodel, by other methods, such as the expectation maximimization algorithm; see\nmodelclustering).  The value need not be a \nconstant.  One approach is to make the value a function of the query size.\nThis is useful because a small amount of smoothing (a\n``conjunctive-like'' search) is more suitable for short queries, while\na lot of smoothing is more suitable for long queries.\n\n\nTo summarize, the retrieval ranking for a query  under the basic LM\nfor IR we have been considering is given by:\n\n\n\n\n\n\n(104)\n\n\nThis equation captures the probability that the document that the user\nhad in mind was in fact .\n\n\nWorked example.\nSuppose the document collection contains two documents:\n\n\n: Xyzzy reports a profit but revenue is down\n\n: Quorus narrows quarter loss but revenue decreases further\n\n\nThe model will be MLE unigram models from the documents and collection,\nmixed with .\n\n\nSuppose the query is revenue down.  Then:\n\n\n\n\n\n\n\n\n(105)\n \n\n\n\n(106)\n\n\n\n\n(107)\n \n\n\n\n(108)\n\n\nSo, the ranking is .\nEnd worked example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Ponte and Croft's Experiments\n Up: The query likelihood model\n Previous: Using query likelihood language\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}