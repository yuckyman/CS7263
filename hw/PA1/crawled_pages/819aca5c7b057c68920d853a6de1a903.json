{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/extended-language-modeling-approaches-1.html",
  "title": "Extended language modeling approaches",
  "body": "\n\n\n\n\nExtended language modeling approaches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Language models for information\n Previous: Language modeling versus other\n    Contents \n    Index\n\n\n\n \n\nExtended language modeling approaches\n\n\nIn this section we briefly mention some of the work that extends the\nbasic language modeling approach. \n\n\nThere are other ways to think of using the language modeling\nidea in IR settings, and many of them have been tried in subsequent\nwork.  Rather than looking at the probability of a document language\nmodel  generating the query, you can look at the probability of a query\nlanguage model  generating the document.  The main reason that doing\nthings in this direction and creating a  document likelihood\n  model  is less appealing is that there is much less\ntext available to estimate a language model based on the query text,\nand so the model will \nbe worse estimated, and will have to depend more on being smoothed with\nsome other language model.  On the other hand, it is easy to see how to\nincorporate relevance feedback into such a model: you can expand the\nquery with terms taken from relevant documents in the usual way and\nhence update the language model  (Zhai and Lafferty, 2001a).  Indeed,\nwith appropriate modeling choices, this approach leads to the BIM model\nof Chapter 11 .  The relevance model of\nLavrenko and Croft (2001) is an instance of a document likelihood\nmodel, which incorporates \npseudo-relevance feedback into a language modeling approach.  It\nachieves very strong empirical results.\n\n\n\n\nFigure 12.5:\nThree ways of developing the language modeling approach: (a) query\n  likelihood, (b) document likelihood, and (c) model\n  comparison.\n\n\n\n\nRather than directly generating in either direction, we can make a\nlanguage model from both the document and query, and then ask how\ndifferent these two language models are from each other.\nLafferty and Zhai (2001) lay out these three ways of thinking about the problem,\nwhich we show in Figure 12.5 , and\ndevelop a general risk minimization approach for document retrieval.\nFor instance, one way to model the risk of returning a document  as\nrelevant to a query  is to use the \n  Kullback-Leibler (KL) divergence \nbetween their respective language models:\n\n\n\n\n\n\n(109)\n\n\nKL divergence is an asymmetric divergence measure originating in\ninformation theory, which measures\nhow bad the probability distribution  is at modeling \n(Manning and Schütze, 1999, Cover and Thomas, 1991). \nLafferty and Zhai (2001) present results suggesting that a model comparison\napproach outperforms both query-likelihood and document-likelihood\napproaches.  One disadvantage of using KL divergence as a ranking\nfunction is that scores are not comparable across queries.  This does\nnot matter for ad hoc retrieval, but is important in other\napplications such as topic tracking.  Kraaij and Spitters (2003) suggest\nan alternative proposal which models similarity as a normalized\nlog-likelihood ratio (or, equivalently, as a difference between\ncross-entropies).\n\n\nBasic LMs do not address issues of alternate expression, that is,\nsynonymy, or any deviation in use of language between queries and\ndocuments.  Berger and Lafferty (1999) introduce translation models to bridge this\nquery-document gap.  A  translation model  lets you generate query words\nnot in a document by translation to alternate terms with similar\nmeaning.  This also provides a basis for performing cross-language IR.\nWe assume that the translation model can be represented by a\nconditional probability distribution \n between\nvocabulary terms.  The form of the translation query generation model\nis then:\n\n\n\n\n\n\n(110)\n\n\nThe term \nis the basic document language\nmodel, and the term  performs translation.  This model is clearly\nmore computationally intensive and we need to build a translation\nmodel.  The translation model is usually built using separate resources \n(such as a traditional thesaurus or bilingual dictionary or a\nstatistical machine translation system's translation dictionary), but\ncan be built using the document collection if there are pieces of text\nthat naturally paraphrase or summarize other pieces of text.\nCandidate examples are documents and their titles or abstracts, or\ndocuments and anchor-text pointing to them in a hypertext environment.\n\n\nBuilding extended LM approaches remains an active area of research.   \nIn general, translation\nmodels, relevance feedback models, and model comparison approaches\nhave all been\ndemonstrated to improve performance over the basic query likelihood LM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Language models for information\n Previous: Language modeling versus other\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}