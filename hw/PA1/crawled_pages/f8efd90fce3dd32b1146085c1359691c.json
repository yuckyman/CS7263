{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/flat-clustering-1.html",
  "title": "Flat clustering",
  "body": "\n\n\n\n\nFlat clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Clustering in information retrieval\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n\n\nFlat clustering\n\n\nClustering algorithms group a set of documents into\nsubsets or  clusters . The\nalgorithms' goal is to create clusters that are coherent internally, but\nclearly different from each other. In other words, documents\nwithin a cluster should be as similar as possible; and\ndocuments in one cluster should be as dissimilar as possible\nfrom documents in other clusters.\n\n\n\n\nFigure 16.1:\nAn example of a data set with a clear cluster\nstructure.\n\n\n\n\nClustering is the most common form of  unsupervised\nlearning .  No supervision means that there is no\nhuman\nexpert who has assigned documents to classes.  In\nclustering, it is the distribution and makeup of the data \nthat will determine cluster membership. A simple example is\nFigure 16.1 . It is visually clear that there\nare three distinct clusters of points. This chapter and\nChapter 17  introduce algorithms that find such clusters\nin an unsupervised fashion.\n\n\nThe difference between clustering and classification may not\nseem great at first. After all, in both cases we have a\npartition of a set of documents into groups.  But as we will\nsee the two problems are fundamentally different. \nClassification is a form of supervised learning\n(Chapter 13 ,\npage 13.1 ): our goal is to replicate a\ncategorical distinction that a human supervisor imposes on\nthe data.  In unsupervised learning, of which clustering is\nthe most important example, we have no such teacher to guide us.\n\n\nThe key input to a clustering algorithm is the distance\nmeasure.  In Figure 16.1 , the distance measure\nis distance in the 2D plane. This measure suggests three\ndifferent clusters in the figure.  In document clustering,\nthe distance measure is often also Euclidean distance.\nDifferent distance measures give rise to different\nclusterings. Thus, the distance measure is an important\nmeans by which we can influence the outcome of clustering.\n\n\n Flat clustering \ncreates a flat set of clusters without any explicit\nstructure that would relate clusters to each other.\n Hierarchical clustering \ncreates a hierarchy of clusters and will be covered in\nChapter 17 . Chapter 17  also addresses the\ndifficult problem of labeling clusters automatically.\n\n\nA second important distinction can be made between\nhard and soft clustering\nalgorithms. \n Hard clustering \ncomputes a \n hard assignment \n- each document is a member of exactly one \ncluster. The assignment of \n soft clustering algorithms \nis\n soft  - a document's assignment is a distribution over all\nclusters. In a soft assignment, a document has fractional membership in several\nclusters. Latent semantic indexing, a form of dimensionality\nreduction, is a soft clustering algorithm\n(Chapter 18 , page 18.4 ). \n\n\nThis chapter motivates the use of clustering in information\nretrieval by introducing a number of applications\n(Section 16.1 ), defines the problem we are trying\nto solve in clustering (Section 16.2 ) and\ndiscusses measures for evaluating cluster quality\n(Section 16.3 ).  It then describes two flat\nclustering algorithms,  -means (Section 16.4 ),\na hard clustering algorithm, and the\nExpectation-Maximization (or EM) algorithm\n(Section 16.5 ), a soft clustering algorithm.\n -means is perhaps the most widely used flat\nclustering algorithm due to its simplicity and\nefficiency. The EM algorithm is a generalization of\n -means and can be applied to a large variety of\ndocument representations and distributions.\n\n\n\n\nSubsections\n\nClustering in information retrieval\nProblem statement\n\nA note on terminology.\nCardinality - the number of clusters\n\n\nEvaluation of clustering\nK-means\n\nCluster cardinality in K-means\n\n\nModel-based clustering\nReferences and further reading\nExercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Clustering in information retrieval\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}