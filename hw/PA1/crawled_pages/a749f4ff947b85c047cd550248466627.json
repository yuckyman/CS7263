{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html",
  "title": "Hierarchical agglomerative clustering",
  "body": "\n\n\n\n\nHierarchical agglomerative clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Single-link and complete-link clustering\n Up: Hierarchical clustering\n Previous: Hierarchical clustering\n    Contents \n    Index\n\n\n\n\n \n\nHierarchical agglomerative clustering\n \nHierarchical clustering algorithms are either top-down or\nbottom-up. Bottom-up algorithms \ntreat  each document\nas a singleton cluster at the outset and then\nsuccessively merge (or agglomerate)\n pairs of clusters until all clusters have been\nmerged into a single cluster that contains all documents.\nBottom-up hierarchical clustering is therefore called \n hierarchical agglomerative clustering \nor  HAC . Top-down clustering requires a method for\nsplitting a cluster. It proceeds by splitting clusters\nrecursively until individual documents are reached.\nSee Section 17.6 . HAC is\nmore frequently used in IR than top-down clustering and is the main subject of this\nchapter.\n\n\n\n\nA dendrogram\nof a \nsingle-link clustering of 30 documents from Reuters-RCV1.\nTwo possible cuts of the dendrogram are shown: at 0.4 into 24\nclusters and at 0.1 into 12 clusters. \n\n\n\n\nBefore looking at specific similarity measures used in\nHAC in Sections 17.2 -17.4 , we first introduce\na method for depicting hierarchical clusterings\ngraphically,\ndiscuss a\nfew key properties of HACs and present a simple algorithm\nfor computing an HAC.\n\n\nAn HAC clustering is typically\nvisualized as a  dendrogram  as shown in\nFigure 17.1 . Each merge is represented by a horizontal line. \nThe y-coordinate of the horizontal line is the \nsimilarity  of the two clusters that were\nmerged, \nwhere documents are viewed as  singleton clusters.\nWe call this similarity\nthe \n combination similarity  of the merged cluster.\nFor example, the combination similarity\nof the cluster consisting of Lloyd's CEO questioned and\nLloyd's chief / U.S. grilling in Figure 17.1  is .\nWe define the combination similarity of a\nsingleton cluster as its document's self-similarity\n(which is 1.0 for cosine similarity).  \n\n\nBy moving up from the\nbottom layer to the top node, a dendrogram allows  us to reconstruct the history\nof merges that resulted in the depicted clustering. For\nexample, we see that the two documents entitled War hero\n  Colin Powell were merged first in Figure 17.1  and that the last merge\nadded Ag trade reform to a cluster consisting of the\nother 29 documents.\n\n\nA fundamental assumption in HAC is that the merge operation\nis   monotonic . Monotonic means that if\n\n are the combination\nsimilarities of the successive merges of an HAC, then \n holds.  A non-monotonic hierarchical clustering\ncontains at least one  inversion  and\ncontradicts the fundamental assumption that we \n\n\nchose the\nbest merge available at each step. We will see an example of\nan inversion in Figure 17.12 .\n\n\nHierarchical clustering does not require a prespecified\nnumber of clusters.  However, in some applications we want a\npartition of disjoint clusters just as in flat\nclustering. In those cases, the hierarchy needs to be cut at\nsome point.  A number of criteria can be used to determine\nthe cutting point:\n\n\nCut at a prespecified level of similarity. For\nexample, we cut the dendrogram at 0.4 if we want clusters with a\nminimum combination similarity of 0.4. \nIn Figure 17.1 , cutting the\ndiagram at  yields 24 clusters (grouping only\ndocuments with high similarity together) and cutting it at\n\nyields 12 clusters (one large financial news cluster and 11\nsmaller clusters).\n\n\n\nCut the dendrogram where the gap between two\nsuccessive combination similarities is largest. Such large gaps\narguably indicate ``natural'' clusterings. Adding one more\ncluster decreases the quality of the clustering\nsignificantly, so cutting before this steep decrease\noccurs is desirable. This strategy is\nanalogous to looking for the knee in the  -means graph in\nFigure 16.8  (page 16.8 ).\n\nApply Equation 195 (page 16.4.1 ):\n\n\n\n\n\nwhere  refers to the cut of the hierarchy that results in  clusters,\nRSS is the residual sum of squares and  is a\npenalty for each additional cluster. Instead of RSS, another\nmeasure of distortion can be used.\n\nAs in flat clustering, we can also prespecify\nthe number of clusters  and \nselect the cutting point that produces  clusters.\n\n\n\n\n\nFigure 17.2:\nA simple, but inefficient HAC algorithm.\n\n\n\n\n\n\n\n\nA simple, naive HAC algorithm is shown in\nFigure 17.2 . \nWe first compute the\n similarity matrix .\nThe algorithm then executes\n steps of merging the currently most\nsimilar clusters. \nIn each iteration,\nthe two most similar clusters are merged and the rows and columns of the\nmerged cluster  in  are updated.The clustering is stored as a list of merges in .\n indicates which clusters are still available to be\nmerged.  The function\nSIM computes the similarity of cluster  with \nthe merge of clusters  and . For some HAC algorithms,\nSIM is simply a function of  and\n, for example,\nthe maximum of these two values for single-link.\n\n\nWe will now refine this algorithm for\nthe different similarity measures of\nsingle-link and\ncomplete-link clustering \n(Section 17.2 )\nand\ngroup-average and\ncentroid clustering ( and 17.4 ).\nThe merge criteria of these four variants of HAC are shown in\nFigure 17.3 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Single-link and complete-link clustering\n Up: Hierarchical clustering\n Previous: Hierarchical clustering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}