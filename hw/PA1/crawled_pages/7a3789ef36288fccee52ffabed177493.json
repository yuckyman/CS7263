{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/group-average-agglomerative-clustering-1.html",
  "title": "Group-average agglomerative clustering",
  "body": "\n\n\n\n\nGroup-average agglomerative clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Centroid clustering\n Up: Hierarchical clustering\n Previous: Time complexity of HAC\n    Contents \n    Index\n\n\n\n\n \n\n\nGroup-average agglomerative clustering\n Group-average agglomerative clustering  or \n GAAC \n(see Figure 17.3 , (d))\nevaluates cluster quality based on\nall similarities between documents, thus avoiding the\npitfalls of the single-link and complete-link criteria,\nwhich equate cluster similarity with the similarity of a\nsingle pair of documents.\nGAAC is also called\n group-average\nclustering \nand\n average-link clustering .\nGAAC computes the average \nsimilarity SIM-GA of all pairs of documents, including pairs from\nthe same cluster. But self-similarities are not included in the average:\n\n\n\n\n\n\n(203)\n\n\nwhere  is the length-normalized vector of document ,\n denotes the dot product, and  and \nare the number of documents in  and , respectively.\n\n\nThe motivation for GAAC is that our goal in selecting two\nclusters  and  as the next merge in HAC\nis that the resulting merge cluster \n should be\ncoherent. To judge the coherence of  ,\nwe need to look at all document-document similarities within ,\nincluding those that occur within  and those that\noccur within .\n\n\nWe can compute\nthe measure SIM-GA\nefficiently because the\nsum of individual vector similarities is equal to the\nsimilarities of their sums:\n\n\n\n\n\n \n \n\n(204)\n\n\nWith gatrick,\nwe have:\n\n\n\n\n\n\n\n\n(205)\n\n\nThe term  on the right is the sum of \n\nself-similarities of value . With this trick we can\ncompute cluster similarity in constant time (assuming we\nhave available the two vector sums\n\n and\n\n\ninstead of in \n.\nThis is important because we need to be able to compute the\nfunction \nSIM\non lines 18 and 20\nin\nEFFICIENTHAC\n(Figure 17.8 )\nin constant time for efficient implementations of GAAC.\nNote that for\ntwo singleton clusters, \nEquation 205 is\nequivalent to the dot product.\n\n\n \nEquation 204  relies on the distributivity of the dot product\nwith respect to vector addition.\nSince this is crucial for the efficient computation of a\nGAAC clustering, the method cannot be easily applied\nto \nrepresentations of documents that are not real-valued vectors.\nAlso, Equation 204 only holds for the dot\nproduct. While many algorithms introduced in this book have near-equivalent\ndescriptions in terms of dot product, cosine similarity and\nEuclidean distance (cf. simdisfigs),  Equation 204 can only be expressed using\nthe dot product. This is a fundamental difference between\nsingle-link/complete-link clustering and GAAC. The first two\nonly require a square matrix of similarities as input and\ndo not care how these similarities were computed. \n\n\nTo\nsummarize, GAAC\nrequires (i) documents represented as vectors, (ii) length\nnormalization of vectors, so that self-similarities are 1.0,\nand (iii) \nthe dot product as the measure of similarity between\nvectors and sums of vectors.\n\n\nThe merge algorithms for GAAC\nand complete-link clustering are the same except that we\nuse \nEquation 205\nas\nsimilarity function \nin\nFigure 17.8 . Therefore, the overall time complexity of\nGAAC is the same as for complete-link\nclustering: \n.\nLike complete-link clustering, GAAC is\nnot best-merge persistent\n(Exercise 17.10 ).\nThis means that\nthere\nis no  algorithm for GAAC\nthat would be analogous to the  algorithm\nfor single-link in Figure 17.9 .\n\n\nWe can also define group-average similarity\nas including self-similarities:\n\n\n\n\n\n \n \n\n(206)\n\n\nwhere the centroid \n is defined as in \nEquation 139 (page 139 ).\nThis definition is equivalent to the intuitive definition of\ncluster quality as average similarity of documents \n to\nthe cluster's centroid .\n\n\nSelf-similarities are always\nequal to 1.0, the maximum possible value for length-normalized vectors.\nThe\nproportion of self-similarities in\nEquation 206 is  for a cluster of size . \nThis gives an unfair advantage to small clusters since they\nwill have proportionally more self-similarities. \nFor two documents ,  with a similarity , \nwe have \n. In contrast, \n\n. This\nsimilarity \n\n of two documents\nis \nthe same as in single-link,\ncomplete-link and centroid\nclustering. We prefer the definition in\nEquation 205, which excludes self-similarities\nfrom the average, because we do not want to penalize large\nclusters for their smaller proportion of self-similarities\nand because we want a consistent similarity value  for\ndocument pairs in all four HAC algorithms.\n\n\nExercises.\n\nApply group-average clustering to the points in\n and 17.7 . Map them onto the surface of the unit sphere in \na three-dimensional space to get length-normalized vectors.\nIs the group-average clustering different from the single-link and\ncomplete-link clusterings?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Centroid clustering\n Up: Hierarchical clustering\n Previous: Time complexity of HAC\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}