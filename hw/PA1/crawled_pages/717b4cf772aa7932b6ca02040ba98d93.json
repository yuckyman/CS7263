{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/finite-automata-and-language-models-1.html",
  "title": "Finite automata and language models",
  "body": "\n\n\n\n\nFinite automata and language models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Types of language models\n Up: Language models\n Previous: Language models\n    Contents \n    Index\n\n\n\n\nFinite automata and language models\n\n\n\n\n\n\n \nWhat do we mean by a document model generating a query?  A traditional\n generative model  of a language, of the kind familiar from formal\nlanguage theory, can be used either to recognize or to generate strings.\nFor example, the finite automaton shown in Figure 12.1  can generate\nstrings that include the examples shown.  The full set of strings that\ncan be generated is called the  language  of the\nautomaton.\n\n\n\n\n\nIf instead each node has a probability distribution over generating\ndifferent terms, we have a language model.  The notion of a language model is inherently probabilistic.  A  language model \nis a function that puts a \nprobability measure over strings drawn from some vocabulary.  That is, for \na language model  over an alphabet :\n\n\n\n\n\n\n(90)\n\n\nOne simple kind of language model is equivalent to a probabilistic\nfinite automaton consisting of just a single node\nwith a single probability distribution over producing different terms,\nso that \n, as\nshown in Figure 12.2 .  After generating each word, we decide whether\nto stop or to loop around and then produce another word, and so the\nmodel also requires a probability of stopping in the\nfinishing state.  Such a model places a probability distribution\nover any sequence of words.  By construction, it also provides a model\nfor generating text according to its distribution.  \n\n\nWorked example. To find the\nprobability of a word sequence, we just multiply the probabilities\nwhich the model gives to each word in the sequence, together with the\nprobability of continuing or stopping after producing each word.  For example,\n\n\n\n\n\n\n\n\n(91)\n \n \n\n\n(92)\n \n\n\n\n(93)\n\n\nAs you can see, the probability of a particular string/document, is usually a\nvery small number!  Here we stopped after generating frog the\nsecond time. The\nfirst line of numbers are the term emission probabilities, and the\nsecond line gives the probability of continuing or stopping after\ngenerating each word.  An explicit\nstop probability is needed for a finite automaton to be a well-formed\nlanguage model according to Equation 90. Nevertheless, most\nof the time, we will omit to include STOP and \n probabilities (as do most\nother authors).  To compare two models for a data set, we can\ncalculate their  likelihood ratio , which results from simply dividing\nthe probability of the data according to one model by the probability\nof the data according to the other model. \nProviding that the stop probability is\nfixed, its inclusion will not alter the likelihood ratio that results\nfrom comparing the likelihood of two language models generating\na string.  Hence, it will not alter the ranking of documents. \nNevertheless, formally, the numbers will no longer truly be\nprobabilities, but only proportional to probabilities.  See\nExercise 12.1.3 . \nEnd worked example.\n\n\n\nFigure 12.3:\nPartial specification of two unigram language\n  models.\n\n\n\n\nWorked example. Suppose, now, that we have two language models  and , shown\npartially in Figure 12.3 .  Each gives a probability estimate to a\nsequence of terms, as already illustrated  in m1probability.\nThe language model that\ngives the higher probability to the sequence of terms is more likely to\nhave generated the term sequence.  This time, we will omit\nSTOP probabilities from our calculations.  \nFor the sequence shown, we get:\n\n\n\nand we see that \n.\nWe present the formulas here in terms of products of probabilities,\nbut, as is common in probabilistic applications, in practice it is\nusually best to work with sums of log probabilities (cf. page 13.2 ).\nEnd worked example.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Types of language models\n Up: Language models\n Previous: Language models\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}