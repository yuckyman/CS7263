{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/distributed-indexing-1.html",
  "title": "Distributed indexing",
  "body": "\n\n\n\n\nDistributed indexing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Dynamic indexing\n Up: Index construction\n Previous: Single-pass in-memory indexing\n    Contents \n    Index\n\n\n\n\n \n\nDistributed indexing\n \nCollections are often so large that we cannot perform index\nconstruction efficiently on a single machine.\nThis is particularly true\nof the World Wide Web for which we need large computer\n clusters to construct any reasonably sized web index. Web\nsearch engines, therefore, use  distributed indexing  algorithms for index\nconstruction.  The result of the construction process is\na  distributed index  that is partitioned across several machines -\neither according to term or according to document. In this\nsection, we describe distributed indexing for a\n\n\n term-partitioned index . Most large search engines prefer a\n document-partitioned index  (which can be easily generated\nfrom a term-partitioned index).\nWe discuss this topic further in\nSection 20.3 (page ).\n\n\nThe distributed index construction method we describe in\nthis section is an application of  MapReduce , a\ngeneral architecture for distributed computing.  MapReduce\nis designed for large computer clusters.  The point of a\ncluster is to solve large computing problems on cheap\ncommodity machines or nodes \nthat are built from standard parts (processor, memory,\ndisk) \nas opposed to on\na supercomputer with specialized hardware.  Although hundreds\nor thousands of machines are available in such clusters,\nindividual machines can fail at any time. One requirement\nfor robust distributed indexing is, therefore, that we divide\nthe work up into chunks that we can easily assign and -\n in\ncase of failure - reassign.  A  master node  directs the process\nof assigning and reassigning tasks to individual worker\nnodes.\n\n\nThe map and reduce phases of MapReduce split\nup the computing job into chunks that\nstandard machines can process in a short time.  The various\nsteps of MapReduce are shown in\nFigure 4.5  and an example on a collection consisting\nof two\ndocuments is shown in Figure 4.6 .  First, the\ninput data, in our case a collection of web pages, are split\ninto   splits  where the size of the split is\nchosen to ensure that the work can be distributed evenly\n(chunks should not be too large) and efficiently (the total\nnumber of chunks we need to manage should not be\ntoo large); 16 or 64 MB are good\nsizes in distributed indexing. Splits are\nnot preassigned to machines, but are instead assigned by the\nmaster node on an ongoing basis: As a machine finishes\nprocessing one split, it is assigned the next one.  If a\nmachine dies or becomes a laggard due to hardware problems,\nthe split it is working on is simply reassigned to another\nmachine.\n\n\n\n\nFigure 4.5:\n An example of distributed indexing with MapReduce.\nAdapted from Dean and Ghemawat (2004).\n\n\n\n\nIn general, MapReduce breaks a large computing problem into\nsmaller parts by recasting it in terms of manipulation of\n key-value pairs . For indexing, a key-value pair has\nthe form (termID,docID).\nIn distributed indexing, the mapping\nfrom terms to termIDs is also distributed and therefore more\ncomplex than in single-machine indexing. \nA simple solution is to maintain a (perhaps precomputed) mapping for frequent\nterms that is copied to all nodes and to use terms directly\n(instead of termIDs) for infrequent terms.\nWe do not address\nthis problem here and assume that all nodes share a\nconsistent term  termID mapping. \n\n\n The  map phase  of MapReduce consists of\nmapping splits of the input data to key-value pairs.  This\nis the same parsing task we also encountered in\nBSBI and SPIMI,\nand we therefore call the\nmachines that execute the map phase  parsers .  Each parser writes its output to\nlocal intermediate files, the  segment files  (shown as \n\n\n\n in\nFigure 4.5 ).\n\n\nFor the  reduce phase , we want all values for a given key to\nbe stored close together, so that they can be read and\nprocessed quickly. This is achieved by partitioning the keys\ninto  term partitions and having the parsers write key-value\npairs for each term partition into a separate segment file. In\nFigure 4.5 , the term partitions are according to first letter:\na-f, g-p, q-z,  and . (We chose these\nkey ranges for ease of exposition. In general, key ranges\nneed not correspond to contiguous terms or termIDs.) The\nterm partitions are defined by the\nperson who operates the indexing system\n(Exercise 4.6 ). The parsers then\nwrite corresponding segment files, one for each\nterm partition. Each term partition thus corresponds to \nsegments files, where  is the number of parsers. For\ninstance, \nFigure 4.5  shows three a-f segment files of the a-f\npartition, corresponding to the three parsers shown in the figure.\n\n\nCollecting all values (here: docIDs) for a given key (here:\ntermID) into one list is the task of the \n inverters  in the reduce phase. The master assigns each term partition to a\ndifferent inverter - and, as in the case of parsers,\nreassigns term partitions in case of failing or slow inverters.\nEach term partition (corresponding to  segment files, one on\neach parser) is processed by one inverter.  \nWe assume here that segment files are of a size that a\nsingle machine can handle (Exercise 4.6 ). Finally, the\nlist of values is sorted for each key and written to the\nfinal sorted postings list (``postings'' in the figure).\n(Note that postings in Figure 4.6  include term frequencies, whereas\neach posting in the other sections of this chapter is simply\na\ndocID without term frequency information.)\nThe data flow is shown for a-f in Figure 4.5 .\nThis completes the construction of the inverted index.\n\n\nParsers and inverters are not\nseparate sets of machines. The master identifies idle\nmachines and assigns tasks to them. The same machine\ncan be a parser in the map phase and an inverter in the\nreduce phase. And there are often other jobs that run in\nparallel \nwith index construction, so in between being a parser and an\ninverter a machine might do some crawling or another\nunrelated task.\n\n\nTo minimize\nwrite times before inverters reduce the data, each parser writes\nits segment files to its local disk. In the reduce phase, the master communicates\nto an inverter the locations of \nthe relevant segment files (e.g., of the  segment files\nof the a-f partition).\nEach segment file\nonly requires one sequential read because all data\nrelevant to a particular inverter were written to a single\nsegment file by the parser.  This setup minimizes\nthe amount of network traffic needed during indexing.\n\n\n\n\n\n\nMap and reduce functions in MapReduce. In\ngeneral, the map\nfunction produces a list of key-value pairs. All values\nfor a key are collected into one list in the reduce\nphase. This list is then processed further.\nThe instantiations of the two functions and an\nexample \nare shown for index construction. Because the map phase\nprocesses documents in a \ndistributed fashion, termID-docID pairs need not be ordered correctly\ninitially as in this example. \nThe example shows terms instead of termIDs\nfor better readability.\nWe abbreviate Caesar\nas C and\nconquered\nas c'ed. \n\n\n\n\nFigure 4.6  shows the general schema of the\nMapReduce functions. Input and output are often lists of\nkey-value pairs themselves, so that several MapReduce jobs\ncan run in sequence. In fact, this was the design of the Google\nindexing system in 2004. What we describe in this section\ncorresponds to only one of five to ten MapReduce operations in that\nindexing system. Another MapReduce operation transforms the\nterm-partitioned index we just created into a document-partitioned\none.\n\nMapReduce offers a robust and conceptually simple framework\nfor implementing index construction in a distributed\nenvironment. By providing a semiautomatic method for\nsplitting index construction into smaller tasks, it can\nscale to almost arbitrarily large collections, given computer\nclusters of sufficient size.  \n\n\nExercises.\n\nFor  splits,  segments, and\n term partitions, how long would distributed index\ncreation take for Reuters-RCV1 in a MapReduce architecture?\nBase your assumptions about cluster machines on\nTable 4.1 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Dynamic indexing\n Up: Index construction\n Previous: Single-pass in-memory indexing\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}