{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/nonlinear-svms-1.html",
  "title": "Nonlinear SVMs",
  "body": "\n\n\n\n\nNonlinear SVMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Experimental results\n Up: Extensions to the SVM\n Previous: Multiclass SVMs\n    Contents \n    Index\n\n\n\n \n\nNonlinear SVMs\n\n\n\n\nFigure 15.6:\nProjecting data that is not linearly separable into a higher\n  dimensional space can make it linearly separable.\n\n\n\n\nWith what we have presented so far, data sets that are linearly\nseparable (perhaps with a few exceptions or some noise) are\nwell-handled.  But what are we going to do if the data set \njust doesn't allow classification by a\nlinear classifier?  Let us look at a one-dimensional case.\nThe top data set in Figure 15.6  is\nstraightforwardly classified by a linear classifier but the middle data\nset is not.  We instead need to be able to pick out an interval.  One way\nto solve this problem is to map the data on to a higher dimensional\nspace and then to use a linear classifier in the higher dimensional\nspace.  For example, the bottom part of the figure shows that a linear\nseparator can easily classify the data if we use a quadratic function to\nmap the data into two dimensions (a polar coordinates projection would\nbe another possibility).  \nThe general idea is to map the original feature\nspace to some higher-dimensional feature space\nwhere the training set is separable.  \nOf course, we would want to\ndo so in ways that preserve relevant dimensions of relatedness between data points,\nso that the resultant classifier should still generalize well.  \n\n\nSVMs, and also a number of other linear classifiers, provide an easy and\nefficient way of doing this mapping to a higher dimensional space, which\nis referred to as ``the  kernel trick ''.  It's not really a\ntrick: it just exploits the math that we have seen.  The SVM linear\nclassifier relies on a dot product between data point vectors.  Let\n\n.  Then the\nclassifier we have seen so far is:\n\n\n\n\n\n\n(172)\n\n\nNow suppose we decide to map every data point into a\nhigher dimensional space via some transformation \n.  Then the dot product becomes \n.\nIf it turned out that this dot product (which is just a real number)\ncould be computed simply and efficiently in terms of the original data\npoints, then we wouldn't have to actually map from \n.\nRather, we could simply compute the quantity \n, and then use the function's\nvalue in Equation 172. \nA  kernel function  is such a function that corresponds to a dot\nproduct in some expanded feature space.\n\n\nWorked example. The quadratic kernel in two dimensions.quad-kernel\nFor 2-dimensional vectors \n,\n\n, consider\n\n.  We wish to show that this is a\nkernel, i.e., that \n for some\n.  Consider \n.  Then:\n\n\n\n\n\n\n\n\n(173)\n \n\n\n\n(174)\n \n\n\n\n(175)\n \n\n\n\n(176)\n\n\nEnd worked example.\n\nIn the language of functional analysis, what kinds of functions are\nvalid  kernel functions ?  Kernel functions are\nsometimes more precisely referred to as  Mercer\nkernels , because they must satisfy Mercer's\ncondition: for any  such that \n\nis finite, we must have that:\n\n\n\n\n\n\n(177)\n\n\nA kernel function  must be continuous, symmetric, and have a\npositive definite gram matrix.  Such a  means that there exists a\nmapping to a reproducing kernel Hilbert space (a Hilbert space is a\nvector space closed under dot \nproducts) such that the dot product there gives the same value as the\nfunction .  \nIf a kernel does not satisfy\nMercer's condition, then the corresponding QP may have no solution.\nIf you would like to better understand these issues, you should consult the\nbooks on SVMs mentioned in Section 15.5 . Otherwise, \nyou can content yourself with knowing that 90% of work with\nkernels uses one of two straightforward families of functions of two\nvectors, which we define below, and which define valid kernels. \n\n\nThe two commonly used families of kernels are polynomial kernels and radial\nbasis functions.  Polynomial kernels are of the form \n.  The case of  is a linear kernel, which is\nwhat we had before the start of this section (the constant 1 just\nchanging the threshold).  The case of  gives a quadratic kernel,\nand is very commonly used.  We illustrated the quadratic kernel in quad-kernel.\n\n\nThe most common form of radial basis function is a Gaussian\ndistribution, calculated as: \n\n\n\n\n\n\n(178)\n\n\nA radial basis function (rbf) is equivalent to mapping the data into an\ninfinite dimensional Hilbert space, and so we cannot illustrate the\nradial basis function concretely, as we did a quadratic kernel.  \nBeyond these two families, there has been interesting work developing other\nkernels, some of which is \npromising for text applications.  In particular, there has been\ninvestigation of string kernels (see Section 15.5 ). \n\n\nThe world of SVMs comes with its own language, which is rather different\nfrom the language otherwise used in machine learning.  The terminology\ndoes have deep roots in mathematics, but\nit's important not to be too awed\nby that terminology.  Really, we are talking about some quite\nsimple things.  A polynomial kernel allows us to model feature\nconjunctions (up to the order of the polynomial).  That is, if we\nwant to be able to model occurrences of pairs of words, which give\ndistinctive information about topic classification, not given by the\nindividual words alone, like perhaps\noperating and system or ethnic and\ncleansing, then we need to use a quadratic kernel.  If\noccurrences of triples of words give distinctive information, then we\nneed to use a cubic kernel.\nSimultaneously you\nalso get the powers of the basic features - for most text applications,\nthat probably isn't useful, but just comes along with the math and\nhopefully doesn't do harm.  A radial basis function allows you to have\nfeatures that pick out circles (hyperspheres) - although the decision\nboundaries become much more complex as multiple such features interact.\nA string kernel lets you have features that are character\nsubsequences of terms.  All of these are straightforward notions which\nhave also been used in many other places under different names.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Experimental results\n Up: Extensions to the SVM\n Previous: Multiclass SVMs\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}