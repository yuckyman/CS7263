{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/gamma-codes-1.html",
  "title": "Gamma codes",
  "body": "\n\n\n\n\nGamma codes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Postings file compression\n Previous: Variable byte codes\n    Contents \n    Index\n\n\n\n \n\nGamma codes\n\n\n\n\n\n\nTable 5.5:\nSome examples of unary and  codes.\nUnary codes are only shown for the smaller numbers.\nCommas in  codes are for readability only and are not part of the actual codes.\n number\nunary code\nlength\noffset\n code\n \n 0\n0\n \n \n \n \n 1\n10\n0\n \n0\n \n 2\n110\n10\n0\n10,0\n \n 3\n1110\n10\n1\n10,1\n \n 4\n11110\n110\n00\n110,00\n \n 9\n1111111110\n1110\n001\n1110,001\n \n 13\n \n1110\n101\n1110,101\n \n 24\n \n11110\n1000\n11110,1000\n \n 511\n \n111111110\n11111111\n111111110,11111111\n \n 1025\n \n11111111110\n0000000001\n11111111110,0000000001\n \n\n \n\n\n\nVB codes use an adaptive number of bytes\ndepending on the size of the gap. Bit-level codes adapt the\nlength of the code on the finer grained bit level.\nThe simplest bit-level code is  unary code . The unary\ncode of  is a string of  1s followed by a 0 (see the\nfirst two columns of Table 5.5 ).\nObviously,\nthis is not a very efficient code, but it will come in handy\nin a moment.\n\n\nHow efficient can a code be in principle?  Assuming the\n gaps  with \n are all equally\nlikely, the optimal encoding uses  bits for each .  So\nsome gaps ( in this case) cannot be encoded with\nfewer than  bits.  Our goal is to get as close to\nthis lower bound as possible.\n\n\nA method that is within a factor of optimal\nis   encoding .\n\ncodes implement variable-length encoding by splitting the\nrepresentation of a\ngap  into a pair of length and offset.\nOffset is  in binary, but with the leading 1\nremoved. For example, for 13 (binary 1101) offset is 101. \nLength encodes the length of offset\nin unary code. \nFor 13,\nthe length of offset is 3 bits, which is 1110 in\nunary. The  code of 13 is therefore 1110101, the\nconcatenation of length 1110 and offset 101.\nThe right hand column of Table 5.5  gives additional\nexamples of  codes. \n\n\nA  code is decoded by first reading the unary code\nup to the 0 that terminates it, for example, the four bits 1110\nwhen decoding 1110101. Now we know how long the\noffset is: 3 bits. The offset 101 can then be read correctly and\nthe 1 that was chopped off in encoding is prepended: 101\n 1101 = 13.\n\n\nThe length of offset is\n\n bits and the length of length \nis \n bits, \nso the length of the entire\ncode is\n\n bits.  codes are\nalways of odd length and they are within a factor of\n2 of what we claimed to be \nthe optimal encoding length .\nWe derived this optimum\nfrom the assumption\nthat the  gaps\nbetween  and  are equiprobable.\nBut this need not be the case. In general, we do not know the\nprobability distribution over gaps a priori.\n\n\n\n\nFigure 5.9:\n\nEntropy  as a function of  for a sample space\nwith two outcomes  and .\n\n\n\n\nThe characteristic of a discrete probability\ndistribution  that\ndetermines its coding properties (including whether a code is\noptimal) is its\n   entropy , which is\ndefined as follows:\n\n\n\n\n\n\n(4)\n\n\nwhere  is the set of all possible numbers  we need to be\nable to encode\n(and therefore \n). Entropy is a\nmeasure of uncertainty as shown in Figure 5.9  for a\nprobability distribution  over two possible outcomes, namely,\n\n. Entropy is maximized () for\n\n when uncertainty about which \nwill appear next is largest; and minimized () for \n\n\nand for \n\nwhen there is absolute certainty.\n\n\nIt can be shown\nthat the lower bound for the expected length  of a\ncode  is  if certain conditions hold (see the references). It can\nfurther be shown that for \n, \nencoding is within a factor of 3 of this optimal encoding,\napproaching 2 for large :\n\n\n\n\n\n\n(5)\n\n\nWhat is remarkable about this result is that it holds for\nany probability distribution . So without knowing\nanything about the properties of the distribution of gaps,\nwe can apply  codes and be certain that they are within a\nfactor of  of the optimal code for distributions\nof large entropy. A code like  code with the property of\nbeing within a factor of optimal for an arbitrary distribution  is\ncalled \n universal . \n\n\nIn addition to universality,\n codes have two other properties that are useful for index\ncompression. First, they are \n prefix free , namely, no\n code is the prefix of another. This means that\nthere is always a unique decoding of a sequence of\n codes - and we do not need delimiters between them,\nwhich would decrease the efficiency of the code. The second\nproperty is that  codes are\n parameter free . For many other efficient codes, we\nhave to fit the parameters of a model (e.g., the \nbinomial distribution) to\nthe distribution of gaps in the index. This complicates the\nimplementation of compression and decompression. \nFor instance, the\nparameters need to be stored and retrieved. And in dynamic indexing, the distribution of\ngaps can change, so that the original parameters are no longer\nappropriate. These problems are avoided with a\nparameter-free code.\n\n\nHow much compression of the inverted index do  codes\nachieve? To answer this question we use Zipf's law, the term\ndistribution model introduced in Section 5.1.2 .\nAccording to Zipf's law, the collection frequency  is proportional to the\ninverse of the rank , that is, there is a constant  such that:\n\n\n\n\n\n \n \n\n(6)\n\n\nWe can choose a different constant  such that the \nfractions \nare relative\nfrequencies and sum to 1 (that is, \n\n):\n\n\n\n\n\n\n\n\n(7)\n\n \n \n\n(8)\n\n\nwhere  is the number of distinct terms and \nis the\nth    harmonic\n  number .\n Reuters-RCV1 has\n distinct terms\nand  \n, so we have \n\n\n\n\n\n\n(9)\n\n\nThus the th term has a relative frequency of roughly\n, and\nthe expected average number of occurrences of term\n in a document of length  is:\n\n\n\n\n\n\n(10)\n\n\nwhere we interpret the relative frequency as a term occurrence\nprobability. Recall that 200 is the average number of\ntokens per document in Reuters-RCV1 (Table 4.2 ).\n\n\n\n\nFigure 5.10:\nStratification of terms for\nestimating the size of a  encoded inverted index.\n\n\n\nNow we have derived term\nstatistics that characterize the distribution of terms in\nthe collection and, by extension, the distribution of gaps in\nthe postings lists.\nFrom these statistics, we can\ncalculate\nthe space requirements for an inverted index compressed with\n encoding. We first stratify the \nvocabulary into blocks of size\n. \nOn average, term  occurs  times per\ndocument. So the average number of occurrences\n per document\nis \n for terms in the\nfirst block, corresponding to a total number of  gaps per\nterm. The average is\n\n for terms in the\nsecond block, corresponding to  gaps per term, and\n\n for terms in the\nthird block, corresponding to  gaps per term, and so on. (We\ntake the lower bound because it simplifies subsequent calculations.\nAs we will see, the final estimate is too\npessimistic, even with this assumption.)\nWe will make the somewhat unrealistic assumption that all\ngaps for a given term have the same size\nas shown in Figure 5.10.\nAssuming such a uniform distribution of gaps,\nwe then have gaps of size 1 in block 1, gaps of size 2 in\nblock 2, and so on.\n\n\nEncoding the  gaps of size  with  codes, the number of\nbits needed for the postings list\nof a term in the th block (corresponding to one row in\nthe figure) is:\n\n\n\n\n\nTo encode the entire block, we need \n\n bits. There are \nblocks, so the postings file as a whole will\ntake up:\n\n\n\n\n \n \n\n\n(11)\n\n\n\nFor Reuters-RCV1, \n 400,000\n and \n\n\n\n\n\n\n(12)\n\n\nSo the postings file of the compressed inverted index \nfor our 960 MB collection has a size of 224 MB, one fourth the\nsize of\nthe original collection.\n\n\nWhen we run  compression on Reuters-RCV1,\nthe actual size of the compressed index is even\nlower: 101 MB, a bit more than one tenth of the size of the\ncollection. The reason for the discrepancy between\npredicted and actual value is\nthat (i) Zipf's law is not a very good\napproximation of the actual distribution of term frequencies\nfor Reuters-RCV1 and (ii) gaps are not uniform. The Zipf model predicts an index size of 251 MB\nfor the unrounded numbers from Table 4.2 . If\nterm frequencies are generated from the Zipf model and a\ncompressed index is created for these artificial terms, then\nthe compressed size is 254 MB. So to the extent that\nthe assumptions about the distribution of term frequencies\nare accurate,\nthe predictions of the model are correct.\n\n\n\n\n\n\nTable:\nIndex and dictionary compression for Reuters-RCV1.\nThe compression ratio depends on the proportion of actual text\nin the collection. Reuters-RCV1 contains a\nlarge amount of XML\nmarkup. Using the two best\ncompression schemes,  encoding and blocking with\nfront coding, the \nratio compressed index to collection size is therefore\nespecially small for Reuters-RCV1:\n\n.\n.\n\n data structure\nsize in MB\n\n dictionary, fixed-width\n19.211.2\n\n dictionary, term pointers into string\n10.8 7.6\n\n , with blocking, \n10.3 7.1\n\n , with blocking & front coding\n7.9 5.9\n\n collection (text, xml markup etc)\n3600.0\n\n collection (text)\n960.0\n\n term incidence matrix\n40,000.0\n\n postings, uncompressed (32-bit words)\n400.0\n\n postings, uncompressed (20 bits)\n250.0\n\n postings, variable byte encoded\n116.0\n\n postings,  encoded\n101.0\n\n\n \n\n\n\nTable 5.6  summarizes\nthe compression techniques covered in this chapter.\nThe\nterm incidence matrix \n(Figure 1.1 , page 1.1 )\nfor Reuters-RCV1 has size \n bits or 40 GB.\nThe numbers were the collection (3600 MB and 960 MB) are for\nthe encoding of RCV1 of CD, which uses one byte per\ncharacter, not Unicode.\n\n\n codes achieve great compression ratios - about\n15% better than variable byte codes for Reuters-RCV1. But they\nare expensive to decode. This is because many\nbit-level operations - shifts and masks - are necessary to\ndecode a sequence of  codes as the boundaries between\ncodes will usually be somewhere in the middle of a machine\nword. As a result, query processing is more expensive for\n codes than for variable byte codes.\nWhether we choose variable byte or  encoding \ndepends on the characteristics of an application, for example,\non the relative\nweights we give to conserving disk space versus maximizing query\nresponse time.\n\n\nThe compression ratio for the index in\nTable 5.6  is about 25%: 400 MB\n(uncompressed, each posting stored as a 32-bit word) versus 101 MB\n() and \n116 MB (VB). This shows that both  and VB codes meet\nthe objectives we stated in the beginning of the chapter.\nIndex compression substantially improves time and space\nefficiency of indexes by reducing the amount of disk space needed,\nincreasing the amount of information that can be kept in\nthe cache, and speeding up data transfers from disk to memory.\n \n\nExercises.\n\nCompute variable byte codes for the numbers in\nTables 5.3 5.5 .\n\n\n\nCompute variable byte and  codes for\nthe\npostings list \n777, 17743, 294068, 31251336. Use gaps instead\nof docIDs where possible. Write binary codes in 8-bit blocks.\n\n\n\n  \nConsider the postings list \n\n\nwith a corresponding list of gaps\n\n.\nAssume that the length of the postings list is stored separately, so the system knows when a postings list is complete. \n Using variable byte encoding:\n(i) What is the largest gap you can encode in 1 byte?\n(ii) What is the largest gap you can encode in 2 bytes?\n(iii) How many bytes will the above postings list require under this encoding? (Count only space for encoding the sequence of numbers.) \n\n\n\nA little trick is to notice that a gap cannot be of length 0\nand that the stuff left to encode after shifting cannot be\n0. Based on these observations:\n(i) Suggest a modification to variable byte encoding that\nallows you to encode slightly larger gaps in the same amount\nof space.\n(ii) What is the largest gap you can encode in 1 byte?\n(iii) What is the largest gap you can encode in 2 bytes? \n(iv) How many bytes will the postings list in Exercise 5.3.2 \nrequire under this encoding? \n(Count only space for encoding the sequence of numbers.) \n\n\n\nFrom the following sequence of -coded gaps,\nreconstruct first the gap sequence and then the postings\nsequence: 1110001110101011111101101111011.\n\n\n\n   codes are relatively\ninefficient for large numbers (e.g., 1025 in\nTable 5.5 ) as they encode the length of the\noffset in inefficient unary code.     codes \ndiffer from  codes in that they encode the first\npart of the code (length) in  code instead of\nunary code.  The encoding of offset is the\nsame. For example, the  code of 7 is 10,0,11 (again,\nwe add commas for readability). 10,0 is the  code\nfor length (2 in this case) and the encoding of offset\n(11) is unchanged.  (i) Compute the  codes for the other\nnumbers in Table 5.5 .  For what range of numbers\nis the  code shorter than the  code?\n(ii)  code beats variable byte code in\nTable 5.6  because the index contains stop words and thus\nmany small gaps. Show that variable byte code is more\ncompact if larger gaps dominate. (iii) Compare the\ncompression ratios of  code and variable byte code\nfor a distribution of gaps dominated by large gaps.\n\n\n\nGo through the above calculation of index size and\nexplicitly state all the approximations that were made to\narrive at Equation 11.\n\n\n\nFor a collection of your choosing, determine the number\nof documents and terms and the average length of a\ndocument. (i) How large is the inverted index predicted to be by\nEquation 11? (ii) Implement an indexer that\ncreates a -compressed inverted index for the\ncollection. How large is the actual index?  (iii) Implement an\nindexer that uses variable byte encoding. How large is the\nvariable byte encoded index?\n\n\n\n\n\n\nTable:\n\nTwo gap sequences to be merged in blocked sort-based indexing\n\n  encoded gap sequence of run 1\n1110110111111001011111111110100011111001\n \n  encoded gap sequence of run 2\n11111010000111111000100011111110010000011111010101\n\n\n \n\n \n\n\n\n\nTo be able to hold as many postings as possible in\nmain memory, it is a good idea to compress intermediate\nindex files during index construction. (i) This makes\nmerging runs in blocked sort-based indexing more complicated. As an\nexample, work out the -encoded merged sequence of\nthe gaps in Table 5.7 .  (ii) Index construction is\nmore space efficient when using compression. Would you also\nexpect it to be faster?\n\n\n\n(i) Show that the size of the vocabulary is finite\naccording to Zipf's law and infinite according to Heaps'\nlaw.  (ii) Can we derive Heaps' law from Zipf's law?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Postings file compression\n Previous: Variable byte codes\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}