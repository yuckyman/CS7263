{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/statistical-properties-of-terms-in-information-retrieval-1.html",
  "title": "Statistical properties of terms in information retrieval",
  "body": "\n\n\n\n\nStatistical properties of terms in information retrieval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Heaps' law: Estimating the\n Up: Index compression\n Previous: Index compression\n    Contents \n    Index\n\n\n\n\n \n\nStatistical properties of terms in information retrieval\n\n\nAs in the last chapter, we use Reuters-RCV1 as our\nmodel collection (see Table 4.2 ,\npage 4.2 ).  We give some term and postings\nstatistics for the collection in Table 5.1 . \n``'' indicates the reduction in size from the\nprevious line.\n``T%'' is the cumulative reduction from unfiltered.\n\n\nThe table shows the\nnumber of terms for different levels of preprocessing\n(column 2). The number of terms is\nthe main factor in determining the\nsize of the dictionary. The number of nonpositional\npostings (column 3) is an indicator of the expected size of the\nnonpositional index of the collection. The expected size of\na positional index is related to the number of positions it\nmust encode (column 4).\n\n\nIn general, the\nstatistics in Table 5.1  show that preprocessing affects the size of the\ndictionary and the number of nonpositional postings greatly.  Stemming and\ncase folding reduce the number of (distinct) terms\nby 17% each and the number of nonpositional\npostings by 4% and 3%, respectively.  The treatment of the\nmost frequent words is also important. \nThe  rule of 30  states that the 30 most common words account for 30% of\nthe tokens in written text (31% in the table).  Eliminating\nthe 150 most common words from indexing (as stop words;\ncf. Section 2.2.2 , page 2.2.2 ) cuts\n25% to 30% of the nonpositional postings.  But, although a stop\nlist of 150 words reduces the number of postings by a\nquarter or more, this size reduction does not carry over to the size\nof the compressed index. As we will see later in this\nchapter,  the postings lists of frequent words require only a\nfew bits per posting after compression.\n\n\n\n\n\n\n\nTable 5.1:\n\nThe effect of preprocessing on \nthe number of terms,\nnonpositional postings, and tokens for Reuters-RCV1.\n``'' indicates the reduction in size from the\nprevious line, except that ``30 stop words'' and ``150 stop\nwords'' both use ``case folding'' as their reference\nline. ``T%'' is the cumulative (``total'') reduction from unfiltered.\nWe performed stemming with the Porter stemmer\n(Chapter 2 , page 2.2.4 ).\n  \n \n \n \n \n \n \ntokens (number of position\n \n  \n(distinct) terms\nnonpositional postings\nentries in postings)\n \n  \n \n \n \n \n  \nnumber\n\nT%\nnumber\n\nT%\nnumber\n\nT%\n \n unfiltered\n484,494\n \n \n109,971,179\n \n \n197,879,290\n \n \n \n no numbers\n473,723\n2\n2\n100,680,242\n8\n8\n179,158,204\n9\n9\n \n case folding\n391,523\n17\n19\n96,969,056\n3\n12\n179,158,204\n0\n9\n \n 30 stop words\n391,493\n0\n19\n83,390,443\n14\n24\n121,857,825\n31\n38\n \n 150 stop words\n391,373\n0\n19\n67,001,847\n30\n39\n94,516,599\n47\n52\n \n stemming\n322,383\n17\n33\n63,812,300\n4\n42\n94,516,599\n0\n52\n \n\n \n\n\n\nThe deltas in the table are in a range typical of large\ncollections. Note, however, that the percentage reductions\ncan be very different for some text collections. For\nexample, for a collection of web pages with a high\nproportion of French text, a lemmatizer for French\nreduces vocabulary size much more than the Porter stemmer\ndoes for an English-only collection because French is a\nmorphologically richer language than English.\n\nThe compression techniques we describe in the remainder of\nthis chapter are  lossless ,\nthat is, all information is preserved.  Better compression\nratios can be achieved with  lossy compression , which\ndiscards some information. Case folding, stemming, and stop\nword elimination are forms of lossy compression. Similarly,\nthe vector space model (Chapter 6 ) and dimensionality\nreduction techniques like  latent semantic indexing\n(Chapter 18 ) create compact representations from which we\ncannot fully restore the original collection. Lossy\ncompression makes sense when the ``lost'' information is\nunlikely ever to be used by the search system. For example,\nweb search is characterized by a large number of documents,\nshort queries, and users who only look at the first few\npages of results. As a consequence, we can discard postings\nof documents that would only be used for hits far down the\nlist.  Thus, there are retrieval scenarios where lossy\nmethods can be used for compression without any reduction in effectiveness.\n\n\nBefore introducing techniques for compressing the\ndictionary, we want to estimate the number of distinct terms \nin a collection.  It is sometimes said that languages have a\nvocabulary of a certain size. The second edition of the\nOxford English Dictionary\n(OED) defines more than 600,000 words.  But the \nvocabulary of most large collections is much larger than \nthe\nOED.\nThe OED does not include most names of people, locations,\nproducts, or scientific entities like genes.  These names\nneed to be included in the inverted index, so our\nusers can search for them.\n\n\n\n\n\nSubsections\n\nHeaps' law: Estimating the number of terms\nZipf's law: Modeling the distribution of terms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Heaps' law: Estimating the\n Up: Index compression\n Previous: Index compression\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}