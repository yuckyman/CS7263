{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/standard-test-collections-1.html",
  "title": "Standard test collections",
  "body": "\n\n\n\n\nStandard test collections\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Evaluation of unranked retrieval\n Up: Evaluation in information retrieval\n Previous: Information retrieval system evaluation\n    Contents \n    Index\n\n\n\n \n\nStandard test collections\n\n\nHere is a list of the most standard test collections and evaluation\nseries.  We focus particularly on test collections for ad hoc information\nretrieval system evaluation, but also mention a couple of \nsimilar test collections for text classification.\n\n\nThe  Cranfield  collection. This was the pioneering test\ncollection in allowing\nprecise quantitative measures  of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments.\nCollected in the United Kingdom starting in the late 1950s, it contains \n1398 abstracts of aerodynamics journal articles, a set of 225 queries,\nand exhaustive relevance judgments of all (query, document) pairs.\n\n\n\n\n Text Retrieval Conference (TREC) .  The U.S. \n   National \n  Institute of Standards and Technology  (NIST) \n  has run a large IR test bed evaluation series since 1992.  Within this\n  framework, there have been many tracks over a range of different test\n  collections, but the best known test collections are the ones used for the \n  TREC Ad Hoc track during the first 8 TREC evaluations between\n  1992 and 1999.  In total, these test collections comprise 6 CDs containing 1.89 million documents (mainly, but not exclusively, newswire articles) and relevance judgments for\n  450 information needs, which are called  topics \nand specified in\ndetailed text passages.  Individual test collections are defined over\ndifferent subsets of this data. The early TRECs each consisted of 50\ninformation needs, evaluated over different but overlapping sets of\ndocuments. TRECs 6-8 provide 150 information needs over about 528,000\nnewswire and Foreign Broadcast Information Service articles.   \nThis is probably the best subcollection to use in future work, because\nit is the largest and the topics are more consistent.\nBecause the test document collections are so\nlarge, there are no exhaustive relevance judgments.  Rather, NIST\nassessors' relevance judgments are available only for the documents\nthat were among the top  returned for some system which\nwas entered in the TREC evaluation for which the information need was\ndeveloped.\n\n\nIn more recent years, NIST has done evaluations on larger document\ncollections, including the 25 million page  GOV2  web page collection.\nFrom the beginning, the NIST test document collections were orders of magnitude\nlarger than anything available to researchers previously and GOV2 is\nnow the largest Web collection easily available for research purposes. \nNevertheless, the size of GOV2 is still more than 2 orders of magnitude\nsmaller than the current size of the document collections indexed by\nthe large web search companies. \n\n\n\n\nNII Test Collections for IR Systems ( NTCIR ). The NTCIR project\n  has built various test collections of similar sizes to the TREC\n  collections, focusing on East Asian language and  cross-language\n  information retrieval , where queries are made in one language over\n  a document collection containing documents in one or more other languages.  See:\n  http://research.nii.ac.jp/ntcir/data/data-en.html\n\n\n\nCross Language Evaluation Forum ( CLEF ).  This evaluation series\n  has concentrated on European languages and cross-language information\n  retrieval.  See: http://www.clef-campaign.org/\n\n\n\n   and Reuters-RCV1. For text classification, the most\n  used test collection has been the \n  Reuters-21578 collection of 21578 newswire articles; see Chapter 13 ,\n  page 13.6 . \n  More recently, Reuters released the much larger Reuters Corpus\n  Volume 1 (RCV1), consisting of 806,791 documents; see\n  Chapter 4 , page 4.2 . Its scale and rich annotation makes it\n  a better basis for future research.\n\n\n\n\n 20 Newsgroups .   This is another\n  widely used text classification collection, collected by Ken Lang.\n  It consists of 1000 articles from each of 20 Usenet newsgroups (the\n  newsgroup name being regarded as the category).  After the removal of \n  duplicate articles,\n  as it is usually used, it contains 18941 articles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Evaluation of unranked retrieval\n Up: Evaluation in information retrieval\n Previous: Information retrieval system evaluation\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}