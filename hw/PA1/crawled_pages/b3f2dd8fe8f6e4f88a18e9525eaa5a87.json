{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/low-rank-approximations-1.html",
  "title": "Low-rank approximations",
  "body": "\n\n\n\n\nLow-rank approximations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Latent semantic indexing\n Up: Matrix decompositions and latent\n Previous: Term-document matrices and singular\n    Contents \n    Index\n\n\n\n\n \n\nLow-rank approximations\n\n\nWe next state a matrix approximation problem that at first seems to have little to do with information retrieval. We describe a solution to this matrix problem using singular-value decompositions, then develop its application to information retrieval.\n\n\nGiven an \n matrix  and a positive integer , we wish to find an \n matrix  of rank at most , so as to minimize the  Frobenius norm  of the matrix difference \n, defined to be\n\n\n\n\n\n\n(238)\n\n\nThus, the Frobenius norm of  measures the discrepancy between  and ; our goal is to find a matrix  that minimizes this discrepancy, while constraining  to have rank at most . If  is the rank of , clearly \n and the Frobenius norm of the discrepancy is zero in this case. When  is far smaller than , we refer to  as a  low-rank approximation .\n\n\nThe singular value decomposition can be used to solve the low-rank matrix approximation problem.  We then derive from it an application to approximating term-document matrices. We invoke the following three-step procedure to this end:\n\n\nGiven , construct its SVD in the form shown in (232); thus, \n.\n\nDerive from  the matrix  formed by replacing by zeros the  smallest singular values on the diagonal of .\n\nCompute and output \n as the rank- approximation to .\n\n\nThe rank of  is at most : this follows from the fact that  has at most  non-zero values. Next, we recall the intuition of Example 18.1: the effect of small eigenvalues on matrix products is small. Thus, it seems plausible that replacing these small eigenvalues by zero will not substantially alter the product, leaving it ``close'' to . The following theorem due to Eckart and Young tells us that, in fact, this procedure yields the matrix of rank  with the lowest possible Frobenius error.\n\n\nTheorem.\n\n\n\n\n\n\n\n(239)\n\n\nEnd theorem.\n\nRecalling that the singular values are in decreasing order\n\n, we learn from\nTheorem 18.3 that  is the best\nrank- approximation to , incurring an error (measured\nby the Frobenius norm of \n) equal to .\nThus the larger  is, the smaller this error (and in particular, for , the error is zero since\n\n; provided , then \n and thus \n).\n\n\n\n\n\n\nTo derive further insight into why the process of truncating the smallest  singular values in  helps generate a rank- approximation of low error, we examine the form of :\n\n\n\n\n\n\n\n\n(240)\n \n\n\n\n(241)\n \n\n\n\n(242)\n\n\nwhere  and  are the th columns of  and , respectively. Thus, \n is a rank-1 matrix, so that we have just expressed  as the sum of  rank-1 matrices each weighted by a singular value. As  increases, the contribution of the rank-1 matrix \n is weighted by a sequence of shrinking singular values .\n\n\nExercises.\n\n  \nCompute a rank 1 approximation  to the matrix  in Example 235, using the SVD as in Exercise 236. What is the Frobenius norm of the error of this approximation?\n\n\n\n  \nConsider now the computation in Exercise 18.3 . Following the schematic in Figure 18.2 , notice that for a rank 1 approximation we have  being a scalar. Denote by  the first column of  and by  the first column of . Show that the rank-1 approximation to  can then be written as \n.\n\n\n\nreduced can be generalized to rank  approximations: we let  and  denote the ``reduced'' matrices formed by retaining only the first  columns of  and , respectively. Thus  is an \n matrix while  is a \n matrix. Then, we have\n\n\n\n\n\n\n(243)\n\n\nwhere  is the square  submatrix of  with the singular values \n on the diagonal. The primary advantage of using (243) is to eliminate a lot of redundant columns of zeros in  and , thereby explicitly eliminating multiplication by columns that do not affect the low-rank approximation; this version of the SVD is sometimes known as the  reduced SVD  or  truncated SVD  and is a computationally simpler representation from which to compute the low rank approximation.\n\n\nFor the matrix  in Example 18.2, write down both  and .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Latent semantic indexing\n Up: Matrix decompositions and latent\n Previous: Term-document matrices and singular\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}