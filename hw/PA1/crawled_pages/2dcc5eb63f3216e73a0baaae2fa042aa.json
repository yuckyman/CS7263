{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/time-complexity-of-hac-1.html",
  "title": "Time complexity of HAC",
  "body": "\n\n\n\n\nTime complexity of HAC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Group-average agglomerative clustering\n Up: Single-link and complete-link clustering\n Previous: Single-link and complete-link clustering\n    Contents \n    Index\n\n\n\n\nTime complexity of HAC\n\n\nThe complexity of the naive HAC algorithm in\nFigure 17.2  is  because we exhaustively scan\nthe  matrix  for the largest similarity in each of  iterations.\n\n\n\n\n\n\nFor the four HAC methods discussed in this chapter a more\nefficient algorithm is the priority-queue algorithm shown in\nFigure 17.8 .  Its time complexity is \n.  The rows  of the \nsimilarity matrix  are sorted in decreasing order of\nsimilarity in the priority queues .  \n\nthen returns the cluster in  that currently has the\nhighest similarity with , where we use \nto denote the  cluster as in Chapter 16 .  After creating the\nmerged cluster of  and ,\n is used as its representative.  The function\nSIM computes the similarity function for potential\nmerge pairs: largest similarity for single-link, smallest\nsimilarity for complete-link, average similarity for GAAC\n(Section 17.3 ), and centroid similarity for\ncentroid clustering (Section 17.4 ).  We give an\nexample of how a row of  is processed\n(Figure 17.8 , bottom panel).  \nThe \nloop in lines 1-7\nis  and the loop\nin lines 9-21 is\n\n for an implementation of priority\nqueues that supports deletion and insertion in \n.  The overall complexity of the algorithm is therefore\n\n.  In the definition of the\nfunction SIM,  and \n are the vector\nsums of \n and\n\n, respectively, and  and \nare the number of documents in \n and \n, respectively.\n\n\nThe argument of\nEFFICIENTHAC in\nFigure 17.8  is a set of vectors (as\nopposed to a set of generic documents) because GAAC and centroid\nclustering ( and 17.4 )\n require vectors as\ninput. The complete-link version of \nEFFICIENTHAC can also be applied to documents that are\nnot represented as vectors.\n\n\n\n\n\n\nFor single-link, we can introduce a\nnext-best-merge array (NBM) as a further optimization\nas shown in Figure 17.9 . \nNBM\nkeeps track of what the best merge\nis for each cluster. \nEach of the two top level for-loops in Figure 17.9  are\n,\nthus the overall complexity of single-link clustering is .\n\n\nCan we also\nspeed up the other three HAC algorithms\nwith an NBM array?\nWe cannot because only single-link clustering is \n best-merge persistent . Suppose that\nthe best merge cluster for  is  in\nsingle-link clustering. Then after merging\n with a third cluster \n, the merge of  and  will\nbe 's best merge cluster\n(Exercise 17.10 ). In other words,\nthe best-merge candidate for the merged cluster is\none of the two best-merge candidates of its components in\nsingle-link clustering. This\nmeans that\n can be updated in  in each iteration - by taking a\nsimple max of two values\non line 14 in\nFigure 17.9  for each of the remaining  clusters.\n\n\n\n\n\n\nFigure 17.10  demonstrates that\nbest-merge persistence\ndoes not hold for complete-link clustering, which means\nthat we cannot use an NBM array to speed up clustering.\nAfter merging 's best merge candidate  with\ncluster , an unrelated cluster \nbecomes the best merge candidate for . This is because the\ncomplete-link\nmerge criterion is non-local and can be affected by points at a\ngreat distance from the area where two merge candidates meet.\n\n\nIn practice, the efficiency\npenalty of the\n\n algorithm is small compared with\nthe  single-link algorithm since computing the\nsimilarity between two documents (e.g., as a\ndot product) \nis an order of magnitude slower than\ncomparing two scalars in sorting. All four HAC algorithms\nin this chapter\nare  with respect to similarity computations.\nSo the difference in complexity is rarely a concern in practice when\nchoosing one of the algorithms.\n\n\nExercises.\n\nShow that complete-link clustering creates the two-cluster\nclustering depicted in \nFigure 17.7 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Group-average agglomerative clustering\n Up: Single-link and complete-link clustering\n Previous: Single-link and complete-link clustering\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}