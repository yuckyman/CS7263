{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html",
  "title": "Feature selectionChi2 Feature selection",
  "body": "\n\n\n\n\nFeature selectionChi2 Feature selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Assessing as a feature\n Up: Feature selection\n Previous: Mutual information\n    Contents \n    Index\n\n\n\n\n Feature selectionChi2 Feature selection\n\n\n  \nAnother popular feature selection\nmethod is   .\nIn statistics, the  test is\napplied to test the independence of two events,\nwhere two events A and B are defined to be\n \n independent  if \n or, equivalently, \n and\n. In\nfeature selection, the two events are occurrence of the\nterm and occurrence of the class.\nWe then rank terms with respect to the following\nquantity:\n\n\n\n\n\n \n \n\n(133)\n\n\nwhere  and  are defined as in Equation 130.  \nis the observed frequency in \n and  the\nexpected frequency.  For example,  is the\nexpected frequency of  and  occurring together\nin a document assuming that term and class are independent.\n\n\nWorked example. We first\ncompute  for the\ndata in Example 13.5.1:\n\n\n\n\n\n\n\n\n\n\n(134)\n \n\n\n\n(135)\n\n\nwhere  is the total number of documents as before.\n\n\nWe compute the other \n in the same way:\n\n\n\n\n\n \n\n\n\n\n     \n\n\n\n\n\n    \n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n    \n\n\n\n\n\nPlugging these values into\nEquation 133, we get a  value of 284:\n\n\n\n\n\n \n \n\n(136)\n\n\nEnd worked example.\n\n is a measure of how much expected counts  and observed\ncounts  deviate from each other.  A high value of  \nindicates that the hypothesis of independence, which implies\nthat expected and observed counts are similar, is\nincorrect. In our example, \n. Based\non Table 13.6 , we can reject the hypothesis that\npoultry and export are independent with only\na 0.001 chance of being wrong.Equivalently, we say that the outcome \n is   statistically\nsignificant  at the 0.001 level.  If the two events are\ndependent, then the occurrence of the term makes the occurrence\nof the class more likely (or less likely), so it should be\nhelpful as a feature. This is the rationale of \nfeature selection.\n\n\n\n\n\n\nTable 13.6:\nCritical values of the \ndistribution with one degree of freedom. For example, if\n  the two events are\nindependent, then \n. So for \n\nthe assumption of independence can be rejected with 99% confidence.  \n\n \n critical value\n \n 0.1\n2.71\n \n 0.05\n3.84\n \n 0.01\n6.63\n \n 0.005\n7.88\n \n 0.001\n10.83\n \n\n\n\n\nAn arithmetically simpler way of computing \n is the\nfollowing:\n\n\n\n\n\n\n(137)\n\n\nThis is equivalent to Equation 133\n(Exercise 13.6 ).\n\n\n\n\nSubsections\n\nAssessing\n     as a feature selection\n    methodAssessing chi-square as a feature\n    selection method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Assessing as a feature\n Up: Feature selection\n Previous: Mutual information\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}