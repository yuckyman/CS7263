{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/time-complexity-and-optimality-of-knn-1.html",
  "title": "Time complexity and optimality of kNN",
  "body": "\n\n\n\n\nTime complexity and optimality of kNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Linear versus nonlinear classifiers\n Up: k nearest neighbor\n Previous: k nearest neighbor\n    Contents \n    Index\n\n\n\n\nTime complexity and optimality of kNN\n\n\n\n\nkNN with preprocessing of training set\n\ntraining\n\n\n\ntesting\n\n\n\nkNN without preprocessing of training set\n\ntraining\n\n\ntesting\n\n\n\n\nTraining and test times for kNN classification. is the average size of the vocabulary of \ndocuments in the collection.\n \n\n\n\nTable 14.3  gives the time complexity of kNN.  kNN has\nproperties that are quite different from most other\nclassification algorithms.  Training a kNN classifier simply\nconsists of determining  and preprocessing documents. In\nfact, if we preselect a value for  and do not preprocess,\nthen kNN requires no training at all. In practice, we have\nto perform preprocessing steps like tokenization.  It makes\nmore sense to preprocess training documents once as part of\nthe training phase rather than repeatedly every time we\nclassify a new test document.\n\n\nTest time is \n for kNN.  It is linear in the\nsize of the training set as we need to compute the distance\nof each training document from the test document.  Test time\nis independent of the number of classes . kNN therefore\nhas a potential advantage for problems with large .\n\n\nIn kNN classification, we do not perform any estimation of\nparameters as we do in Rocchio classification (centroids) or\nin Naive Bayes (priors and conditional probabilities). kNN\nsimply memorizes all examples in the training set and then\ncompares the test document to them. For this reason, kNN is\nalso called  memory-based learning  or\n instance-based learning .  It is usually desirable to\nhave as much training data as possible in machine\nlearning. But in kNN large training sets come with a severe\nefficiency penalty in classification.\n\n\n  Can kNN testing be\nmade more efficient than\n\n or, ignoring the\nlength of documents, more efficient than\n\n? There are fast kNN\nalgorithms for small dimensionality  (Exercise 14.8 ).  There\nare also approximations for large  that give error bounds\nfor specific efficiency gains (see\nSection 14.7 ). These approximations have not been\nextensively tested for text classification applications, so\nit is not clear whether they can achieve much better\nefficiency than \n without a significant loss\nof accuracy.\n\n\nThe reader may have noticed the similarity between the\nproblem of finding nearest neighbors of a test document and\nad hoc retrieval, where we search for the documents with the\nhighest similarity to the query (Section 6.3.2 ,\npage 6.3.2 ). In fact, the two problems are both\n nearest neighbor problems and only differ in the\nrelative density of (the vector of) the test document in kNN\n(10s or 100s of non-zero entries) versus the sparseness of\n(the vector of) the query in ad hoc retrieval (usually fewer\nthan 10 non-zero entries). We introduced the inverted index\nfor efficient ad hoc retrieval in Section 1.1 \n(page 1.1 ). Is the inverted index also the\nsolution for efficient kNN?\n\n\nAn inverted index restricts a search to those documents that\nhave at least one term in common with the query. Thus in the\ncontext of kNN, the inverted index will be efficient if the\ntest document has no term overlap with a large number of\ntraining documents. Whether this is the case depends on\nthe classification problem. If documents are long and no\nstop list is used, then less time will be saved. But with\nshort documents and a large stop list, an inverted index may\nwell cut the average test time by a factor of 10 or more.\n\n\nThe search time in an inverted index is a function of the\nlength of the postings lists of the terms in the\nquery. Postings lists grow sublinearly with the length of\nthe collection since the vocabulary increases according to\nHeaps' law - if the probability of occurrence of some terms\nincreases, then the probability of occurrence of others must\ndecrease. However, most new terms are infrequent. We\ntherefore take the complexity of inverted index search to be\n (as discussed in Section 2.4.2 ,\npage 2.4.2 ) and, assuming average document\nlength does not change over time, \n\n.\n\n\n  As we will see in the next\nchapter, kNN's effectiveness is close to that of the most\naccurate learning methods in text classification\n(Table 15.2 , page 15.2 ).\nA measure of the quality of a learning method is its\n Bayes error rate , the average error rate of\nclassifiers learned by it for a particular problem.\nkNN\nis not optimal for problems with a non-zero Bayes error rate\n- that is, for problems where even the best possible classifier has\na non-zero classification error.  The error of 1NN\nis asymptotically (as the training set increases) bounded by\ntwice the Bayes error rate. That is, if the optimal\nclassifier has an error rate of , then 1NN has an\nasymptotic error rate of less than .  This is due to the\neffect of noise - we already saw one example of noise in\nthe form of noisy features in Section 13.5 \n(page 13.5 ), but noise can also take other\nforms as we will discuss in the next section.  Noise affects\ntwo components of kNN: the test document and the closest\ntraining document. The two sources of noise are additive, so\nthe overall error of 1NN is twice the optimal error\nrate. For problems with Bayes error rate 0, the error rate\nof 1NN will approach 0 as the size of the training set\nincreases.\n\n\nExercises.\n\nExplain why kNN handles multimodal classes better\nthan Rocchio.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Linear versus nonlinear classifiers\n Up: k nearest neighbor\n Previous: k nearest neighbor\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}