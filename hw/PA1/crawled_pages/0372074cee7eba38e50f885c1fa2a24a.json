{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/probabilistic-approaches-to-relevance-feedback-1.html",
  "title": "Probabilistic approaches to relevance feedback",
  "body": "\n\n\n\n\nProbabilistic approaches to relevance feedback\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: An appraisal and some\n Up: The Binary Independence Model\n Previous: Probability estimates in practice\n    Contents \n    Index\n\n\n\n \n\nProbabilistic approaches to relevance feedback\n\n\nWe can use (pseudo-)relevance feedback, perhaps in an iterative process of estimation, to get a more accurate estimate of .\nThe probabilistic approach to relevance feedback works as follows:\n\n\nGuess initial estimates of  and . This\n  can be done using the probability estimates of the previous\n  section.  For instance, we can assume that  is constant over all  in the query, in particular, perhaps taking \n.\n\nUse the current estimates of  and  to determine a best guess at the set of relevant documents \n.  Use this model\nto retrieve a set of candidate relevant documents, which we present to the user.\n\n\n\nWe interact with the user to refine the model of .  We do this by learning from the user relevance judgments for some subset of documents .\nBased on relevance judgments,  is partitioned into two subsets: \n and\n  \n, which is disjoint from .\n\n\n\nWe reestimate  and  on the basis of known relevant and nonrelevant documents. If the sets  and  are large enough, we may be able to estimate these quantities directly from these documents as maximum likelihood estimates:\n\n\n\n\n\n\n(77)\n\n\n(where  is the set of documents in  containing ). In practice, we usually need to smooth these estimates.  We can do this by  adding   to both the count  and to the number of relevant documents not containing the term, giving:\n\n\n\n\n\n\n(78)\n\n\nHowever, the set of documents judged by the user () is usually very small, and so the resulting statistical estimate is quite unreliable (noisy), even if the estimate is smoothed.  So it is often better to combine the new information with the original guess in a process of  Bayesian updating . In this case we have:\n\n\n\n\n\n\n(79)\n\n\n\nHere  is the  estimate for  in an\niterative updating process and is used as a Bayesian prior in the next iteration with a weighting of .  Relating this equation back to Equation 59 requires a bit more probability theory than we have presented here (we need to use a beta distribution prior, conjugate to the Bernoulli random variable ).  But the form of the resulting equation is quite straightforward: rather than uniformly distributing pseudocounts, we now distribute a total of  pseudocounts according to the previous estimate, which acts as the prior distribution.\nIn the absence of other evidence (and assuming that the user is perhaps indicating roughly 5 relevant or nonrelevant documents) then a value of around  is perhaps appropriate. That is, the prior is strongly weighted so that the estimate does not change too much from the evidence provided by a very small number of documents.\n\n\n\nRepeat the above process from step 2, generating a succession of approximations to  and hence , until the user is satisfied.\n\n\n\nIt is also straightforward to derive a pseudo-relevance feedback version of this algorithm, where we simply pretend that .  More briefly:\n\n\nAssume initial estimates for  and  as above.\n\nDetermine a guess for the size of the relevant document set. If unsure, a conservative (too small) guess is likely to be best. This motivates use of a fixed size set  of highest ranked documents.\n\n\n\nImprove our guesses for  and . We choose from the methods of  and 79  for re-estimating , except now based on the set  instead of . If we let  be the subset of documents in  containing  and use  add  smoothing , we get:\n\n\n\n\n\n\n(80)\n\n\nand if we assume that documents that are not retrieved are nonrelevant then we can update our  estimates as:\n\n\n\n\n\n\n(81)\n\n\n\n\nGo to step 2 until the ranking of the returned results converges.\n\n\n\nOnce we have a real estimate for  then the  weights used in the  value look almost like a tf-idf value. For instance, using Equation 73, Equation 76, and Equation 80, we have:\n\n\n\n\n\n\n(82)\n\n\nBut things aren't quite the same:  measures the (estimated) proportion of relevant documents that the term  occurs in, not term frequency.  Moreover, if we apply log identities:\n\n\n\n\n\n\n(83)\n\n\nwe see that we are now adding the two log scaled components rather than multiplying them.\n\n\nExercises.\n\nWork through the derivation of Equation 74 from  and 3()I .\n\n\n\nWhat are the differences between standard\nvector space tf-idf weighting and the BIM probabilistic\nretrieval model (in the case where no document relevance information is available)?\n\n\n\nLet  be a random variable indicating whether the term  appears in a document.  Suppose we have  relevant documents in the document collection and that  in  of the documents.  Take the observed data to be just these observations of  for each document in .  Show that the MLE for the parameter \n, that is, the value for  which maximizes the probability of the observed data, is .\n\n\n\nDescribe the differences between vector space relevance feedback and probabilistic relevance feedback.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: An appraisal and some\n Up: The Binary Independence Model\n Previous: Probability estimates in practice\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}