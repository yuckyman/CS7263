{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/relation-to-multinomial-unigram-language-model-1.html",
  "title": "Relation to multinomial unigram language model",
  "body": "\n\n\n\n\nRelation to multinomial unigram language model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The Bernoulli model\n Up: Naive Bayes text classification\n Previous: Naive Bayes text classification\n    Contents \n    Index\n\n\n\n\n \n\n\nRelation to multinomial unigram language model\nThe multinomial NB model is formally identical to the\nmultinomial unigram language model \n(Section 12.2.1 ,\npage 12.2.1 ).\nIn particular, Equation 113  is a special case of\nEquation 104 from page 12.2.1 ,\nwhich we repeat here for :\n\n\n\n\n\n \n \n\n(120)\n\n\nThe document  in text classification\n(Equation 113) takes the role of the query in\nlanguage modeling (Equation 120) and the classes\n in text classification take the role of the documents\n in language modeling.  We used Equation 120\nto rank documents according to the probability that they\nare relevant to\n the query . In NB classification, we are\nusually only interested in the top-ranked class.\n\n\nWe also used MLE estimates in Section 12.2.2 (page )\nand encountered the problem of zero estimates owing to sparse\ndata (page 12.2.2 ); but instead of add-one\nsmoothing, we used a mixture of two distributions to address\nthe problem there.\nAdd-one smoothing is closely related to\n add-  smoothing in\nSection 11.3.4 (page ).\n\n\nExercises.\n\n Why is \n in\nTable 13.2  expected to hold for most text\ncollections ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The Bernoulli model\n Up: Naive Bayes text classification\n Previous: Naive Bayes text classification\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}