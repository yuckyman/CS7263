{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/connections-to-text-summarization-1.html",
  "title": "Connections to text summarization.",
  "body": "\n\n\n\n\nConnections to text summarization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Machine learning methods in\n Up: Document zones in text\n Previous: Separate feature spaces for\n    Contents \n    Index\n\n\n\n\nConnections to text summarization.\n\n\nIn Section 8.7 , we mentioned the field of text summarization,\nand how most work in that field has adopted the limited goal of\nextracting and assembling pieces of the original text that are judged\nto be central based on\nfeatures of sentences that consider the sentence's position and\ncontent.  Much of this work can be used to suggest zones that may be\ndistinctively useful for text classification.  For example\nKocz et al. (2000) consider a form of feature selection\nwhere you classify documents based only on words in certain zones.\nBased on text summarization research, they consider using\n(i) only the title, (ii) only the first paragraph, (iii) only the\nparagraph with the most title words or keywords, (iv) the first two\nparagraphs or the first and last paragraph, or (v) all sentences with\na minimum number of title words or keywords.  In general, these\npositional feature selection methods produced as good results as mutual\ninformation (Section 13.5.1 ), and resulted in quite competitive\nclassifiers.  Ko et al. (2004) also took inspiration from text\nsummarization research to upweight sentences with either words from\nthe title or words that are central to the document's content, leading\nto classification accuracy gains of almost 1%.\nThis presumably works because most such sentences are somehow\nmore central to the concerns of the document.\n\n\nExercises.\n\nSpam email often makes use of various cloaking techniques to try to\nget through.  One method is to pad or substitute characters so as to\ndefeat word-based text classifiers.  For example, you see terms like\nthe following in spam email:\n\n\nRep1icaRolex\nbonmus\nViiiaaaagra\npi11z\n\nPHARlbdMACY\n[LEV]i[IT]l[RA]\nsexual\nClAfLlS\n\n\n\nDiscuss how you could engineer features that would largely defeat this\nstrategy. \n\n\n\nAnother strategy often used by purveyors of email spam is to follow\nthe message they \nwish to send (such as buying a cheap stock or whatever) with a\nparagraph of text from another innocuous source (such as a news\narticle).  Why might this strategy be effective?  How might it be\naddressed by a text classifier?\n\n\n\nWhat other kinds of features appear as if they would be useful in an\nemail spam classifier?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Machine learning methods in\n Up: Document zones in text\n Previous: Separate feature spaces for\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}