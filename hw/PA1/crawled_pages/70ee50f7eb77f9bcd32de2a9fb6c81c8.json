{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html",
  "title": "Mutual information",
  "body": "\n\n\n\n\nMutual information\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Feature selectionChi2 Feature selection\n Up: Feature selection\n Previous: Feature selection\n    Contents \n    Index\n\n\n\n\n \n\nMutual\n  information\n\n\nA common feature selection method is to compute  as\nthe expected  mutual information  (MI) of term  and\nclass . MI measures how much information the\npresence/absence of a term contributes to making the correct\nclassification decision on . Formally:\n\n\n\n\n\n\n\n\n(130)\n\n\nwhere  is a random variable that\ntakes values  (the document contains term ) and\n (the document does not contain ),\nas defined on page 13.4 , and  is a random variable\nthat takes values \n(the document is in class ) and  (the document is not in\nclass ).\nWe write  \nand  \nif it is not clear from context which term  and class \nwe are referring to.\n\n\nFor\nMLEs of the probabilities,\nEquation 130  is equivalent to Equation 131:\n\n\n\n\n\n\n\n\n(131)\n \n \n\n\n(132)\n\n\nwhere the s are\ncounts of documents that have the values of\n and  that are indicated by the two subscripts.\nFor example,\n\n\nis the number of documents that contain  () and\nare not in  ().\n\n is the number of documents that\ncontain  () and we count documents independent\nof class membership (\n).\n\n\nis the total number of documents. An example of one of the MLE\nestimates that transform Equation 130 into Equation 131 is\n\n.\n\n\nWorked example.\nConsider the class poultry and the\nterm export in Reuters-RCV1. The counts of the\nnumber of documents with the four possible combinations of\nindicator values are as follows:\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter plugging these values into Equation 131\nwe get:\n\n\n\n\n\n\nEnd worked example.\n\nTo select  terms \n for a given class, we\nuse the feature selection algorithm in\nFigure 13.6 : We compute the utility measure as\n\n and select the  terms with the\nlargest values.\n\n\nMutual information measures how much information - in the\ninformation-theoretic sense - a term contains about the\nclass. If a term's distribution is the same in the class as\nit is in the collection as a whole, then \n. MI\nreaches its maximum value if the term is a perfect indicator\nfor class membership, that is, if the term is present in a document if\nand only if the document is in the class.\n\n\n\n\nFigure 13.7:\nFeatures with high\nmutual information scores for six Reuters-RCV1 classes.\n\n\n\n\nFigure 13.7  shows terms with high\nmutual information scores for the six classes\nin Figure 13.1 . The selected terms (e.g.,\nlondon, uk, british for the class UK)\nare of\nobvious utility for making classification decisions for their respective classes.\nAt the bottom of the list for UK we find terms like peripherals and\ntonight (not shown in the figure) that are clearly not helpful in deciding whether the\ndocument is in the class. As you might expect, keeping the\ninformative terms and eliminating the non-informative ones\ntends to reduce noise and improve the classifier's accuracy.\n\n\n\n\nFigure 13.8:\nEffect of feature set size on accuracy for\nmultinomial and Bernoulli models.\n\n\n\nSuch an accuracy increase can be observed in\nFigure 13.8 , which shows  as a function of\nvocabulary size after feature selection for\nReuters-RCV1.  Comparing\n at 132,776 features (corresponding to selection of all\nfeatures) and at 10-100 features, we see that MI feature\nselection increases  by about 0.1 for the multinomial\nmodel and by more than 0.2 for the Bernoulli model.  For the\nBernoulli model,  peaks early, at ten features selected.\nAt that point, the Bernoulli model is better than the\nmultinomial model.  When basing a classification decision on\nonly a few features, it is more robust to consider binary\noccurrence only.  For the multinomial model (MI feature selection), the peak occurs\nlater, at 100 features, and its effectiveness recovers somewhat\nat the end when we use all features.  The reason is that the\nmultinomial takes the number of occurrences into account in\nparameter estimation and classification and therefore better\nexploits a larger number of features than the Bernoulli\nmodel. Regardless of the differences between the two\nmethods, using a carefully selected subset of the features\nresults in better effectiveness than using all\nfeatures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Feature selectionChi2 Feature selection\n Up: Feature selection\n Previous: Feature selection\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}