{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/list-of-figures-1.html",
  "title": "List of Figures",
  "body": "\n\n\n\n\nList of Figures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Table of Notations\n Up: irbook\n Previous: List of Tables\n    Contents \n    Index\n\n\n\n\n\nList of Figures\n\nResults from Shakespeare for the query\nBrutus AND Caesar AND NOT\nCalpurnia.\nIntersecting the postings lists for Brutus and\n  Calpurnia from Figure 1.3 .\nAlgorithm for the intersection of two postings lists  and .\nAlgorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms.\nA stop list of 25\n  semantically non-selective words which are common in Reuters-RCV1.\nAn example of how asymmetric expansion of query terms can\n  usefully model users' expectations.\nA comparison of three stemming algorithms on a sample text.\nPostings lists intersection with skip pointers.\nA portion of a permuterm index.\nDynamic programming algorithm for computing the edit distance between strings  and .\nMatching at least two of the three 2-grams in the query bord.\nDocument from the Reuters newswire.\nInversion of a block in \nsingle-pass in-memory indexing\n An example of distributed indexing with MapReduce.\nAdapted from Dean and Ghemawat (2004).\nLogarithmic merging. Each token\n(termID,docID) is\ninitially added to in-memory index  by LMERGEADDTOKEN. \nLOGARITHMICMERGE initializes  and .\nA user-document matrix for \naccess control lists. Element \nis 1 if user  has access to document  and 0\notherwise. During query processing, a user's access postings list is intersected with\nthe results list returned by the text part of the index.\n\nStoring the dictionary as an array of fixed-width entries.\n\nSearch of the uncompressed dictionary (a) and a\ndictionary compressed by blocking with  (b).\nEntropy  as a function of  for a sample space\nwith two outcomes  and .\n\nStratification of terms for\nestimating the size of a  encoded inverted index.\nZone index in which the zone is encoded in the postings rather than the dictionary.\nAn illustration of training examples.\nThe four possible combinations of  and .\nCollection frequency (cf) and document frequency (df) behave differently, as in this example from the Reuters collection.\nTable of tf values for Exercise 6.2.2.\nEuclidean normalized tf values for documents in Figure 6.9 .\nThe basic algorithm for computing vector space scores.\nPivoted document length normalization.\nImplementing pivoted document length normalization by linear scaling.\nA faster algorithm for vector space scores.\nCluster pruning.\nPrecision/recall graph.\nThe ROC curve corresponding to the precision-recall curve in\n  Figure 8.2 .\nThe Rocchio optimal query for separating relevant and\n  nonrelevant documents.\nAn XML document.\nThe XML document in Figure 10.1  as a\n  simplified DOM object.\nAn XML query in NEXI format and its partial\n  representation as a tree.\nTree representation of XML documents and queries.\nPartitioning an XML document\ninto non-overlapping indexing units.\nSchema heterogeneity: intervening nodes and\n  mismatched names.\nA structural mismatch between two queries and a document.\nA mapping of an XML document (left) to a set of\nlexicalized subtrees (right).\nThe algorithm for scoring documents with SIMNOMERGE.\nScoring of a query with one structural term in SIMNOMERGE.\n\nSimplified schema of the documents in the\nINEX collection.\nPartial specification of two unigram language\n  models.\nThree ways of developing the language modeling approach: (a) query\n  likelihood, (b) document likelihood, and (c) model\n  comparison.\nClasses, training set, and test set in text\nclassification .\nNaive Bayes algorithm (multinomial model):\nTraining and testing.\nThe multinomial NB model.\nThe Bernoulli NB model.\nBasic feature selection algorithm for selecting the\n best features.\nFeatures with high\nmutual information scores for six Reuters-RCV1 classes.\nEffect of feature set size on accuracy for\nmultinomial and Bernoulli models.\nA sample document from the  Reuters-21578\ncollection.\nVector space classification into three classes.\nRocchio classification.\nRocchio classification: Training and\ntesting.\nThere are an infinite number of\nhyperplanes that separate two linearly separable classes.\nLinear classification algorithm.\nA nonlinear problem.\n hyperplanes\ndo not divide space into  disjoint regions.\nA simple non-separable set of points.\nThe support vectors are the 5 points right up against the margin of\n  the classifier.\nThe geometric margin of a point () and a decision boundary\n  ().\nA tiny 3 data point training set for an SVM.\nLarge margin classification with slack \nvariables.\nProjecting data that is not linearly separable into a higher\n  dimensional space can make it linearly separable.\nAn example of a data set with a clear cluster\nstructure.\nA simple, but inefficient HAC algorithm.\nThe documents of Example 18.4 reduced to two dimensions in .\nDocuments for Exercise 18.5.\nGlossary for Exercise 18.5.\nTwo nodes of the web graph joined by a link.\nCloaking as used by spammers.\nThe various components of a web search engine.\nTwo sets  and ; their Jaccard coefficient is .\nThe basic crawler architecture.\nDistributing the basic crawl architecture.\nExample of an auxiliary hosts-to-back queues table.\nA lexicographically ordered set of URLs.\nA four-row segment of the table of links.\nThe random surfer at node A proceeds with probability 1/3 to each of B, C and D.\nA simple Markov chain with three states; the numbers on the links indicate the transition probabilities.\nThe sequence of probability vectors.\nA sample run of HITS on the query japan elementary schools.\nWeb graph for Exercise 21.3.1 .\n\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}