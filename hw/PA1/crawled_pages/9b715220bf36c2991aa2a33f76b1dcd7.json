{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html",
  "title": "Tokenization",
  "body": "\n\n\n\n\nTokenization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Dropping common terms: stop\n Up: Determining the vocabulary of\n Previous: Determining the vocabulary of\n    Contents \n    Index\n\n\n\n \n\nTokenization\n\n\nGiven a character sequence and a defined document unit, tokenization is\nthe task of chopping it up \ninto pieces, called  tokens , perhaps\nat the same time \nthrowing away certain characters, such as punctuation.\nHere is an example of tokenization:\n\nInput: Friends, Romans, Countrymen, lend me your ears; \n\nOutput: \n \n\n\n\n\n \n \n\n\n\n\nThese tokens are often loosely referred to as terms or\nwords, but it is sometimes important to make a\ntype/token  distinction.  A\n token   is an instance of a\nsequence of characters in some particular document that\nare grouped together as a useful semantic unit for\nprocessing.  A  type  is the class of all tokens\ncontaining the same character sequence.  A  term  is a\n(perhaps normalized) type that is included in the IR system's\ndictionary.  The set of index terms could be entirely\ndistinct from the tokens, for instance, they could be\nsemantic identifiers in a taxonomy, but in practice in\nmodern IR systems they are strongly related to the tokens in\nthe document. However, rather than being exactly the tokens\nthat appear in the document, they are usually derived from them\nby various normalization processes which are discussed in\nSection 2.2.3 .For example, if the document to be indexed is to sleep\nperchance to dream, then there are 5 tokens, but only 4\ntypes (since there are 2 instances of to). However, if\nto is omitted from the index (as a stop word, see\nSection 2.2.2 (page )), then there will be only 3 terms:\nsleep, perchance, and dream.\n\n\nThe major question of the tokenization phase is what are the correct tokens to\nuse?  In this example, it looks\nfairly trivial: you chop on whitespace and throw away punctuation\ncharacters.  This is a starting point, but even for English there are a\nnumber of tricky cases.  For example, what do you do about the various\nuses of\nthe apostrophe for possession and contractions?\n\nMr. O'Neill\nthinks that the boys' stories about Chile's capital aren't amusing.\n\n\nFor O'Neill, which of the following is the desired tokenization?\n\n\n\n \n\n \n\n  \n\n \n \n?\n\n\nAnd for aren't, is it:\n\n\n\n\n\n\n \n\n\n ?\n\n\nA simple strategy is to just split on all non-alphanumeric characters, but while \n\n \n looks okay, \n\n  \nlooks intuitively bad.  For all of them, the choices determine\nwhich Boolean queries will match.  A query of neill AND\ncapital will match in three cases but not the other two.  In how\nmany cases would a query of o'neill AND capital\nmatch?  If no preprocessing of a query is done, then it would match\nin only one of the five cases.  For either\nBoolean or free text queries, you always want to do\nthe exact same tokenization of document and query words, generally \nby processing queries with the same tokenizer.\nThis guarantees that a sequence of characters in a text will always match the\nsame sequence typed in a query.\n\nThese issues of tokenization are language-specific.  It thus\nrequires the language of the document to be known.  \n Language identification \nbased on classifiers that use short character subsequences as \nfeatures is highly effective; most languages have distinctive\nsignature patterns (see page 2.5  for references). \n\n\nFor most languages and particular domains within them there are unusual\nspecific tokens that \nwe wish to recognize as terms, such as the programming languages\nC++ and C#, aircraft names like B-52,  or a\nT.V. show name such as M*A*S*H - \nwhich is sufficiently integrated into popular culture that you find\nusages such as \nM*A*S*H-style hospitals.  Computer technology has introduced\nnew types of character sequences that a tokenizer should probably \ntokenize as a single token, including \nemail addresses (jblack@mail.yahoo.com), \nweb URLs (http://stuff.big.com/new/specials.html), \nnumeric IP addresses (142.32.48.231),\npackage tracking numbers (1Z9999W99845399981),\nand more.  One possible solution is to omit from indexing\ntokens such as monetary amounts,\nnumbers, and URLs, since their presence greatly expands the size of the\nvocabulary.  However, this comes at a large cost in restricting what\npeople can search for.  For instance, people might want to search in a\nbug database for the line number where an error occurs.\nItems such as the date of an email, which have a clear semantic type,\nare often indexed separately as document  metadata  \nparametricsection.\n\n\nIn English,  hyphenation  is used for various purposes\nranging from \nsplitting up vowels in words (co-education)\nto joining nouns as names (Hewlett-Packard) to a\ncopyediting device to show word grouping (the\nhold-him-back-and-drag-him-away maneuver).  It is easy to feel that the\nfirst example should be regarded as one token (and is indeed more commonly\nwritten as just coeducation), the last should be separated into\nwords, and that the middle case is unclear.  Handling hyphens\nautomatically can thus be complex: it can either be done as a\nclassification problem, or more commonly by some heuristic rules, such\nas allowing short hyphenated prefixes on words, but not longer\nhyphenated forms.  \n\n\nConceptually, splitting on white space can also split what should be\nregarded as a single token.  This occurs most commonly with names\n(San Francisco, Los Angeles) but also with borrowed foreign phrases\n(au fait) and compounds that are sometimes written as a single\nword and sometimes space separated (such as white space vs. whitespace).  Other cases with internal spaces that we\nmight wish to regard as a single token include phone numbers\n((800) 234-2333) and dates (Mar 11, 1983).\nSplitting tokens on spaces can cause bad retrieval results, for example,\nif a search for York University mainly returns documents containing\nNew York University.\nThe problems of hyphens and non-separating \nwhitespace can even interact.  Advertisements for air fares frequently\ncontain items like San Francisco-Los Angeles, where simply doing\nwhitespace splitting would give unfortunate results.  In such cases,\nissues of tokenization interact with handling phrase queries (which we\ndiscuss in Section 2.4 (page )), particularly if we would like queries for\nall of lowercase, lower-case and lower case to\nreturn the same results.  The last two can be handled by splitting on\nhyphens and using a phrase index.\nGetting the first case right would depend on knowing that\nit is sometimes written as two words and also indexing it in this way.\nOne effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis\n(westlaw), is to encourage\nusers to enter hyphens wherever they may be possible, and whenever there is\na hyphenated form, the system will generalize the query\nto cover all three of the one word, hyphenated, and two word forms, \nso that a query for over-eager will search for \nover-eager OR ``over eager'' OR overeager.\nHowever, this strategy depends on user training, since if you query using\neither of the other two forms, you get no generalization. \n\n\nEach new\nlanguage presents some new issues.  For instance,\nFrench has a variant use of \nthe apostrophe for a reduced definite article the before a word\nbeginning with a vowel (e.g., l'ensemble) and has some uses of\nthe hyphen \nwith postposed clitic pronouns in imperatives and questions (e.g.,\ndonne-moi give me).  Getting the first case correct \nwill affect the correct indexing of a fair percentage of nouns and\nadjectives: you would want documents mentioning both l'ensemble\nand un ensemble to be indexed under ensemble.  Other\nlanguages make the problem harder in new ways. \nGerman writes  compound nouns  without spaces (e.g.,\nComputerlinguistik `computational linguistics'; \nLebensversicherungsgesellschaftsangestellter\n`life insurance company employee').  Retrieval systems for German greatly benefit\nfrom the use of a  compound-splitter  module, which is usually implemented\nby seeing if a word can be subdivided into multiple words that appear in a \nvocabulary. \nThis phenomenon reaches its limit\ncase with major East Asian Languages (e.g., Chinese, Japanese, Korean,\nand Thai), where text is written without any spaces between words. An\nexample is shown in Figure 2.3 .   One approach here is to\nperform   word segmentation  as prior linguistic processing.  \nMethods of word segmentation vary from having a large\nvocabulary and taking the longest vocabulary match with some heuristics\nfor unknown words to the use of machine learning sequence models, such\nas hidden Markov models or conditional random fields, trained over\nhand-segmented words (see the references in Section 2.5 ).\nSince there are multiple possible segmentations of character sequences \n(see Figure 2.4 ), all such methods make mistakes sometimes, and so\nyou are never guaranteed a consistent unique tokenization.  The other\napproach is to abandon word-based indexing and to do all indexing via\njust short subsequences of characters (character -grams), regardless of\nwhether particular sequences cross word boundaries or not. Three reasons\nwhy this approach is appealing are that an individual Chinese character\nis more like a syllable than a letter and usually has some semantic\ncontent, that most words are short (the commonest length is 2\ncharacters), and that, given the lack of\nstandardization of word breaking in the writing system, it is not always\nclear where word boundaries should be placed anyway.  Even in English,\nsome cases of where to put word boundaries are just orthographic\nconventions - think of notwithstanding vs. not to mention\nor into vs. on to - but people are educated to write the\nwords with consistent use of spaces.\n\n\n\n\nThe standard\n  unsegmented form of Chinese text using the\n  simplified characters of mainland China.There is no\n  whitespace between words, not even between sentences - the apparent\n  space after the Chinese period () is just a typographical\n  illusion caused by placing the character on the left side of its\n  square box.  The first sentence is just words in Chinese characters\n  with no spaces between them. The second and third sentences include\n  Arabic numerals and punctuation breaking up the Chinese characters.\n\n\n\n\n\nAmbiguities in Chinese word segmentation.The two characters can be treated as one word meaning `monk' or as a sequence of two words meaning `and' and `still'.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Dropping common terms: stop\n Up: Determining the vocabulary of\n Previous: Determining the vocabulary of\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}