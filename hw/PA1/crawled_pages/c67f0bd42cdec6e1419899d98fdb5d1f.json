{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/feature-selection-for-multiple-classifiers-1.html",
  "title": "Feature selection for multiple classifiers",
  "body": "\n\n\n\n\nFeature selection for multiple classifiers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Comparison of feature selection\n Up: Feature selection\n Previous: Frequency-based feature selection\n    Contents \n    Index\n\n\n\n\n\n\nFeature selection for multiple classifiers\nIn an operational system with a large number of classifiers,\nit is desirable to select a single set of features instead\nof a different one for each classifier. One way of doing\nthis is to compute the  statistic for an \ntable where the columns are occurrence and nonoccurrence of\nthe term and each row corresponds to one of the classes.  We\ncan then select the  terms with the highest \nstatistic as before.\n\n\nMore commonly, feature selection statistics are first\ncomputed separately for each class on the two-class\nclassification task  versus  and then\ncombined. One combination method computes \na single figure of\nmerit for each feature, for example, by averaging the values \n for feature , and then selects the\n features with highest figures of merit. Another frequently used combination method selects\nthe top  features for each of  classifiers and\nthen combines these  sets into one global feature set.\n\n\nClassification accuracy often decreases when\nselecting  common features for a system with \nclassifiers as opposed to  different sets of size\n. But even if it does, the gain in efficiency owing to\na common document representation may be worth the loss in\naccuracy .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Comparison of feature selection\n Up: Feature selection\n Previous: Frequency-based feature selection\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}