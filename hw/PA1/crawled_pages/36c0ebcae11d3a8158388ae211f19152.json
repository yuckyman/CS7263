{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/crawling-1.html",
  "title": "Crawling",
  "body": "\n\n\n\n\nCrawling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Crawler architecture\n Up: Web crawling and indexes\n Previous: Features a crawler should\n    Contents \n    Index\n\n\n\n\n\n\nCrawling\n\nThe basic operation of any hypertext crawler (whether for the Web, an\nintranet or other hypertext document collection) is as follows. The crawler\nbegins with one or more URLs that constitute a seed set. It\npicks a URL from this seed set, then fetches the web page at that\nURL. The fetched page is then parsed, to extract both the text and\nthe links from the page (each of which points to another URL). The\nextracted text is fed to a text indexer (described in Chapters 4 5 ). The extracted links (URLs)\nare then added to a URL frontier, which at all times consists of URLs whose corresponding pages have yet to be fetched by the\ncrawler. Initially, the URL frontier contains the seed set; as pages are fetched, the corresponding URLs are deleted from the URL frontier. The entire process may be viewed as traversing the web graph (see Chapter 19 ).  In continuous crawling, the URL of a fetched page is added back to the frontier for fetching again in the future.\n\n\nThis seemingly simple recursive traversal of the web graph is\ncomplicated by the many demands on a practical web crawling system: the crawler has to be distributed, scalable, efficient, polite, robust and extensible while fetching pages of high quality. We examine the effects of each of these issues. Our treatment follows\nthe design of the  Mercator  crawler that has formed the basis of a number of research and commercial crawlers. As a reference point, fetching a billion pages (a small fraction of the static Web at present) in a month-long crawl requires fetching several hundred pages each second. We will see how to use a multi-threaded design to address several bottlenecks in the overall crawler system in order to attain this fetch rate.\n\n\nBefore proceeding to this detailed description, we reiterate for readers who may attempt to build crawlers of some basic properties any non-professional crawler should satisfy:\n\n\nOnly one connection should be open to any given host at a time.\n\nA waiting time of a few seconds should occur between successive requests to a host.\n\nPoliteness restrictions detailed in Section 20.2.1  should be obeyed.\n\n\n\n\n\nSubsections\n\nCrawler architecture\n\nDistributing the crawler\n\n\nDNS resolution\nThe URL frontier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Crawler architecture\n Up: Web crawling and indexes\n Previous: Features a crawler should\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}