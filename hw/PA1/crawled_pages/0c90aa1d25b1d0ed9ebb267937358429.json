{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-theory-1.html",
  "title": "Probability estimates in theory",
  "body": "\n\n\n\n\nProbability estimates in theory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Probability estimates in practice\n Up: The Binary Independence Model\n Previous: Deriving a ranking function\n    Contents \n    Index\n\n\n\n \n\nProbability estimates in theory\n\n\nFor each term , what would these  numbers look like for the whole collection? odds-ratio-ct-contingency gives a contingency table of counts of documents in the collection, where  is the number of documents that contain term :\n\n\n\nUsing this,  and \n and\n\n\n\n\n\n\n(74)\n\n\nTo avoid the possibility of zeroes (such as if every or no relevant\ndocument has a particular term) it is fairly standard to  add   to each of the quantities in the center 4 terms of odds-ratio-ct-contingency, and then to adjust the marginal counts (the totals) accordingly (so, the bottom right cell totals ). Then we have:\n\n\n\n\n\n\n(75)\n\n\n\nAdding  in this way is a simple form of\nsmoothing.  For trials with categorical outcomes (such as\nnoting the presence or absence of a term),\none way to estimate the probability of\nan event from data is simply to count the number of times an\nevent occurred divided by the total number of trials.\nThis is referred to as the  relative frequency  of the event.\nEstimating the\nprobability as the relative frequency is the  maximum\nlikelihood estimate  (or \n MLE ),\nbecause this value\nmakes the observed data maximally likely.  However, if we\nsimply use the MLE, then the probability given to events we\nhappened to see is usually too high, whereas other\nevents may be completely unseen and giving them as a\nprobability estimate their relative frequency of 0 is both\nan underestimate, and normally breaks our models, since\nanything multiplied by 0 is 0.  Simultaneously decreasing\nthe estimated\nprobability of seen events and increasing the probability of\nunseen events is referred to as  smoothing .  One\nsimple way of smoothing is to \n add a number  \nto each\nof the observed counts.  These  pseudocounts \ncorrespond to the use of a uniform distribution over the vocabulary as a  Bayesian\nprior , following\nEquation 59.  We initially assume a uniform\ndistribution over events, where the size of  denotes\nthe strength of our belief in uniformity, and we then update\nthe probability based on observed events. Since our belief\nin uniformity is weak, we use \n\n.  This\nis a form of  maximum a posteriori  ( MAP )\nestimation, where we choose the most likely point value for\nprobabilities based on the prior and the observed evidence,\nfollowing Equation 59.  We will further discuss\nmethods of smoothing estimated counts to give probability\nmodels in Section 12.2.2 (page ); the simple method of\n adding  \nto each observed count will do for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Probability estimates in practice\n Up: The Binary Independence Model\n Previous: Deriving a ranking function\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}