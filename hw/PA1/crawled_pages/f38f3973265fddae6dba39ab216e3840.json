{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-unranked-retrieval-sets-1.html",
  "title": "Evaluation of unranked retrieval sets",
  "body": "\n\n\n\n\nEvaluation of unranked retrieval sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Evaluation of ranked retrieval\n Up: Evaluation in information retrieval\n Previous: Standard test collections\n    Contents \n    Index\n\n\n\n\n   \n\nEvaluation of unranked retrieval sets\n\n\nGiven these ingredients, how is system effectiveness measured?  The two\nmost frequent and basic measures for information retrieval effectiveness\nare precision and recall.  These are first defined for the simple case\nwhere an IR system returns a set of documents for a query.  We will see\nlater how to extend these notions to ranked retrieval situations.\n\n\n Precision  () is the fraction of retrieved documents that\n  are relevant\n\n\n\n\n\n\n(36)\n\n\n\n\n Recall  () is the fraction of relevant documents that\n  are retrieved\n\n\n\n\n\n\n(37)\n\n\n\n\nThese notions can be made clear by examining the following contingency\ntable:\n\n\n\nThen:\n\n\n\n\n\n\n\n\n(38)\n\n\n\n\n(39)\n\n\n\n \nAn obvious alternative that may occur to the reader is to \njudge an information retrieval system by its \n accuracy , that is, the fraction of its classifications that are\ncorrect.  In \nterms of the contingency table above, \n.  \nThis seems plausible, since there are two actual classes, relevant and\nnonrelevant, and an information retrieval system can be thought of as a\ntwo-class classifier which attempts to label them as such\n(it retrieves the subset of documents which it believes to be relevant).\nThis is precisely the effectiveness measure often\nused for evaluating machine learning classification problems.\n\n\nThere is a good reason why accuracy is not an appropriate measure for\ninformation retrieval problems.  In almost all circumstances, the data\nis extremely skewed: normally over 99.9% of the documents are in the \nnonrelevant category.  A system tuned to maximize\naccuracy can appear to perform  well by simply deeming all documents nonrelevant \nto all queries. Even if\nthe system is quite good, trying\nto label some documents as relevant will almost always lead to a\nhigh rate of false positives.  However, \nlabeling all documents as nonrelevant is\ncompletely unsatisfying to an information retrieval system user.  Users are\nalways going to want to see some documents, and can be assumed to have a\ncertain tolerance for seeing some false positives providing that they\nget some useful information.  The measures of precision and recall\nconcentrate the evaluation on the return of true positives, asking what\npercentage of the relevant documents have been found and how many false\npositives have also been returned.\n\n\nThe advantage of having the two numbers for precision and recall is that\none is more important than the other in many circumstances.  Typical web\nsurfers would like every result on the first page to be relevant (high\nprecision) but have not the slightest interest in knowing let alone\nlooking at every document that is relevant.  In contrast, various\nprofessional searchers such as paralegals and intelligence analysts are\nvery concerned with trying to get as high recall as possible, and will\ntolerate fairly low precision results in order to get it.  Individuals\nsearching their hard disks are also often interested in high recall\nsearches.  Nevertheless,\nthe two quantities clearly trade off against one another: you can always\nget a recall of 1 (but very low precision) by retrieving all documents\nfor all queries!  Recall is a non-decreasing function of the number of\ndocuments retrieved.  On the other hand, in a good system, precision\nusually decreases as the number of documents retrieved is increased.\nIn general we want to get some amount of recall while tolerating only\na certain percentage of false positives.\n\n\nA single measure that trades off precision versus\nrecall is the  F measure , which is the weighted harmonic mean of\nprecision and recall:\n\n\n\n\n\n\n(40)\n\n\nwhere \n and thus \n.\nThe default \n balanced F measure \nequally weights precision and recall,\nwhich means making  or .\nIt is commonly written as , which is short for\n, even though the formulation in terms of \nmore transparently exhibits the F measure as a weighted harmonic mean.\nWhen using , the formula on the right simplifies\nto:\n\n\n\n\n\n\n(41)\n\n\nHowever, using an even weighting is not the only choice.  Values of\n emphasize precision, while values of  emphasize\nrecall.  For example, a value of  or  might be used if\nrecall is to be emphasized.\nRecall, precision, and the F measure are\ninherently measures between 0 and 1, but they are also very commonly\nwritten as percentages, on a scale between 0 and 100.\n\n\n\n\nGraph comparing the harmonic mean to other means.The graph\n  shows a slice through the calculation of various means of precision\n  and recall for the fixed recall value of 70%. The\n  harmonic mean is always less than either the arithmetic or geometric\n  mean, and often quite close to the minimum of the two numbers.  When the\n  precision is also 70%, all the measures coincide.\n\n\nWhy do we use a harmonic mean rather than the simpler\naverage (arithmetic mean)?  Recall that we can always get 100% recall by\njust returning all documents, and therefore we can always get a 50%\narithmetic mean by the same process.  This strongly suggests that the\narithmetic mean is an unsuitable measure to use.\nIn contrast, if we assume that\n1 document in 10,000 is relevant to the query, the harmonic mean score of \nthis strategy is 0.02%.  \nThe harmonic mean  is always less than or equal\nto the arithmetic mean and the geometric mean.\nWhen the values of two numbers differ greatly, the harmonic mean is\ncloser to their minimum than to \ntheir arithmetic mean; see Figure 8.1 .\n\n\nExercises.\n\nAn IR system returns 8 relevant\ndocuments, and 10 nonrelevant documents. There are a total\nof 20 relevant documents in the collection. What is the\nprecision of the system on this search, and what is its\nrecall?\n\n\n\nThe balanced F measure (a.k.a. F)\nis defined as the harmonic mean of precision and\nrecall. What is the advantage of using the harmonic mean\nrather than ``averaging'' (using the arithmetic mean)?\n\n\n\nDerive the equivalence between the\ntwo formulas for F measure shown in Equation 40, given\nthat \n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Evaluation of ranked retrieval\n Up: Evaluation in information retrieval\n Previous: Standard test collections\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}