{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/overview-1.html",
  "title": "Overview",
  "body": "\n\n\n\n\nOverview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Features a crawler must\n Up: Web crawling and indexes\n Previous: Web crawling and indexes\n    Contents \n    Index\n\n\n\n\n \n\nOverview\n \nWeb crawling is the process by which we gather pages from the Web,\nin order to index them and support a search engine. The objective of crawling is to quickly and efficiently\ngather as many useful web pages as possible, together with the link\nstructure that interconnects them. In Chapter 19  we studied\nthe complexities of the Web stemming from its creation by millions\nof uncoordinated individuals. In this chapter we study the\nresulting difficulties for crawling the Web.  The focus of this chapter is the component shown in Figure 19.7  as  web crawler ; it is sometimes referred to as a  spider .\n\n\nThe goal of this chapter is not to describe how to build the crawler for a full-scale commercial web search engine. We focus instead on a range of issues that are generic to crawling from the student project scale to substantial research projects. We begin (Section 20.1.1 ) by listing desiderata for web crawlers, and then discuss in Section 20.2 how each of these issues is addressed.  The remainder of this chapter describes the architecture and some implementation details for a distributed web crawler that satisfies these features. Section 20.3  discusses distributing indexes across many machines for a web-scale implementation.\n\n\nSubsections\n\nFeatures a crawler must provide\nFeatures a crawler should provide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Features a crawler must\n Up: Web crawling and indexes\n Previous: Web crawling and indexes\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}