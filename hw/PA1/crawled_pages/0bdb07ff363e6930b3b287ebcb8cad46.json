{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-xml-retrieval-1.html",
  "title": "Evaluation of XML retrieval",
  "body": "\n\n\n\n\nEvaluation of XML retrieval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Text-centric vs. data-centric XML\n Up: XML retrieval\n Previous: A vector space model\n    Contents \n    Index\n\n\n\n\n \n\nEvaluation of XML retrieval\n\n\n\n\n\nTable 10.2:\nINEX 2002 collection statistics.\n\n12,107\nnumber of documents\n\n494 MB\nsize\n\n1995-2002\ntime of publication of articles\n\n1,532\naverage number of XML nodes per document\n\n6.9\naverage depth of a node\n\n30\nnumber of CAS topics\n\n30\nnumber of CO topics\n\n\n \n\n\n\n\n\n\n\nFigure 10.11:\nSimplified schema of the documents in the\nINEX collection.\n\n\n\n\nThe premier venue\nfor research on XML retrieval\nis the  INEX  (INitiative\nfor the Evaluation of XML retrieval) program, a\ncollaborative effort that has produced reference collections,\nsets of queries, and relevance judgments. A yearly INEX\nmeeting is held to\npresent and discuss research results.\nThe INEX 2002 collection consisted of about 12,000 articles from\nIEEE journals. We give collection statistics  in\nTable 10.2  and show\npart of the schema of the collection in\nFigure 10.11 . The IEEE journal collection was expanded\nin 2005. Since 2006 INEX uses the much larger English\n Wikipedia  as a test collection. The relevance of\ndocuments is judged by human assessors using the methodology introduced in\nSection 8.1 (page ), appropriately modified for structured documents\nas we will discuss shortly.\n\n\nTwo types of information needs or\n  \nin\nINEX are content-only or CO topics and content-and-structure\n(CAS) topics.  CO topics  are regular\nkeyword queries as in unstructured information retrieval.  CAS topics \nhave structural constraints in addition to keywords.\nWe already encountered an example of a CAS\ntopic in\nFigure 10.3 . The keywords in this case are\nsummer and holidays and the structural\nconstraints specify that the keywords occur in a section\nthat in turn is part of an article and that this article has\nan embedded year attribute with value\n2001 or 2002.\n\n\nSince CAS queries have both structural and content criteria,\nrelevance assessments are more complicated than in\nunstructured retrieval. INEX 2002 defined component\ncoverage and topical relevance as orthogonal\ndimensions of relevance. The  component coverage  dimension\nevaluates whether the element retrieved is ``structurally''\ncorrect, i.e., neither too low nor too high in the tree. We\ndistinguish four cases:\n\n\nExact coverage (E). The information sought is\nthe main topic of the component and the component is a meaningful unit of information.\n\nToo small (S). The information sought is the main\ntopic of the component, but the component is not a\nmeaningful (self-contained) unit of information.\n\nToo large (L). The information sought is present in\nthe component, but is not the main topic.\n\nNo coverage (N). The information sought is not a topic\nof the component.\n\n\n\nThe  topical relevance  dimension also has four levels: highly\nrelevant (3), fairly relevant (2), marginally relevant (1)\nand nonrelevant (0).\nComponents are judged on both\ndimensions and the judgments are then combined into a\ndigit-letter code.  2S is a fairly relevant component that\nis too small and 3E is a highly relevant component that has\nexact coverage.  In theory, there are 16 combinations of\ncoverage and relevance, but many cannot occur. For example,\na nonrelevant component cannot have exact coverage, so the\ncombination 3N is not possible.\n\n\nThe relevance-coverage combinations are quantized\nas follows:\n\n\n\n\n\n\n(54)\n\n\nThis evaluation scheme takes account of the fact that binary\nrelevance judgments, which are standard in unstructured\ninformation retrieval (Section 8.5.1 , page 8.5.1 ), are not appropriate for XML\nretrieval. A 2S component\nprovides incomplete information and may be difficult to\ninterpret without more context,\nbut it does answer the query partially. The quantization\nfunction Q does not impose a binary choice\nrelevantnonrelevant and\ninstead allows us to grade the component as partially relevant.\n\n\nThe number of relevant components in a retrieved set  of\ncomponents can\nthen be computed as:\n\n\n\n\n\n\n(55)\n\n\nAs an approximation, the standard definitions of precision,\nrecall and F from Chapter 8 \ncan be applied to this modified definition of\nrelevant items retrieved, with some subtleties\nbecause we sum\ngraded as opposed to binary relevance assessments. See the\nreferences on focused retrieval in Section 10.6  for further discussion.\n\n\nOne flaw of measuring relevance this way is that overlap is\nnot accounted for. We discussed the concept of marginal\nrelevance in the context of unstructured retrieval in\nSection 8.5.1 (page ).  This problem is worse in XML retrieval\nbecause of the problem of multiple nested elements occurring\nin a search result as we discussed on\npage 10.2 .\nMuch of the recent focus at\nINEX has been on developing algorithms and evaluation\nmeasures that return non-redundant results lists and evaluate\nthem properly. See the references in Section 10.6 .\n\n\n\n\n\nTable 10.3:\nINEX 2002 results of the vector space model in Section 10.3  for\ncontent-and-structure (CAS) queries and the quantization function Q.\n\nalgorithm\naverage precision\n\nSIMNOMERGE\n0.242\n\nSIMMERGE\n0.271\n\n\n \n\n\n\n\n\nTable 10.3  shows two INEX 2002 runs of\nthe vector space system we described in Section 10.3 .\nThe better run is the SIMMERGE run, which incorporates few\nstructural constraints and mostly relies on keyword\nmatching.  SIMMERGE's median average precision (where the\nmedian is with respect to average precision numbers over\ntopics) is only 0.147.  Effectiveness in XML retrieval is often\nlower than in unstructured retrieval since XML retrieval is\nharder. Instead of just finding a document,\nwe have to find the subpart of a document that is most\nrelevant to the query.  Also, XML retrieval effectiveness -\nwhen evaluated as described here - can be lower than\nunstructured retrieval effectiveness on a standard\nevaluation because graded judgments lower measured\nperformance.  Consider a system that returns a document with\ngraded relevance 0.6 and binary relevance 1 at the top of\nthe retrieved list. Then, interpolated precision at 0.00\nrecall (cf. page 8.4 ) is 1.0 on a binary evaluation, but can be as low as\n0.6 on a graded evaluation.\n\n\n\n\n\nTable 10.4:\nA comparison of content-only and full-structure\nsearch in INEX 2003/2004.\n\n \ncontent only\nfull structure\nimprovement\n\nprecision at 5\n0.2000\n0.3265\n63.3%\n\nprecision at 10\n0.1820\n0.2531\n39.1%\n\nprecision at 20\n0.1700\n0.1796\n5.6%\n\nprecision at 30\n0.1527\n0.1531\n0.3%\n\n\n \n\n\n\n\n\nTable 10.3  gives us a sense of the typical\nperformance of XML retrieval, but it does not\ncompare structured with unstructured retrieval.\nTable 10.4  directly shows the effect of using\nstructure in retrieval. The results are for a\nlanguage-model-based system (cf. Chapter 12 ) that is\nevaluated on a subset of CAS topics from INEX 2003 and\n2004. The evaluation metric is precision at  as defined\nin Chapter 8  (page 8.4 ). The\ndiscretization function used for the evaluation maps \nhighly relevant elements\n(roughly corresponding to the 3E elements defined for\nQ) to 1\nand all other elements to 0. The content-only\nsystem treats queries and documents as unstructured bags of\nwords.  The full-structure model ranks elements that satisfy\nstructural constraints higher than elements that do not. For\ninstance, for the query in Figure 10.3  an element\nthat contains the phrase summer holidays in a section\nwill be rated higher than one that contains it in an\nabstract.\n\n\nThe table shows that structure helps increase precision at\nthe top of the results list. There is a large increase of\nprecision at  and at . There is almost no improvement\nat . \nThese results demonstrate the benefits of structured retrieval.\nStructured retrieval imposes\nadditional constraints on what to return and documents that\npass the structural filter are more likely to be relevant. \nRecall may suffer because some\nrelevant documents will be filtered out, but for\nprecision-oriented tasks structured retrieval is superior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Text-centric vs. data-centric XML\n Up: XML retrieval\n Previous: A vector space model\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}