{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/critiques-and-justifications-of-the-concept-of-relevance-1.html",
  "title": "Critiques and justifications of the concept of relevance",
  "body": "\n\n\n\n\nCritiques and justifications of the concept of relevance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: A broader perspective: System\n Up: Assessing relevance\n Previous: Assessing relevance\n    Contents \n    Index\n\n\n\n \n\nCritiques and justifications of the concept of relevance\n\n\nThe advantage of system evaluation, as enabled by the standard model of\nrelevant and nonrelevant documents, is that we have a fixed setting\nin which we can vary IR systems and system parameters to carry out comparative\nexperiments.  Such formal testing is much less expensive and allows\nclearer diagnosis of the effect of changing system parameters than doing user\nstudies of retrieval effectiveness.\nIndeed, once we have a formal measure that we\nhave confidence in, we can\nproceed to optimize effectiveness by machine learning \nmethods, rather than tuning parameters by hand.  \nOf course, if the formal measure poorly describes what users\nactually want, doing this will not be effective in \nimproving user satisfaction.  Our\nperspective is that, in practice, the \nstandard formal measures for IR evaluation, although a simplification,\nare good enough, and \nrecent work in optimizing formal evaluation measures \nin IR has succeeded brilliantly.  There are numerous examples of\ntechniques developed in formal evaluation settings, which improve\neffectiveness in operational settings, such as the development of\ndocument length normalization methods within the context of TREC\n( and 11.4.3 ) and machine learning methods for\nadjusting parameter weights in scoring (Section 6.1.2 ). \n\n\nThat is not to say that there are not problems\nlatent within the abstractions used.  The relevance of one document is\ntreated as \nindependent of the relevance of other documents in the collection.\n(This assumption is actually built into most retrieval systems -\ndocuments are scored against queries, not against each other - as well\nas being assumed in the evaluation methods.)\nAssessments are\nbinary: there aren't any nuanced assessments of relevance.\nRelevance of a document to an information need is treated as an\nabsolute, objective decision. But judgments of relevance are\nsubjective, varying across people, as we discussed above.  In\npractice, human assessors are also imperfect measuring instruments,\nsusceptible to failures of understanding and attention. We also have\nto assume that users' \ninformation needs do not change as they start looking at retrieval\nresults.  Any results based on one collection are heavily skewed by\nthe choice of collection, queries, and relevance judgment set: the results \nmay not translate from one domain to another or to a different user population.\n\n\nSome of these problems may be fixable.  A number of recent evaluations,\nincluding INEX, some TREC tracks, and NTCIR \nhave adopted an ordinal notion of relevance with documents divided\ninto 3 or 4 classes, distinguishing slightly relevant documents from\nhighly relevant documents.  See Section 10.4 (page ) for a detailed discussion\nof how this is implemented in the INEX evaluations.\n\n\nOne clear problem with the relevance-based assessment that we have\npresented is the distinction between relevance and   marginal relevance :\nwhether a document still has distinctive usefulness after the user has looked\nat certain other documents (Carbonell and Goldstein, 1998).  Even if a document\nis highly relevant, its \ninformation can be completely redundant with other documents which have\nalready been examined.  The most extreme case of this is documents that\nare duplicates - a phenomenon that is actually very common on the World\nWide Web - but it can also easily occur when several documents provide a\nsimilar precis of an event.  In such circumstances, marginal relevance\nis clearly a better measure of utility to the user.  Maximizing\nmarginal relevance requires returning documents that exhibit diversity\nand novelty.  One way to approach measuring this is by using distinct\nfacts or entities as \nevaluation units.  This perhaps more directly measures true utility to\nthe user but doing this makes it harder to create a test collection.\n\n\nExercises.\n\nBelow is a table showing how two human judges rated the relevance of a\nset of 12 documents to a particular information need (0 = nonrelevant, 1\n= relevant).  Let us assume that you've written an IR system that for\nthis query returns the set of documents {4, 5, 6, 7, 8}.\n\n\ndocID\nJudge 1\nJudge 2\n\n1\n0\n0\n\n2\n0\n0\n\n3\n1\n1\n\n4\n1\n1\n\n5\n1\n0\n\n6\n1\n0\n\n7\n1\n0\n\n8\n1\n0\n\n9\n0\n1\n\n10\n0\n1\n\n11\n0\n1\n\n12\n0\n1\n\n\n\n\nCalculate the kappa measure between the two judges.\n\nCalculate precision, recall, and  of your system if a\n  document is considered relevant only if the two judges agree.\n\nCalculate precision, recall, and  of your system if a\n  document is considered relevant if either judge thinks it is relevant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: A broader perspective: System\n Up: Assessing relevance\n Previous: Assessing relevance\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}