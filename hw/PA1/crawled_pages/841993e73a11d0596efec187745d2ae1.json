{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/multinomial-distributions-over-words-1.html",
  "title": "Multinomial distributions over words",
  "body": "\n\n\n\n\nMultinomial distributions over words\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The query likelihood model\n Up: Language models\n Previous: Types of language models\n    Contents \n    Index\n\n\n\n\nMultinomial distributions over words\n\n\nUnder the unigram language model the order of words is irrelevant, and\nso such models are often called ``bag of words'' models, as discussed in\nChapter 6  (page 6.2 ).  Even though there is no\nconditioning on preceding context, this model nevertheless\nstill gives the probability of a particular ordering of terms.\nHowever, any other ordering of this bag\nof terms will have the same probability.  So, really, we have\na  multinomial distribution  over words.  So long as we\nstick to unigram models, the language model name and motivation could\nbe viewed as historical rather than necessary. We could instead just\nrefer to the model as a multinomial model.  From this perspective, the\nequations presented above do not present the multinomial probability\nof a bag of words, since they do not sum over all possible orderings\nof those words, as is done by the multinomial coefficient (the first\nterm on the right-hand side) in the\nstandard presentation of a multinomial model:\n\n\n\n\n\n\n(97)\n\n\nHere, \n is the length of\ndocument ,  is the size of the term\nvocabulary, and the products are now over the \nterms in the vocabulary, not the positions in the document.\nHowever, just as with STOP probabilities, in practice we can\nalso leave out the multinomial coefficient in our calculations,\nsince, for a particular bag of words, it will be a constant, and so it\nhas no effect on the likelihood ratio of two different models\ngenerating a particular bag of words.  Multinomial distributions\nalso appear in Section 13.2 (page ).\n\n\nThe fundamental problem in designing language models is that we do\nnot know what exactly we should use as the model .  However, we do\ngenerally have a sample of text that is representative of that model.\nThis problem makes a lot of sense in the original, primary uses of\nlanguage models.  For example, in speech recognition, we have a training\nsample of (spoken) text.  But we have to expect that, in the future,\nusers will use \ndifferent words and in different sequences, which we have never observed\nbefore, and so the model has to generalize beyond the observed data to\nallow unknown words and sequences.  This interpretation is not so clear\nin the IR case, where a document is finite and usually fixed.  \nThe strategy we adopt in IR is as follows.\nWe pretend that the\ndocument  is only a representative sample of text drawn from a model\ndistribution, treating it like a fine-grained topic.\nWe then estimate a language model from this sample, and use that\nmodel to calculate the probability of observing any word sequence, and,\nfinally, we rank documents according to their probability of generating\nthe query.\n\n\nExercises.\n\nIncluding stop probabilities in the calculation,\nwhat will the sum of the probability estimates of all strings in the\nlanguage of length 1 be?  Assume that you generate a word and then\ndecide whether to stop or not (i.e., the null string is not part of\nthe language).\n\n\n\nIf the stop probability is omitted from\ncalculations, what will the sum of the scores assigned to strings in\nthe language of length 1 be?\n\n\n\nWhat is the likelihood ratio of\nthe document according to  and  in\nm1m2compare?  \n\n\n\n No explicit STOP \nprobability appeared in\nm1m2compare.  Assuming that the STOP\nprobability of each model is 0.1, does this change the likelihood\nratio of a document according to the two models?\n\n\n\nHow might a language model be used in a  spelling correction  system?\nIn particular, consider the case of context-sensitive spelling\ncorrection, and correcting incorrect usages of words, such as\ntheir in Are you their?  (See Section 3.5 (page )\nfor pointers to some literature on this topic.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: The query likelihood model\n Up: Language models\n Previous: Types of language models\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}