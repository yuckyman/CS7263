{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-text-classification-1.html",
  "title": "Evaluation of text classification",
  "body": "\n\n\n\n\nEvaluation of text classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Text classification and Naive\n Previous: Comparison of feature selection\n    Contents \n    Index\n\n\n\n \n\nEvaluation of text classification\n\n\n \n\n\nHistorically, the classic Reuters-21578 collection was the\nmain benchmark for text classification evaluation. This is\na collection of 21,578 newswire articles, originally\ncollected and labeled by Carnegie Group, Inc. and Reuters,\nLtd. in the course of developing the CONSTRUE text\nclassification system. It is much smaller than and predates\nthe Reuters-RCV1 collection discussed in Chapter 4 \n(page 4.2 ). The articles are assigned classes from\na set of 118 topic categories. A document may be assigned\nseveral classes or none, but the commonest case is single\nassignment (documents with at least one class received an\naverage of 1.24 classes). The standard approach to this\nany-of problem (Chapter 14 ,\npage 14.5 ) is to learn 118 two-class\nclassifiers, one\nfor each class, where the   two-class classifier  for class \nis the classifier for the two classes  and its complement\n\n.\n\n\n\n\n\n\nTable 13.7:\nThe ten largest classes in the\n  Reuters-21578\ncollection with number of documents in training and test sets.  \n class\n# train\n# test\n class\n# train\n# test\n \n earn\n2877\n1087\n trade\n369\n119\n \n acquisitions\n1650\n179\n interest\n347\n131\n \n money-fx\n538\n179\n ship\n197\n89\n \n grain\n433\n149\n wheat\n212\n71\n \n crude\n389\n189\n corn\n182\n56\n \n\n\n\n\nFor each of these classifiers, we can\nmeasure recall, precision, and accuracy. In recent work,\npeople almost invariably use the   ModApte split , which\nincludes only documents that were viewed and assessed by a\nhuman indexer, and\ncomprises 9,603 training documents and 3,299 test documents.\nThe distribution of documents in classes is very uneven,\nand some work evaluates systems on only documents in the ten\nlargest classes.\nThey are listed in Table 13.7 . A typical document\nwith topics is shown in Figure 13.9 .\n\n\nIn Section 13.1 , we stated as our goal in\ntext classification the minimization of classification error\non test data.  Classification error is 1.0 minus\nclassification accuracy, the proportion of correct\ndecisions, a measure we introduced in Section 8.3 \n(page 8.3 ). This measure is appropriate if the\npercentage of documents in the class is high, perhaps 10%\nto 20% and\nhigher. But as we discussed in Section 8.3 ,\naccuracy is not a good measure for ``small'' classes because\nalways saying no, a strategy that defeats the purpose of\nbuilding a classifier, will achieve high accuracy. The\nalways-no classifier is 99% accurate for a class with\nrelative frequency 1%. For small classes, precision, recall\nand  are better measures. \n\n\nWe will use   effectiveness  as a generic term for\nmeasures that evaluate the quality of classification\ndecisions, including precision, recall, , and accuracy.\n\n Performance  refers\nto the computational efficiency    of classification\nand IR systems in this book. However, many researchers mean effectiveness,\nnot efficiency of text classification when they use the term\nperformance.\n\n\n\n\nFigure 13.9:\nA sample document from the  Reuters-21578\ncollection.\n\n\n\n\nWhen we process a collection with several two-class classifiers\n(such as Reuters-21578 with its 118 classes), we\noften want to compute a single aggregate measure that\ncombines the measures for individual classifiers.\nThere are two methods for doing\nthis.\n   Macroaveraging \ncomputes a simple average over\nclasses.  Microaveraging  pools\nper-document decisions across classes, and then computes an\neffectiveness measure on the pooled contingency\ntable. Table 13.8  gives an example.\n\n\nThe differences between the two methods can be\nlarge. Macroaveraging gives equal weight to each class,\nwhereas microaveraging gives equal weight to each\nper-document classification decision.\nBecause the\n measure ignores true negatives and its magnitude is\nmostly determined by the number of true positives, large\nclasses dominate small classes in microaveraging.\nIn the example, microaveraged precision (0.83) is much closer to the\nprecision of  (0.9) than to the precision of \n(0.5) because  is five times larger than .\nMicroaveraged results are therefore really a measure of\neffectiveness on the large classes in a test collection. To\nget a sense of effectiveness on small classes, you should compute macroaveraged\nresults.  \n\n\n\n\n\n\nTable 13.8:\nMacro- and microaveraging.\n``Truth'' is the true class and\n``call'' the\ndecision of the classifier. In this example, macroaveraged precision is\n\n. Microaveraged precision is\n\n.  \n\n \nclass 1\n\n \ntruth:\ntruth:\n\n \nyes\nno\n\ncall: yes\n10\n10\n\ncall: no\n10\n970\n\n\n\n\nclass 2\n\n \ntruth:\ntruth:\n\n \nyes\nno\n\ncall: yes\n90\n10\n\ncall: no\n10\n890\n\n\n\n\npooled table\n\n \ntruth:\ntruth:\n\n \nyes\nno\n\ncall: yes\n100\n20\n\ncall: no\n20\n1860\n\n\n \n\n\n\n\n\n\n\n\nTable 13.9:\nText  classification effectiveness numbers on Reuters-21578\n  for F (in percent). Results from \nLi and Yang (2003) (a), Joachims (1998) (b: kNN)\nand Dumais et al. (1998) (b: NB, Rocchio, trees, SVM).  \n\n (a)\n \nNB\nRocchio\nkNN\n \nSVM\n \n  \nmicro-avg-L (90 classes)\n80\n85\n86\n \n89\n \n  \nmacro-avg (90 classes)\n47\n59\n60\n \n60\n \n  \n \n \n (b)\n \nNB\nRocchio\nkNN\ntrees\nSVM\n \n \nearn\n96\n93\n97\n98\n98\n \n  \nacq\n88\n65\n92\n90\n94\n \n  \nmoney-fx\n57\n47\n78\n66\n75\n \n  \ngrain\n79\n68\n82\n85\n95\n \n  \ncrude\n80\n70\n86\n85\n89\n \n  \ntrade\n64\n65\n77\n73\n76\n \n  \ninterest\n65\n63\n74\n67\n78\n \n  \nship\n85\n49\n79\n74\n86\n \n  \nwheat\n70\n69\n77\n93\n92\n \n  \ncorn\n65\n48\n78\n92\n90\n \n \nmicro-avg (top 10)\n82\n65\n82\n88\n92\n \n  \nmicro-avg-D (118 classes)\n75\n62\nn/a\nn/a\n87\n \n\n\n\n\n\nIn one-of classification (more-than-two-classes),\nmicroaveraged  is the\nsame as accuracy (Exercise 13.6 ).\n\n\nTable 13.9  gives microaveraged and\nmacroaveraged effectiveness of Naive Bayes for the ModApte\nsplit of Reuters-21578.\nTo give a sense of the relative effectiveness of NB, we compare it with linear SVMs (rightmost column; see Chapter 15 ), one of the\nmost effective classifiers, but also one that is more expensive\nto train than NB.  NB has a microaveraged  of 80%,\nwhich is 9% less than the SVM (89%), a 10% relative\ndecrease (row ``micro-avg-L (90 classes)'').  So there is a surprisingly small effectiveness\npenalty for its simplicity and efficiency.  However, on\nsmall classes, some of which only have on the order of ten\npositive examples in the training set, NB does much\nworse.  Its macroaveraged  is 13% below the SVM, a\n22% relative decrease (row ``macro-avg (90 classes)'' ).\n\n\nThe table also compares NB with the\nother classifiers we cover in this book: Rocchio and kNN.\nIn addition, we give numbers for   decision trees , an\nimportant classification method we do not cover.\nThe bottom part of the table shows that there is considerable\nvariation from class to class. For instance, NB beats kNN on\nship, but is much worse on money-fx. \n\n\nComparing parts (a) and (b) of the table, one is struck by\nthe degree to which the cited papers' results\ndiffer. \nThis is partly due to the fact that the numbers in (b) are\nbreak-even scores (cf. page 8.4 ) averaged\nover 118 classes, whereas the numbers in (a) are true \nscores (computed without any knowledge of the test set)\naveraged over ninety classes. This is unfortunately typical of what happens when\ncomparing different results in text classification: There\nare often differences in the experimental setup\nor the evaluation that complicate the interpretation of the results.\n\n\nThese and other results have shown that\nthe average effectiveness of NB is uncompetitive with\nclassifiers like  SVMs when trained and tested on \n  independent\nand identically distributed \n( i.i.d. ) data, that is, uniform data\nwith all the good properties of statistical sampling.\nHowever, these differences may often be invisible or even\nreverse themselves when working in the real world where,\nusually, the training sample is drawn from a subset of the\ndata to which the classifier will be applied, the nature of\nthe data drifts over time rather than being stationary\n(the problem of  concept drift  we mentioned on page 13.4 ), and there may well\nbe errors in the data (among other problems). Many\npractitioners have had the experience of being unable to\nbuild a fancy classifier for a certain problem that\nconsistently performs better than NB.\n\n\nOur conclusion from the results in Table 13.9  is that,\nalthough most researchers believe that an SVM is\nbetter than kNN and kNN better than NB, the ranking of\nclassifiers ultimately depends on the class, the document collection, and the\nexperimental setup.  In text classification, there is always\nmore to know than simply which machine learning algorithm\nwas used, as we further discuss in Section 15.3 (page ).\n\n\nWhen performing evaluations like the one in\nTable 13.9 , it is important to maintain a\nstrict separation between the  training set  and\nthe  test set . We can easily make correct\nclassification decisions on the test set by using\ninformation we have gleaned from the test set, such as the\nfact that a particular term is a good predictor in the test\nset (even though this is not the case in the training set). A\nmore subtle example of using knowledge about the test set is\nto try a large number of values of a parameter (e.g., the\nnumber of selected features) and select the value that is\nbest for the test set.\nAs a rule, accuracy on new data - the\ntype of data we will encounter when we use the classifier in\nan application - will be much lower than accuracy on a test\nset that the classifier has been tuned for. \nWe discussed \nthe same problem in ad hoc retrieval\nin Section 8.1  (page 8.1 ).\n\n\nIn a clean statistical text classification experiment, \nyou should never run any program on or even look at the test set\nwhile developing a text classification system.\nInstead, set aside a\n   development set  for\ntesting while you develop your method. When such a set\nserves the primary purpose of finding a good value for a\nparameter, for example, the number of selected features, then it is\nalso called \n  held-out data . Train the\nclassifier on the rest of the training set with different\nparameter values, and then select the value that gives best\nresults on the held-out part of the training set.\nIdeally, at the very end, when all parameters have been set and the\nmethod is fully specified, you run one final experiment on the\ntest set and publish the results.\nBecause no information about the test set was used in\ndeveloping the classifier, the results of this experiment\nshould be indicative of actual performance in practice.\n\n\nThis ideal often cannot be met; researchers tend to evaluate several systems on the same test set over a\nperiod of several years.  But it is nevertheless highly\nimportant to not look at the test data and to run systems on\nit as sparingly as possible.  Beginners often violate this\nrule, and their results lose validity because they have\nimplicitly tuned their system to the test data simply by\nrunning many variant systems and keeping the tweaks to the\nsystem that worked best on the test\nset.\n\n\nExercises.\n\nAssume a situation where every document in the test collection has\nbeen assigned exactly one class, and that a classifier also assigns\nexactly one class to each document.  This setup is called\n one-of classification  more-than-two-classes.\nShow that in one-of classification (i) the total number of false positive decisions\nequals the total number of false negative decisions and\n(ii) microaveraged  and accuracy are identical.\n\n\n\nThe class priors in Figure 13.2  are\ncomputed as the fraction of documents in the class as\nopposed to the fraction of tokens in the class. Why?\n\n\n\n\n\nThe function APPLYMULTINOMIALNB in\nFigure 13.2  has time complexity\n\n.  How would you\nmodify the function so that its time complexity is\n\n?\n\n\n\n\n\n\nTable 13.10:\nData for parameter estimation exercise.  \n  \ndocID\nwords in document\nin   China?\n \n training set\n1\nTaipei Taiwan\nyes\n \n  \n2\nMacao Taiwan Shanghai\nyes\n \n  \n3\nJapan Sapporo\nno\n \n  \n4\nSapporo Osaka Taiwan\nno\n \n test set\n5\nTaiwan Taiwan Sapporo\n?\n \n\n\n\n\n\nBased on the data in Table 13.10 ,\n(i) estimate a multinomial Naive Bayes classifier, (ii) apply the\nclassifier to the test document,\n(iii) estimate a Bernoulli NB classifier, (iv) apply the\nclassifier to the test document. You need not estimate\nparameters that you don't need for classifying the test document.\n\n\n\nYour task is to classify words as English or not\nEnglish. Words are generated by a source with the following\ndistribution:\n\n\n event\nword\nEnglish?\nprobability\n \n 1\nozb\nno\n4/9\n \n 2\nuzu\nno\n4/9\n \n 3\nzoo\nyes\n1/18\n \n 4\nbun\nyes\n1/18\n \n\n\n(i) Compute the\nparameters (priors and conditionals) of a multinomial\nNB classifier that uses the letters b, n, o, u, and z as\nfeatures.\nAssume a training set that reflects the\nprobability distribution of the source perfectly. Make the\nsame independence assumptions that are usually made for a\nmultinomial classifier that uses terms as features for text\nclassification. Compute parameters using smoothing, in which\ncomputed-zero probabilities are smoothed into probability\n0.01, and computed-nonzero probabilities are\nuntouched. (This simplistic smoothing may cause \n. Solutions are not required to \ncorrect this.)\n(ii)\nHow does the classifier\nclassify the word zoo?\n(iii) Classify the word zoo using a\nmultinomial classifier as in part (i), but do not make the\nassumption of positional independence. That is, estimate\nseparate parameters for each position in a word. You only\nneed to compute the parameters you need for classifying\nzoo.\n\n\n\nWhat are the values of \n and \n if term and class are\ncompletely independent? What are the values if they are\ncompletely dependent?\n\n\n\nThe feature selection method in Equation 130\nis most appropriate for\nthe Bernoulli model. Why? How could one modify\nit for the multinomial model?\n\n\n\n   Features can also be\nselected according to\n information gain  (IG), which is defined\nas:\n\n\n\n\n\n\n\n\n(138)\n\n\nwhere  is entropy, \n is the training set, and \n, and \n are\nthe subset of \n with term ,\nand the subset of \n without term ,\nrespectively.  is the class distribution\nin (sub)collection , e.g., \n\nif a quarter of the documents in  are in class .\n\n\nShow that mutual information and\ninformation gain are equivalent.\n\n\n\nShow that the two  formulas\n( and 137 )\nare equivalent.  \n\n\nIn the  example on page 13.5.2  we have\n\n. Show that this holds\nin general.\n\n\n\n and mutual information do not distinguish between\npositively and negatively correlated features. Because most\ngood text classification features are positively correlated\n(i.e., they occur more often in  than in ), one\nmay want to explicitly rule out the selection of\nnegative indicators. How would you do this?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Text classification and Naive\n Previous: Comparison of feature selection\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}