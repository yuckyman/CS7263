{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-relevance-feedback-strategies-1.html",
  "title": "Evaluation of relevance feedback strategies",
  "body": "\n\n\n\n\nEvaluation of relevance feedback strategies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Pseudo relevance feedback\n Up: Relevance feedback and pseudo\n Previous: Relevance feedback on the\n    Contents \n    Index\n\n\n\n\nEvaluation of relevance feedback strategies\n\n\nInteractive relevance feedback can give very substantial gains in\nretrieval performance. Empirically, one round of relevance feedback is often very useful. Two rounds is sometimes marginally more useful. Successful use of relevance feedback requires enough judged documents, otherwise the process is unstable in that it may drift away from the user's information need. Accordingly, having at least five judged documents is recommended.\n\n\nThere is some subtlety to evaluating the effectiveness of relevance\nfeedback in a sound and enlightening way. The obvious first strategy is to start with an initial query  and to compute a precision-recall graph. Following one round of feedback from the user, we compute the modified query  and again compute a precision-recall graph. Here, in both rounds we assess performance over all documents in the collection, which makes comparisons straightforward. If we do this, we find spectacular gains from relevance feedback: gains on the order of 50% in mean average precision. But unfortunately it is cheating. The gains are partly due to the fact that known relevant documents (judged by the user) are now ranked higher. Fairness demands that we should only\nevaluate with respect to documents not seen by the user.\n\n\nA second idea is to use documents in the residual collection (the set of documents minus those assessed relevant) for the second round of evaluation. This seems like a more realistic evaluation.\nUnfortunately, the measured performance can then often be lower than for the original query. This is particularly the case if there are few relevant documents, and so a fair proportion of them have been judged by the user in the first round. The relative performance of variant relevance feedback methods can be validly compared, but it is difficult to validly compare performance with and without relevance feedback because the collection size and the number of relevant documents changes from before the feedback to after it.\n\n\nThus neither of these methods is fully satisfactory. A\nthird method is to have two collections, one which is used for the\ninitial query and relevance judgments, and the second that is then used\nfor comparative evaluation. The performance of both  and  can\nbe validly compared on the second collection.\n\n\nPerhaps the best evaluation of the utility of relevance feedback is to\ndo user studies of its effectiveness, in particular by doing a\ntime-based comparison: how fast does a user find relevant documents\n  with relevance feedback vs. another strategy (such as query\n  reformulation), or alternatively, how many relevant documents does a\n  user find in a certain amount of time. Such notions of user utility\n  are fairest and closest to real system usage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Pseudo relevance feedback\n Up: Relevance feedback and pseudo\n Previous: Relevance feedback on the\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}