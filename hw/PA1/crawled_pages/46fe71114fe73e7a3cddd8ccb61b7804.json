{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Flat clustering\n Up: Support vector machines and\n Previous: Result ranking by machine\n    Contents \n    Index\n\n\n\n \n\nReferences and further reading\n\n\nThe somewhat quirky name  support vector machine  originates in the\nneural networks literature, where learning algorithms were thought of\nas architectures, and often referred to as ``machines''.  The\ndistinctive element of this model is that the decision boundary to\nuse is completely decided (``supported'') by a few training data\npoints, the support vectors.\n\n\nFor a more detailed presentation of SVMs, a good, well-known\narticle-length introduction is (Burges, 1998).\nChen et al. (2005) introduce the more recent\n-SVM, which provides an alternative parameterization for dealing\nwith inseparable problems, whereby rather than specifying a penalty ,\nyou specify a parameter  which bounds the number of examples\nwhich can appear on the wrong side of the decision surface.\nThere are now also several books dedicated to SVMs, large margin\nlearning, and kernels: (Cristianini and Shawe-Taylor, 2000) and \n(Schölkopf and Smola, 2001) are more mathematically oriented, while\n(Shawe-Taylor and Cristianini, 2004) aims to be more practical.    For the\nfoundations by \ntheir originator, see (Vapnik, 1998).  Some recent, more general\nbooks on statistical learning, such as (Hastie et al., 2001) also\ngive thorough coverage of SVMs.\n\n\nThe construction of \n multiclass SVMs \nis discussed in \n(Weston and Watkins, 1999), (Crammer and Singer, 2001), and\n(Tsochantaridis et al., 2005).  The last reference provides an\nintroduction to the general framework of structural SVMs.\n\n\nThe kernel trick was first presented in (Aizerman et al., 1964).\nFor more about string kernels and other kernels for structured data, see\n(Lodhi et al., 2002) and (Gaertner et al., 2002).  The Advances in Neural\nInformation Processing (NIPS) conferences have become the premier venue\nfor theoretical machine learning work, such as on SVMs.  Other venues\nsuch as SIGIR are much stronger on experimental methodology and using\ntext-specific features to improve classifier effectiveness.\n\n\nA recent comparison of most current machine learning classifiers (though\non problems rather different from typical text problems) can be found in\n(Caruana and Niculescu-Mizil, 2006).  (Li and Yang, 2003), discussed in\nSection 13.6 , is the most recent comparative evaluation of\nmachine learning classifiers on text classification.  \nOlder examinations of classifiers on text problems can be found in\n(Yang and Liu, 1999, Dumais et al., 1998, Yang, 1999).\nJoachims (2002a)\npresents his work on SVMs applied to text problems in detail.\nZhang and Oles (2001) present an insightful comparison of Naive Bayes,\nregularized logistic regression and SVM classifiers.\n\n\nJoachims (1999) discusses methods of making SVM learning\npractical over large text data sets.  Joachims (2006a)\nimproves on this work.\n\n\nA number of approaches to  hierarchical\nclassification  have been developed in order to deal with\nthe common situation where the classes to be assigned have a\nnatural hierarchical organization\n(Weigend et al., 1999, Dumais and Chen, 2000, Koller and Sahami, 1997, McCallum et al., 1998).\nIn a recent large study on scaling SVMs to the entire Yahoo! directory,\nLiu et al. (2005) conclude that hierarchical classification\nnoticeably if still modestly outperforms flat\nclassification. \nClassifier effectiveness remains\nlimited by the very small number of training documents for many classes.\nFor a more general approach that can be\napplied to modeling relations between classes, which may be arbitrary\nrather than simply the case of a hierarchy, see Tsochantaridis et al. (2005).\n\n\nMoschitti and Basili (2004) investigate the use of complex\nnominals, proper nouns and word senses as features in text classification.\n\n\nDietterich (2002) overviews ensemble methods for classifier\ncombination, while Schapire (2003) focuses particularly on\nboosting, which is applied to text classification in (Schapire and Singer, 2000).\n\n\nChapelle et al. (2006) present an introduction to work in\nsemi-supervised methods, including in particular chapters on using EM\nfor semi-supervised text classification (Nigam et al., 2006)\nand on transductive SVMs (Joachims, 2006b).\nSindhwani and Keerthi (2006) present a more efficient implementation of a\ntransductive SVM for large data sets.\n\n\nTong and Koller (2001) explore active learning with SVMs for text\nclassification; Baldridge and Osborne (2004) point out that examples\nselected for annotation with one classifier in an active learning\ncontext may be no better than random examples when used with another\nclassifier.\n\n\nMachine learning approaches to ranking for ad hoc retrieval were\npioneered in (Wong et al., 1988), (Fuhr, 1992), and \n(Gey, 1994). But limited training\ndata and poor machine learning techniques meant that these pieces of\nwork achieved only middling results, and hence they only had limited\nimpact at the time.\n\n\nTaylor et al. (2006) study using machine learning\nto tune the parameters of\nthe BM25 family of ranking functions okapi-bm25 so as to\nmaximize NDCG (Section 8.4 , page 8.4 ).\nMachine learning approaches to ordinal regression appear in\n(Herbrich et al., 2000) and (Burges et al., 2005), and are applied\nto clickstream data in (Joachims, 2002b).\nCao et al. (2006) study how to make this approach effective in \nIR, and Qin et al. (2007) suggest an extension involving using\nmultiple hyperplanes.\nYue et al. (2007) study how to do ranking with a structural SVM\napproach, and in particular show how this construction can be\neffectively used to directly optimize for MAP\nranked-evaluation, \nrather than using surrogate measures like accuracy or area under the\nROC curve.\nGeng et al. (2007) study feature selection for the ranking problem.\n\n\nOther approaches to learning to rank have also been shown to be\neffective for web search, such as (Richardson et al., 2006, Burges et al., 2005).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Flat clustering\n Up: Support vector machines and\n Previous: Result ranking by machine\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}