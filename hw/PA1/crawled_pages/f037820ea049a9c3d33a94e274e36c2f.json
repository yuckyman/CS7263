{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/types-of-language-models-1.html",
  "title": "Types of language models",
  "body": "\n\n\n\n\nTypes of language models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Multinomial distributions over words\n Up: Language models\n Previous: Finite automata and language\n    Contents \n    Index\n\n\n\n\nTypes of language models\n\n\nHow do we build probabilities over sequences of terms? We can always use\nthe chain rule from Equation 56 to decompose the probability of a \nsequence of events into the probability of each successive event\nconditioned on earlier events:\n\n\n\n\n\n\n(94)\n\n\nThe simplest form of language model simply throws away all\nconditioning context, and estimates each term independently.  Such a\nmodel is called a  unigram language model :\n\n\n\n\n\n\n(95)\n\n\nThere are many more\ncomplex kinds of language models, such as  bigram language models , which\ncondition on the previous term,\n\n\n\n\n\n\n(96)\n\n\nand even more complex grammar-based language models such as\nprobabilistic context-free grammars.  \nSuch models are vital for tasks\nlike  speech recognition ,  spelling correction ,\nand  machine translation ,\nwhere you need the probability of a term conditioned on surrounding\ncontext.   However, most language-modeling\nwork in IR has used unigram language models.  IR is not the\nplace where you most immediately need complex language models, since IR\ndoes not directly depend on the structure of sentences to the extent\nthat other tasks like speech recognition do.  Unigram models are often\nsufficient to judge the topic of a text.  Moreover, as we\nshall see, IR language models are frequently estimated from a single\ndocument and so it is questionable whether there is enough training\ndata to do more.  Losses from data\n sparseness  \n(see the discussion on page 13.2 )\ntend to outweigh\nany gains from richer models. \nThis is an example of the  bias-variance tradeoff  (cf. secbiasvariance): \nWith limited training data, a more constrained model\ntends to perform better.\nIn addition, unigram models are more efficient to estimate\nand apply than higher-order models.\nNevertheless, the importance of phrase\nand proximity queries in IR in general suggests that future work\nshould make use of more sophisticated language models, and some has\nbegun to lmir-refs.  Indeed,\nmaking this move parallels the model of van Rijsbergen in\nChapter 11  (page 11.4.2 ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Multinomial distributions over words\n Up: Language models\n Previous: Finite automata and language\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}