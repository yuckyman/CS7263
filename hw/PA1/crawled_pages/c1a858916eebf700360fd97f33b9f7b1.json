{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-8.html",
  "title": "References and further reading",
  "body": "\n\n\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Relevance feedback and query\n Up: Evaluation in information retrieval\n Previous: Results snippets\n    Contents \n    Index\n\n\n\n\nReferences and further reading\n\n\nDefinition and implementation of the notion of relevance to a query\ngot off to a rocky start in 1953.  Swanson (1988) reports\nthat in an evaluation in that year between two teams, they agreed that 1390\ndocuments were variously relevant to a set of 98 questions, but\ndisagreed on a further 1577 documents, and the disagreements were\nnever resolved.  \n\n\nRigorous formal testing of IR systems was first completed in the\nCranfield experiments, beginning in the late 1950s.  A\nretrospective discussion of the Cranfield test collection\nand experimentation with it can be found in\n(Cleverdon, 1991).  The other seminal series of\nearly IR experiments were those on the SMART system by\nGerard Salton and colleagues\n(Salton, 1971b;1991).  The TREC evaluations\nare described in detail by Voorhees and Harman (2005).\nOnline information is available at\nhttp://trec.nist.gov/.  \nInitially, few researchers computed the statistical\nsignificance of their experimental results, but \nthe IR community increasingly demands this (Hull, 1993).\nUser studies of IR system\neffectiveness began more recently\n(Saracevic and Kantor, 1988;1996).\n\n\nThe notions of recall and precision were first used by\nKent et al. (1955), although the term precision did not\nappear until later.\nThe    (or, rather\nits complement ) was introduced by van Rijsbergen (1979).\nHe provides an extensive theoretical discussion, which shows\nhow adopting a principle of decreasing marginal relevance\n(at some point a user will be unwilling to sacrifice a unit\nof precision for an added unit of recall) leads to the\nharmonic mean being the appropriate method for combining\nprecision and recall (and hence to its adoption rather than\nthe minimum or geometric mean).\n\n\nBuckley and Voorhees (2000) compare several evaluation measures,\nincluding precision at , MAP, and R-precision, and evaluate the\nerror rate of each measure.\n   was adopted\n as the official evaluation metric in the TREC HARD track\n (Allan, 2005).  Aslam and Yilmaz (2005) examine its\nsurprisingly close correlation to MAP, which had been noted in earlier studies \n(Buckley and Voorhees, 2000, Tague-Sutcliffe and Blustein, 1995).\n  A standard program for\n evaluating IR systems which computes many measures of\n ranked retrieval effectiveness is Chris Buckley's trec_eval\n program used in the TREC evaluations. It can be downloaded\n from: http://trec.nist.gov/trec_eval/.\n\n\nKekäläinen and Järvelin (2002) argue for the superiority of graded\nrelevance judgments when dealing with very large document\ncollections, and Järvelin and Kekäläinen (2002) introduce cumulated\ngain-based methods for IR system evaluation in this context.\nSakai (2007) does a study of the stability and\nsensitivity of evaluation measures based on graded relevance\njudgments from  NTCIR  tasks, and concludes that NDCG is\nbest for evaluating document ranking.\n\n\nSchamber et al. (1990) examine the concept of relevance,\nstressing its multidimensional and context-specific nature, but also\narguing that it can be measured effectively.\n(Voorhees, 2000) is the standard article for\nexamining variation in relevance judgments and their\neffects on retrieval system scores and ranking for the TREC\nAd Hoc task.  Voorhees concludes that although the numbers\nchange, the rankings are quite stable.   Hersh et al. (1994)\npresent similar analysis for a medical IR collection.\nIn contrast,\nKekäläinen (2005) analyze some of the later\nTRECs, exploring a 4-way relevance judgment and the notion\nof cumulative gain, arguing that the relevance measure used\ndoes substantially affect system rankings.  See also\nHarter (1998).  Zobel (1998) studies whether\nthe  pooling  method used by TREC to collect a subset of\ndocuments that will be evaluated for relevance is reliable and fair, and\nconcludes that it is.\n\n\nThe    and its use for language-related purposes is\ndiscussed by Carletta (1996).  Many standard sources (e.g., Siegel and Castellan, 1988) present pooled calculation of the expected agreement, but Di Eugenio (2004) argue for preferring the unpooled agreement (though perhaps presenting multiple measures).  For further discussion of alternative measures of agreement, which may in fact be better, see Lombard et al. (2002) and Krippendorff (2003).\n\n\nText summarization has been actively explored for many\nyears.  Modern work on sentence selection was initiated by\nKupiec et al. (1995).  More recent work includes\n(Barzilay and Elhadad, 1997) and (Jing, 2000),\ntogether with a broad selection of work appearing at the\nyearly DUC conferences and at other NLP venues.\nTombros and Sanderson (1998) demonstrate\nthe advantages of dynamic summaries in the IR context.\nTurpin et al. (2007) address how to generate snippets efficiently.\n\n\nClickthrough log analysis is studied in\n(Joachims, 2002b, Joachims et al., 2005). \n\n\nIn a series of papers, Hersh, Turpin and colleagues show how\nimprovements in formal retrieval effectiveness, as evaluated in batch\nexperiments, do not always translate into an improved system for users\n(Hersh et al., 2000b, Turpin and Hersh, 2002, Hersh et al., 2000a;2001, Turpin and Hersh, 2001).\n\n\nUser interfaces for IR and human factors such as models of\nhuman information seeking and usability testing are outside\nthe scope of what we cover in this book.  More information\non these topics can be found in other textbooks, including\n(Baeza-Yates and Ribeiro-Neto, 1999, ch. 10) and (Korfhage, 1997), and\ncollections focused on cognitive aspects\n(Spink and Cole, 2005).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Relevance feedback and query\n Up: Evaluation in information retrieval\n Previous: Results snippets\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}