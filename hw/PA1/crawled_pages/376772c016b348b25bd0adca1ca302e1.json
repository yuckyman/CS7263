{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/index-compression-1.html",
  "title": "Index compression",
  "body": "\n\n\n\n\nIndex compression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Statistical properties of terms\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n\n\n\nIndex compression\n\n\nChapter 1  introduced the dictionary and the inverted\nindex as the central data structures in information\nretrieval (IR).  In this chapter, we employ a number of\ncompression techniques for dictionary and inverted index\nthat are essential for efficient IR systems.\n\n\nOne benefit of compression is immediately clear.  We\nneed less disk space. As we will see, compression ratios of\n1:4 are easy to achieve, potentially cutting the cost of\nstoring the index by 75%. \n\n\nThere are two\nmore subtle benefits of compression. The first is increased use of\ncaching. Search systems use some parts of the dictionary and\nthe index much more than others.  For example, if we cache\nthe postings list of a frequently used query term , then\nthe computations necessary for responding to the one-term\nquery \ncan be entirely done in memory.\nWith compression, we can fit a lot more information into\nmain memory. Instead of having to expend a disk seek when\nprocessing a query with , we instead access its postings\nlist in memory and decompress it. As we will see below,\nthere are simple and efficient decompression methods,\nso that the penalty of having to\ndecompress the postings list is small.  As a result, we are\nable to decrease the response time of the IR system\nsubstantially.  Because memory is a more expensive resource\nthan disk space, increased speed owing to caching - rather\nthan decreased space requirements - is often the prime\nmotivator for compression.\n\n\nThe second more subtle advantage of compression is faster\ntransfer of data from disk to memory.  Efficient decompression\nalgorithms run so fast on modern hardware that the total\ntime of transferring a compressed chunk of data from disk and then\ndecompressing it is usually less than transferring the same chunk of\ndata in uncompressed form. \nFor instance, we can reduce input/output (I/O) time by loading\na much smaller compressed postings list, even when you add on\nthe cost of decompression.  So, in most cases, the retrieval\nsystem runs faster on compressed postings lists than on\nuncompressed postings lists.\n\n\n If the main goal of compression is to conserve disk space,\nthen the speed of compression algorithms is of no\nconcern. But for improved cache utilization and faster\ndisk-to-memory transfer,  decompression speeds must be\nhigh. \nThe compression algorithms we discuss in\nthis chapter are highly efficient and can therefore serve\nall three purposes of index compression.\n\nIn this chapter, we\ndefine a  posting  as a docID in a postings list.  For\nexample, the postings list (6; 20, 45, 100), where 6 is the\ntermID of the list's term, contains three postings. \nAs discussed in Section 2.4.2 (page ),\npostings in\nmost search systems also contain frequency and position\ninformation; but \nwe will only consider simple docID\npostings here. See\nSection 5.4  for references on compressing\nfrequencies and positions.\n\n\nThis chapter first gives a statistical characterization of\nthe distribution of the entities we want to compress -\nterms and postings in large collections\n(Section 5.1 ).  We then look at compression of\nthe dictionary, using the dictionary-as-a-string method and\nblocked storage (Section 5.2 ).\nSection 5.3  describes two techniques for\ncompressing the postings file, variable byte encoding and\n encoding.\n\n\n \n\nSubsections\n\nStatistical properties of terms in information retrieval\n\nHeaps' law: Estimating the number of terms\nZipf's law: Modeling the distribution of terms\n\n\nDictionary compression\n\nDictionary as a string\nBlocked storage\n\n\nPostings file compression\n\nVariable byte codes\nGamma codes\n\n\nReferences and further reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Statistical properties of terms\n Up: irbook\n Previous: References and further reading\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}