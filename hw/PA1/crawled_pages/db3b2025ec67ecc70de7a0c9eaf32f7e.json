{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/matrix-decompositions-1.html",
  "title": "Matrix decompositions",
  "body": "\n\n\n\n\nMatrix decompositions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Term-document matrices and singular\n Up: Linear algebra review\n Previous: Linear algebra review\n    Contents \n    Index\n\n\n\n\n \n\nMatrix decompositions\n \nIn this section we examine ways in which a square matrix can be factored into the product of matrices derived from its eigenvectors; we refer to this process as  matrix decomposition .  \nMatrix decompositions similar to the ones in this section will form the basis of our principal text-analysis technique in Section 18.3 , where we will look at decompositions of non-square term-document matrices.  The square decompositions in this section are simpler and can be treated with sufficient mathematical rigor to help the reader understand how such decompositions work.  The detailed mathematical derivation of the more complex decompositions in Section 18.2  are beyond the scope of this book.\n\n\nWe begin by giving two theorems on the decomposition of a square matrix into the product of three matrices of a special form. The first of these, Theorem 18.1.1, gives the basic factorization of a square real-valued matrix into three factors. The second, Theorem 18.1.1, applies to square symmetric matrices and is the basis of the singular value decomposition described in Theorem 18.2.\n\n\nTheorem.\n(Matrix diagonalization theorem)\nLet  be a square real-valued \n matrix with  linearly independent eigenvectors. Then there exists an  eigen decomposition\n\n\n\n\n\n\n(223)\n\n\nwhere the columns of  are the eigenvectors of  and  is a diagonal matrix whose diagonal entries are the eigenvalues of  in decreasing order\n\n\n\n\n\n\n(224)\n\n\nIf the eigenvalues are distinct, then this decomposition is unique.\nEnd theorem.\n\nTo understand how Theorem 18.1.1 works, we note that  has the eigenvectors of  as columns\n\n\n\n\n\n\n(225)\n\n\nThen we have\n\n\n\n\n\n\n\n\n(226)\n \n\n\n\n(227)\n \n\n\n\n(228)\n\n\nThus, we have , or \n.\n\n\nWe next state a closely related decomposition of a symmetric square matrix into the product of matrices derived from its eigenvectors. This will pave the way for the development of our main tool for text analysis, the singular value decomposition (Section 18.2 ).\n\n\nTheorem.\n(Symmetric diagonalization theorem)\nLet  be a square, symmetric real-valued \n matrix with  linearly independent eigenvectors. Then there exists a  symmetric diagonal decomposition\n\n\n\n\n\n\n(229)\n\n\nwhere the columns of  are the orthogonal and normalized (unit length, real) eigenvectors of , and  is the diagonal matrix whose entries are the eigenvalues of . Further, all entries of  are real and we have .\nEnd theorem.\n\nWe will build on this symmetric diagonal decomposition to build low-rank approximations to term-document matrices.\n\n\nExercises.\n\nWhat is the rank of the  diagonal matrix below?\n\n\n\n\n\n\n(230)\n\n\n\n\nShow that  is an eigenvalue of\n\n\n\n\n\n\n(231)\n\n\nFind the corresponding eigenvector.\n\n\n\nCompute the unique eigen decomposition of the  matrix in (222).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Term-document matrices and singular\n Up: Linear algebra review\n Previous: Linear algebra review\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}