{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/automatic-thesaurus-generation-1.html",
  "title": "Automatic thesaurus generation",
  "body": "\n\n\n\n\nAutomatic thesaurus generation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Global methods for query\n Previous: Query expansion\n    Contents \n    Index\n\n\n\n \n\nAutomatic thesaurus generation\n\n\nAs an alternative to the cost of a manual thesaurus, we could attempt to\ngenerate a thesaurus automatically by analyzing a collection of\ndocuments. There are two main approaches. One is simply to exploit\nword cooccurrence. We say that words co-occurring in a document or\nparagraph are likely to be in some sense similar or related in meaning,\nand simply count text statistics to find the most similar words.\nThe other approach is to use a shallow grammatical analysis of the text\nand to exploit grammatical relations or grammatical dependencies.\nFor example, we say that entities that are grown, cooked, eaten,\nand digested, are more likely to be food items. Simply using\nword cooccurrence is more robust (it cannot be misled by parser errors), but using grammatical relations is more accurate. \n\n\n\n\n\n\nThe simplest way to compute a co-occurrence thesaurus is based on\nterm-term similarities.  We begin with a term-document matrix , where each cell   is a weighted count  for term  and document , with weighting so  has  length-normalized rows.  If we then calculate , then  is a\nsimilarity score between terms  and , with a larger number being better.\nFigure 9.8  shows an example of a thesaurus\nderived in basically this manner, but with an extra step of\ndimensionality reduction via Latent Semantic Indexing, which we\ndiscuss in Chapter 18 . While some of the thesaurus terms \nare good or at\nleast suggestive, others are marginal or bad. The quality of the\nassociations is typically a problem. Term ambiguity easily\nintroduces irrelevant statistically correlated terms. For example, a query for\nApple computer may expand to Apple red fruit computer.\nIn general these thesauri suffer from both false positives\nand false negatives. Moreover, since the terms in the\nautomatic thesaurus are highly correlated in documents anyway (and often\nthe collection used to derive the thesaurus is the same as the one being\nindexed), this form of query expansion may not retrieve many additional\ndocuments.\n\n\nQuery expansion is often effective in increasing recall.\nHowever, there is a high cost to manually producing a\nthesaurus and then updating it for scientific and terminological\ndevelopments within a field.  In general a\ndomain-specific thesaurus is required: general thesauri and dictionaries\ngive far too little coverage of the rich domain-particular vocabularies\nof most scientific fields.\nHowever, query expansion may also significantly decrease\nprecision, particularly when the query contains ambiguous terms. For\nexample, if the user searches for interest rate, expanding the\nquery to interest rate fascinate evaluate is unlikely to be\nuseful.\nOverall, query expansion is less successful than\nrelevance feedback, though it may be as good as pseudo relevance\nfeedback. It does, however, have the advantage of being much more\nunderstandable to the system user.\n\n\nExercises.\n\nIf  is simply a Boolean cooccurrence matrix,\nthen what do you get as the entries in ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: References and further reading\n Up: Global methods for query\n Previous: Query expansion\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}