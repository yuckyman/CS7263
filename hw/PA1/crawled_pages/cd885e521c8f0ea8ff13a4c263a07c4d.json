{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/linear-algebra-review-1.html",
  "title": "Linear algebra review",
  "body": "\n\n\n\n\nLinear algebra review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Matrix decompositions\n Up: Matrix decompositions and latent\n Previous: Matrix decompositions and latent\n    Contents \n    Index\n\n\n\n\n \n\nLinear algebra review\n \nWe briefly review some necessary background in linear algebra. Let  be an \n matrix with real-valued entries; for a term-document matrix, all entries are in fact non-negative.\n\n\nThe  rank  of a matrix is the number of linearly independent rows (or columns) in it; thus, \n. A square  matrix all of whose off-diagonal entries are zero is called a diagonal matrix; its rank is equal to the number of non-zero diagonal entries. If all  diagonal entries of such a diagonal matrix are , it is called the identity matrix of dimension  and represented by .\n\n\nFor a square \n matrix  and a vector  that is not all zeros, the values of  satisfying\n\n\n\n\n\n\n(213)\n\n\nare called the \n eigenvalues \nof\n\n. The -vector  satisfying Equation 213 for an eigenvalue  is the corresponding right eigenvector. The eigenvector corresponding to the eigenvalue of largest magnitude is called the principal eigenvector. In a similar fashion, the left eigenvectors of  are the -vectors  such that\n\n\n\n\n\n\n(214)\n\n\nThe number of non-zero eigenvalues of  is at most \n.\n\n\nThe eigenvalues of a matrix are found by solving the\ncharacteristic equation, which is obtained by\nrewriting Equation 213 in the form \n. The eigenvalues of  are then the solutions of\n\n, where  denotes the  determinant of a square matrix .\nThe equation \n is an th order polynomial equation in  and can have at most  roots, which are the\neigenvalues of . These eigenvalues can in general be complex, even if all entries of  are real.\n\n\nWe now examine some further properties of eigenvalues and eigenvectors, to set up the central idea of singular value decompositions in Section 18.2  below. First, we look at the relationship between matrix-vector multiplication and eigenvalues.\n\n\nWorked example.\nConsider the matrix\n\n\n\n\n\n\n(215)\n\n\nClearly the matrix has rank 3, and has 3 non-zero eigenvalues   and , with the three corresponding eigenvectors\n\n\n\n\n\n\n(216)\n\n\nFor each of the eigenvectors, multiplication by  acts as if we were multiplying the eigenvector by a multiple of the identity matrix; the multiple is different for each eigenvector. Now, consider an arbitrary vector, such as \n We can always express  as a linear combination of the three eigenvectors of ; in the current example we have\n\n\n\n\n\n\n(217)\n\n\nSuppose we multiply  by :\n\n\n\n\n\n\n\n\n(218)\n \n\n\n\n(219)\n \n\n\n\n(220)\n \n\n\n\n(221)\n\n\nEnd worked example.\n\nExample 18.1 shows that even though  is an arbitrary vector, the effect of multiplication by  is determined by the eigenvalues and eigenvectors of . Furthermore, it is intuitively apparent from Equation 221 that the product  is relatively unaffected by terms arising from the small eigenvalues of ; in our example, since , the contribution of the third term on the right hand side of Equation 221 is small.  In fact, if we were to completely ignore the contribution in Equation 221 from the third eigenvector corresponding to , then the product  would be computed to be \n rather than the correct product which is \n; these two vectors are relatively close to each other by any of various metrics one could apply (such as the length of their vector difference).\n\n\nThis suggests that the effect of small eigenvalues (and their eigenvectors) on a matrix-vector product is small. We will carry forward this intuition when studying matrix decompositions and low-rank approximations in Section 18.2 . Before doing so, we examine the eigenvectors and eigenvalues of special forms of matrices that will be of particular interest to us.\n\n\nFor a symmetric matrix , the eigenvectors corresponding to distinct eigenvalues are orthogonal. Further, if  is both real and symmetric, the eigenvalues are all real.\n\n\nWorked example.\nConsider the real, symmetric matrix\n\n\n\n\n\n\n(222)\n\n\nFrom the characteristic equation \n, we have the quadratic \n, whose solutions yield the eigenvalues  and . The corresponding eigenvectors \n and \n are orthogonal.\nEnd worked example.\n\n\n\nSubsections\n\nMatrix decompositions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Matrix decompositions\n Up: Matrix decompositions and latent\n Previous: Matrix decompositions and latent\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}