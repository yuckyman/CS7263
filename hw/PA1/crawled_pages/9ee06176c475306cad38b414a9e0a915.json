{
  "url": "http://nlp.stanford.edu/IR-book/html/htmledition/soft-margin-classification-1.html",
  "title": "Soft margin classification",
  "body": "\n\n\n\n\nSoft margin classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Multiclass SVMs\n Up: Extensions to the SVM\n Previous: Extensions to the SVM\n    Contents \n    Index\n\n\n\n \n\nSoft margin classification\n\n\nFor the very high dimensional problems common in text classification,\nsometimes the data are linearly separable.  But in the general case\nthey are\nnot, and even if they are, we might prefer a solution that better separates\nthe bulk of the data while ignoring a few weird noise documents.\n\n\n\n\nFigure 15.5:\nLarge margin classification with slack \nvariables.\n\n\n\n\nIf the training set \n is not linearly separable, the\nstandard approach is to allow the\nfat decision margin to make a few mistakes (some points - outliers\nor noisy examples - are inside or on\nthe wrong side of the margin).  We then pay a cost\nfor each misclassified example, which depends on how far it is from\nmeeting the margin requirement given in Equation 169.\nTo implement this, we introduce  slack variables .  A\nnon-zero value for  allows  to not meet the margin\nrequirement at a cost proportional to the value of . \nSee Figure 15.5 .\n\n\nThe formulation of the SVM optimization problem with slack variables is:\n\n\n\nThe optimization problem is then\ntrading off how fat it can make the margin versus how many points have\nto be moved around to allow this margin.  The margin can be less than 1\nfor a point  by setting , but then one pays a penalty of\n in \nthe minimization for having done that.  \nThe sum of the  gives an upper bound on the number of training errors. Soft-margin SVMs minimize training error traded off against margin.\nThe parameter  is a\n regularization  term, which provides\na way to control overfitting: as  becomes large, it\nis unattractive to not respect the data at the cost of reducing the\ngeometric margin; when it is small, it is easy to account for some\ndata points with the use of slack variables and to have a fat\nmargin placed so it models the bulk of the data.\n\n\nThe dual problem for soft margin classification becomes:\n\n\n\nNeither the slack variables  nor Lagrange multipliers\nfor them appear in the dual problem.  All we are left with is the\nconstant  bounding the possible size of the Lagrange multipliers for\nthe support vector data points.  As before, the  with non-zero\n will be the support vectors.  The solution of the dual problem is of\nthe form: \n\n\n\nAgain  is not needed explicitly for classification, which can be\ndone in terms of dot products with data points, as in Equation 170.\n\n\nTypically, the support vectors will be a small proportion of the\ntraining data.  However, if the problem is non-separable or with small\nmargin, then every data point which is misclassified or within\nthe margin will have a non-zero . If this set of points\nbecomes large, then, for the nonlinear case which we turn to in\nSection 15.2.3 , this can be a major slowdown for using SVMs at\ntest time.\n\n\n\n\n\nClassifier\nMode\nMethod\nTime complexity\n\nNB\ntraining\n \n\n\n\nNB\ntesting\n \n\n\n\nRocchio\ntraining\n \n\n\n\nRocchio\ntesting\n \n\n\n\nkNN\ntraining\npreprocessing\n\n\n\nkNN\ntesting\npreprocessing\n\n\n\nkNN\ntraining\nno preprocessing\n\n\nkNN\ntesting\nno preprocessing\n\n\n\nSVM\ntraining\nconventional\n\n;\n\n \n \n \n\n, empirically\n\nSVM\ntraining\ncutting planes\n\n\n\nSVM\ntesting\n \n\n\n\n\nTraining and testing complexity of various classifiers including SVMs.\nTraining is the time the learning method takes to learn a\nclassifier over \n, while testing is the time it takes a classifier to\nclassify one document.  For SVMs, multiclass classification is assumed to be done by a set of  one-versus-rest classifiers.   is the average number of\ntokens  per document, while  is the average vocabulary (number of non-zero features) of a document.\n and  are the numbers of\ntokens and types, respectively, in the test document.  \n\n\n\nThe complexity of training and testing with linear SVMs is shown in\nTable 15.1 .  \nThe time for training an SVM is dominated by\nthe time for solving the underlying QP, and so the theoretical and\nempirical complexity \nvaries depending on the method used to solve it.\nThe standard result for solving QPs is that it takes time cubic\nin the size of the data set (Kozlov et al., 1979).  All the\nrecent work on SVM training has worked to reduce that complexity,\noften by being satisfied with approximate solutions.  Standardly, empirical\ncomplexity is about \n\n(Joachims, 2006a). \nNevertheless, the super-linear training time\nof traditional SVM algorithms makes them difficult or impossible to use on\nvery large training data sets.  Alternative traditional SVM solution\nalgorithms which are \nlinear in the number of training examples scale badly with a large\nnumber of features, which is another standard attribute of text problems.\nHowever, a new training algorithm based on cutting plane techniques\ngives a promising answer to this issue \nby having running time linear in the number of training examples and the number of\nnon-zero features in examples (Joachims, 2006a).  \nNevertheless, the actual speed of doing quadratic optimization remains\nmuch slower than simply counting terms as is done in a Naive Bayes\nmodel. Extending SVM\nalgorithms to nonlinear SVMs, as in the next section, standardly increases\ntraining complexity by a factor of \n (since dot\nproducts between \nexamples need to be calculated), making them impractical.\nIn practice it can often be cheaper to\nmaterialize the higher-order features and to train a linear\nSVM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Next: Multiclass SVMs\n Up: Extensions to the SVM\n Previous: Extensions to the SVM\n    Contents \n    Index\n\n\n© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.\n2009-04-07\n\n\n\n"
}