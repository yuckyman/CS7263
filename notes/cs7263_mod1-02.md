# text properties
## regex
`.` matches any single character.
- a.b` matches `acb`, `aac`, `a1b`, etc.
- `[.]` matches `.`
`+` matches one or more instance.
- `abc+` matches `abcccccc`, `abc`, not `ab`
`?` matches ==zero or one== instances.
- `colou?r` matches `color` and `colour`
`*` matches ==zero or more== instances.
- `colou*r` matches `colour`, `colour`, `colouuuuuuur`
`^` represents the starting position.
`$` represents the ending position.
`[...]` bracket expressions represent a single character within the bounds of the brackets. this is similar to the ==OR== expression, considered a *disjunction*. 
- `[abc]` matches `a`,`b`, or `c`
- `[a-z]` matches `a`,`b`,`c`,`d`,`e`,...,`Z`.
- `[a-zA-Z0-9]` matches one alphanumeric character.
`[^...]` is a NOT bracket essentially. it is a *negation*.
- `[^a]` would match `b`, `c`, but not `a`.
`|` is an OR expression.
- `and|or` would match `and` or `or`.
`()` parentheses define a subexpression.
`{m,n}` matches elements appearing at least `m` times and no more than `n` times.
- `a{3,5}` matches `aaa`, `aaaa`, and `aaaaa`, not `aa` or `aaaaaa`

hexadecimal color values: `/^#?([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})`
- zero or one instances of `#`
- exactly six occurrences of any letter a through f and numbers 0-9, case insensitive
- or exactly 3 occurrences of any hexadecimal character 

email addresses: `/^([a-z0-9_.-]+)@([\da-z.-]+\.([a-z.]{2,6})$/`
- at least one alphanumeric character (also includes hyphens, underscores, and periods)
- the `@` symbol
- same kind of character matching for the domain, `\d` being a shorthand for `0-9`
- one `.`, escaped from the wildcard with `\.`
- a 2-6 character long top-level domain, e.g., `.com`, `.co.uk`, etc.

## text normalization
- tokenization
- lemmatization
- stemming
- word count
	- `type`: element of vocabulary
	- `token`: an instance of type in running text
	- `N`: number of tokens
	- `V`: vocabulary (set of types)
	- `|V|`: size of vocabulary

`tokenization`: sequence segmentation in running text. identifying word boundaries.
- challenges: apostrophes, multi-word terms, abbreviations, other languages
`lemmatization`: returns words to their "dictionary form", their headword. `breaking` or `broke` becomes `break`. `am`,`are`,`is` becomes `be`

`stemming`: removing `affixes` from word, reducing it to root. `beautiful` becomes `beauti`, `plotted` becomes `plot`

lemmatization is more proper than stemming.

# indexing and query processing
## indexing process
### motivation
after web crawing, extracting, tokenizing and normalizing, we now can represent the document as a bag-of-words.

a document is represented as a vector of term frequencies,
$ d = (4,0,0,0,3,0, \ldots, tf_{|V|}) $.

a query can also be represented as a vector of term frequencies,
$ q = (1,0,0,0,0,0, \ldots, tf_{|V|}) $.

the matching terms between the query and the documents are used to compute a similarity score.

*dot product* is a common similarity measure,
$ sim(q,d) = q \cdot d = \sum_{i=1}^{|V|} q_i \cdot d_i $.

regarding time complexity, it is $O(q \cdot |D| \cdot l)$, where q is the average length of queries, l is the average length of documents, and |D| (the bottleneck) is the number of documents.

regarding space complexity, it is $O(|D| \cdot |V|)$, where |V| is the size of the vocabulary. 

zipf's law: the frequency of a word is inversely proportional to its rank in the frequency table. each document contains only a small fraction of the vocabulary. 

space efficiency can be greatly improved by storing only the non-zero entries of the document-term matrix.

solution?

### inverted index
enter the inverted index. a mapping of terms to the documents that contain them.

*dictionary*: data structure for storing the term vocabulary.

*posting* is a list that records which documents the term occurs in.

if a forward index looks like:
```
doc1: tropical, fish, include, found, in, environments, around, the, world, ...
doc2: fishkeepers, often, use, the, term, tropical, to, refer, to, ...
doc3: tropical, fish, are, popular, aquarium, fish, ...
doc4: in, freshwater, environments, tropical, fish, are, found, ...
```
then an inverted index looks like:
```
tropical: doc1, doc2, doc3, doc4
fish: doc1, doc2, doc3
include: doc1
found: doc1, doc2
fishkeepers: doc2
often: doc2
```

dictionary data structure:
- modest size
- stays in memory
- allows for flexibility when choosing data structure for postings:
	- hash,
	- b-tree,
	- b+-tree
- some information retrieval systems use hashes, others use trees
	- fixed number of terms or growing?
	- relative frequencies with which various keys are accessed?
	- how many terms?

posting data structure:
- sequential access is expected
- stays on disk
- information to store: docID, term frequency, position
- likely to be huge-- compression is needed

hashes:
- each vocabulary term is hashed to a unique, fixed-size integer
- pros:
	- lookup is fast (O(1))
- cons:
	- no easy way to find all terms starting with prefix (lemmatization)
	- need to rehash when vocabulary changes

#### search trees
binary tree, b-tree, b+tree:
- pros:
	- solves the prefix problem
- cons:
	- slower (O(log n))
	- rebalancing binary trees is expensive
	- b+trees mitigate the rebalancing problem

b-tree:
- each node contains multiple keys and pointers to child nodes
- keys are sorted
- each node can have a variable number of keys and pointers
- the root node has at least two children

### index construction

### index compression

## query processing